%todo opmaak
%todo print een documentje met whiteboard
%todo oefenen op eigen whiteboard

\documentclass{beamer}
\usetheme[compress]{Dresden}
\setbeamertemplate{navigation symbols}{} 

\include{commands}



%% notes: say welcome to my master thesis' defence
%\title{\LARGE \sc{Semantic annotation of urban scenes:}\\Skyline and window detection}
\title{\textsc{Semantic annotation of urban scenes:\\Skyline and window detection}}
\subtitle{\vspace{0.5cm}Speaker: Tjerk Kostelijk\\ 
\vspace{1cm}
Supervisors: \\Isaac Esteban\\Prof. dr ir Frans C. A. Groen}
\date{\today}


\begin{document}


%Committee:
%Prof. dr ir Frans C. A. Groen
%dr. P.H. Rodenburg
%dr. Arnoud Visser


\frame{\titlepage}

%\section[Outline]{}
%\frame{\tableofcontents}

% highlightoutline
\AtBeginSection[]
{
 \begin{frame}
  \frametitle{Outline}
  \small
  \tableofcontents[currentsection,hideothersubsections]
  \normalsize
 \end{frame}
}



%-------------------------------------------------------------------
\frame
{
	\frametitle{Experiment}
	% blank dia
	%notes:
	%this is for a little warming up
	%and to answer the question why I did my research
}


%-------------------------------------------------------------------
\frame
{
	\frametitle{Experiment}
	\fig{floriande_back.eps} %{420px}
}
%-------------------------------------------------------------------


\frame
{
}



%-------------------------------------------------------------------
\frame
{
	\frametitle{Did you see?}
	%todo Yes en No op bord schrijven
	\begin{itemize}
		\item <+-| alert@+> building
		\item <+-| alert@+> tree
		\item <+-| alert@+> bicycle
		\item <+-| alert@+> street light
		\item <+-| alert@+> blue car
		\item <+-| alert@+> red car
		\item <+-| alert@+> brand of the car?
		% blank dia
	\end{itemize}
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Experiment}
	\fig{floriande_back.eps}{420px}
}


%-------------------------------------------------------------------
\frame
{
	\frametitle{Q}
	\begin{itemize}
	\item <+-| alert@+> Why are we so good at depth recognition/object detection?
	\item <+-| alert@+> How can we apply this to a computer system?
		\begin{itemize}
			\item <+-| alert@+> {\textit{Computer Vision}}
			% this is the domain which interest me the most and on which I did my research
		\end{itemize}
	\end{itemize}
	%---------------- notes --------------------
	% These are two questions that keeps us AI people bussy
	%-------------------------------------------
	% I will answer the first one shortley and the rest of my talk is about the second question
}

%\section[Outline]{}
\frame{
  \frametitle{Outline}
	\tableofcontents
}

\section{Introduction}
%-------------------------------------------------------------------
% \frame
% {
% 	\frametitle{Human perception}
% 	% lets talk about how you perceived the scene of the building
% 	\begin{itemize}
% 	\item <+-| alert@+> Why are we so good at depth recognition/object detection?
% 	\item <+-| alert@+> Depth cues
% 	\item <+-| alert@+> Binocular disparity
% 	% vingertest
% 	% everybody raise your index finger, 
% 	% now close your left eye and switch to see your finger hopping
% 	% do the same with your finger further away
% 	% the displacement is smaller
% 
% 	% --------------------- notes -------------------
% 	%We use two eyes and look at the sam scene from slightly diffeent angles
% 	%We perceive two different images, if an object appears close the differenc
% 	%displacement difference between the images is high
% 	%this makes it possible to triangulate the distance to an abject with a high
% 	%degree of accuracy
% 
% 	\item <+-| alert@+> Classify objects: feature detection
% 	\end{itemize}
% }
% 
% \frame
% {
% 	\frametitle{Classify objects by feature detection}
% 	\fig{demonsCrop.eps}
% }
% 
% \frame
% {
% 	\frametitle{Classify objects by feature detection}
% 	\fig{demonsCropSelect.eps}
% }
% 
% %--------- notes---------------------
% % different types of features are activated
% % horizontal lines
% % vertical lines
% % curves
% %------------------------------------
% \frame
% {
% 	\fig{opticalillusions1.eps}
% }

%-------------------------------------------------------------------
\frame
{
	\frametitle{What is my research about?}
	\begin{itemize}
	\item <+-| alert@+> Semantic annotation of urban scenes
	\item <+-| alert@+> Describe the scene on a higher level
	\item <+-| alert@+> Based on 2D input images of different views
		\begin{itemize}
			\item <+-| alert@+> Skyline detection
		\end{itemize}
	\end{itemize}
}

\frame
{
	\fig{outputSkylineSpil-Im29.eps}%{width=200px}
}


\frame
{
	\frametitle{Where is my research about?}
	\begin{itemize}
	\item Semantic annotation of urban scenes
	\item Describe the scene on a higher level
	\item Based on 2D input images of different views
		\begin{itemize}
			\item Skyline detection
			\item <+-| alert@+> 3D building reconstruction
		\end{itemize}
	\end{itemize}
}

\frame
{
	\fig{3dModel}% {width=200px}
}
\frame
{
	\frametitle{Where is my research about?}
	\begin{itemize}
	\item Semantic annotation of urban scenes
	\item Describe the scene on a higher level
	\item Based on 2D input images of different views
		\begin{itemize}
			\item Skyline detection
			\item 3D building reconstruction
			\item <+-| alert@+> Window detection
		\end{itemize}
	\end{itemize}
}
%-------------------------------------------------------------------
\frame
{
	\frametitle{Application examples of semantic annotation of urban scenes}
	%What can we do with this annotation of urban scenes?}
	\begin{itemize}
	\item <+-| alert@+> 3D city models
	\end{itemize}
}

\frame
{
	\fig{3dcitymodels0.eps}
}

\frame
{
	\fig{3dcitymodels1.eps}
}

\frame
{
	\fig{3dcitymodels3.eps}
}

\frame
{
	\fig{3dcitymodels4.eps}
}

\frame
{
	\frametitle{Application examples of annotation of urban scenes}
	%What can we do with this annotation of urban scenes?}
	\begin{itemize}
	\item 3D city models
	\item <+-| alert@+> Driving simulation
	\item <+-| alert@+> Building recognition
	\item <+-| alert@+> Augmented reality
	\end{itemize}
}

\frame
{
	\fig{augmentedReality.eps}
}
\frame
{
	\fig{augmentedReality2.eps}
}

\frame
{
	\frametitle{Application examples of annotation of urban scenes}
	%What can we do with this annotation of urban scenes?}
	\begin{itemize}
	\item 3D city models
	\item Driving simulation
	\item Building recognition
	\item Augmented reality
	%todo image
	\item <+-| alert@+> Analysis building deformation
		\begin{itemize}
		\item <+-| alert@+> 	'noord-zuidlijn'
		\end{itemize}
	\end{itemize}
}

\section{Skyline detection}
%-------------------------------------------------------------------
\frame
{
	\frametitle{Skyline detection application example}
	\begin{itemize}
		\item <+-| alert@+> Horizon detection for Unmanned Air Vehicles
		%notes:used to stabilize the vehicle
		% todo vet plaatje op google vinden van een uav
		\fig{p_uav_skyline_all.eps}{width=420px}
	\end{itemize}
}

\frame
{
	\frametitle{Skyline detection in urban scenes}% the method
	\fig{outputSkylineIm3-3.eps} %{The output of the skyline detector on the \emph{Floriande} dataset. The skyline elements are marked red.}{0.45}
}
%-------------------------------------------------------------------
\frame
{
	\frametitle{Skyline detection in urban scenes}% the method
	% ----------- notes ----------------
	%we use it to detect the contour of the building
	% ----------------------------------
	\begin{itemize}
		\item <+-| alert@+> Canny edge detection (based on intensity change)
		\item <+-| alert@+> Result: binary image (edge or no edge)
		\fig{e_floriande_canny_050.eps}
		\fig{e_floriande_canny_050.eps}

	\end{itemize}
}


%-------------------------------------------------------------------
\frame
{
	\frametitle{Skyline detection algorithm}% the method
	\begin{itemize}
		\item <+-| alert@+> Which edge represents the building contour?
		\item <+-| alert@+> Top sharp edge assumption\\
		\emph{"The first sharp edge (seen from top to bottom) in the image represents the skyline."}
		\item <+-| alert@+> Algorithm:
			\begin{itemize}
			\item <+-| alert@+> Apply gaussian smoothing
			% ------- notes -----------------------------
			% assume the buildingwall to be a sharp edge
			% to remove vague edges and make sharp edges stand out more
			% ------------------------------------------
			\item <+-| alert@+> The image is sliced in \#w pixel columns
			\item <+-| alert@+> Each column present \#h binary edge values (edge or no edge)
			\item <+-| alert@+> y-location of the most upper edge value is stored 
			\end{itemize}
	\end{itemize}
}



%-------------------------------------------------------------------
\frame
{
	\frametitle{Skyline detection result}% the method
	\fig{outputSkylineIm3-3.eps} %{The output of the skyline detector on the \emph{Floriande} dataset. The skyline elements are marked red.}{0.45}
}

\frame
{
	\frametitle{Skyline detection result}% the method
	\fig{outputSkylineSpil-Im29.eps} % {The output of the skyline detector on the \emph{Bram} dataset. The skyline elements are marked red.}{0.3}
}

%\subsection{Future research}
%-------------------------------------------------------------------
\frame
{
	\frametitle{Future research}
}

\frame
{
	\frametitle{Hypothesis based skyline detection, assumption}
	\emph{"The first sharp edge (seen from top to bottom) in the image represents the skyline."}
	\begin{itemize}
	\item <+-| alert@+> Example of a scene where this assumption is violated
	\end{itemize}
	\fig{outputSkylineSpil-Im13-thresh030.eps}
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Hypothesis based skyline detection, assumption}
	\begin{itemize}
	\item <+-| alert@+> Change of assumption
	\end{itemize}
	\emph{"The skyline is part of the first $n$ sharp edges (e.g.  $n=3$)}
	\fig{folkert_edgeHypothesis.eps}
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Hypothesis based skyline detection, algorithm}
	\begin{itemize}
	\item <+-| alert@+> Generate $n$ hypothesis
	\item <+-| alert@+> Classify hypothesis with additional info
		\begin{itemize}
		\item <+-| alert@+> texture 
		\item <+-| alert@+> color
		\item <+-| alert@+> height variation 
		\end{itemize}
	\fig{outputSkylineSpil-Im13-thresh030.eps}
	\end{itemize}
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Expected result on hypothesis classification based on color}
	\fig{outputSkylineSpil-Im13-thresh070.eps}
	\fig{outputSkylineSpil-Im13-thresh070.eps}
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Expected result classification based on height variation}
	\fig{outputSkylineIm3-3-zoom.eps}
}
\frame
{
	\frametitle{Expected result classification based on height variation}
	\fig{outputSkylineIm3-3-zoom-con.eps}
}

\frame
{
	\frametitle{Conclusion}
	\begin{itemize}
	\item <+-| alert@+> The skyline detection algorithm 
		\begin{itemize}
		\item <+-| alert@+> is simple and has a low complexity
		\item <+-| alert@+> works well under the assumption skyline is upper edge
		\item <+-| alert@+> needs future research
			\begin{itemize}
			\item <+-| alert@+> can provide a set of hypothesis which should be evaluated using additional features
			(e.g. color and height variation)
			\end{itemize}
		%\item <+-| alert@+>
		\end{itemize}
	\end{itemize}
}

	%todo close up skyline above sky below bricks
	%todo close up skyline above sky below sky (lamp) 


	%notes
	%The existing column based approach could be used to generate $n$ hypotheses and we can build
	%an algorithm on top of that to classify which hypothesis is right (i.e. which edge
	%presents the skyline).
	%
	%The hypothesis classifier must gather additional information. We could
	%discriminate the hypothesis for example by texture, color distributions or height
	%variation. We discuss the last two.


\section{Extracting the 3D model}
%-------------------------------------------------------------------
\frame
{
	\frametitle{Overview}
	%notes a skyline detection application
	\begin{itemize}
	\item <+-| alert@+> Create (top view) 2D model of the scene using \emph{Openstreetmap}
	\item <+-| alert@+> Align 2D model with the scene 
	\item <+-| alert@+> Transform the 2D model to a 3D model by extending the walls
	\end{itemize}
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Extract 2D model}
	\fig{openstreetmap}
	%--------notes----------------------------------------
	% whiteboard draw 2d model
	%-----------------------------------------------------
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Align 2D model}
	\begin{itemize}
	\item <+-| alert@+> Isaac Esteban's \emph{FIT3D toolbox}
	% out of the scope, is explained in my thesis
		\begin{itemize}
		\item <+-| alert@+> Input: sequence of images (different views of the building)
		\item <+-| alert@+> Output: a 3D point cloud of the building
		\end{itemize}
	\figCustom{FIT3DImgSequence.eps}{width=200px}
	\end{itemize}
}

\frame
{
	\fig{pc_3d.eps} %{ 3D point cloud of the walls of the building}{0.5}
}



%-------------------------------------------------------------------
\frame
{
	\frametitle{Align 2D model}
	\fig{pc_topview.eps} %{The projected 3D point cloud of the walls of the
}


%-------------------------------------------------------------------
\frame
{
	\frametitle{Align 2D model}
	\fig{pc_topview_linefit.eps} %{M2, The fitted line segments define (a top view of
	%---------------- notes ------------------------------
	% explain point to point correspondence 
	%-----------------------------------------------------
}


%-------------------------------------------------------------------
\frame
{
	\frametitle{Align 2D model}
	\fig{pc_topview2DModel.eps} %{The 2D model M1 aligned with the fitted line
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Results}
	\fig{3dModel} %{The basic 3D model, generated by extending the 
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Wall height estimation}
	% -------------- notes -------------------
	% use this as a motivation for skyline detection
	% -----------------------------------------
	\begin{itemize}
	%---------------- notes ------------------------------
	% because 3D model based on aerial images (openstreetmap)
	% -> Wall heights of 3D model are not accurate
	% explain point to point correspondence 
	% although it looks like a nice 3D model the wall heights..
	%-----------------------------------------------------
	%
	\item <+-| alert@+> Wall heights of 3D model are not accurate
	\item <+-| alert@+> Improve the 3D model by wall height estimation
	\figsHor{3dModel}{floriande_back.eps} %{The walls of the building differ in height: the back
	\end{itemize}
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Skyline detection for wall height estimation}
	%---------------- notes ------------------------------
	% Because we want to estimate the height of the building walls of the 3D
	% model, we need to know how high the walls in the 2D images are. 
	%---------------- notes ------------------------------
	\begin{itemize}
		\item <+-| alert@+> Straight lines assumption:
		\begin{itemize}
		\item <+-| alert@+> \emph{Straight lines in the skyline are likely to come from the
		building contour}
		\end{itemize}
		\item <+-| alert@+> Flat roof assumption:
		\begin{itemize}
		\item <+-| alert@+> \emph{building contour is equal to upper side of building walls}
		\end{itemize}
	\end{itemize}
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Extracting line segments}
	% a widely used method for extracting line segments is the Hough transform
	\begin{itemize}
	\item <+-| alert@+> Output of skyline detector
	\item <+-| alert@+> Hough transform
	\end{itemize}
	%---------------- notes ------------------------------
	% the technique behind this method is discussed in detail in my thesis
	% unfortunately, for now we don't have enough time to dive in to this topic
	%---------------- notes ------------------------------

	%\item <+-| alert@+> Basic idea: Lines -> points in Hough space
	%\item <+-| alert@+> Straight line segment that spans $y=mx+b$ represents $(m,b)$ point in Hough parameter space

	%---------------- notes ------------------------------
	% Because the algorithm detects straight lines containing only skyline pixels
	% it returns only the straight parts of the skyline.
	%---------------- notes ------------------------------
	\fig{outputHoughlines2d} %{Three best ranked lines of the Hough transform on the skyline detector match the three most prominent displayed building walls}{0.4}
}


%-------------------------------------------------------------------
\frame
{
	\frametitle{Project to the 3D model}
	\begin{itemize}
	\item <+-| alert@+> Estimate wall heights 
	\item <+-| alert@+> Project line segments to specific walls of 3D model
	\fig{skyline_proj.eps} %{The Houghlines collected from different views projected on their corresponding
	%---------------- notes ------------------------------
	% From image point (2D) to possible points in scene (3D)} 
	%---------------- notes ------------------------------
	\end{itemize}
}


% %-------------------------------------------------------------------
% \frame
% {
% 	\fig{coordinateSystemsCopy.eps} %{The blue line spanned by the camera center
% 	%\fignocaption{overlaytypes}{}{0.37}
% }


%-------------------------------------------------------------------
\frame
{
	\frametitle{Results}
	\fig{outputHoughlines3d} %{The Houghlines collected from different views re-projected on the 3D model according to the line-wall correspondence.}{0.5}
}


%-------------------------------------------------------------------
\frame
{
	\frametitle{Results}
	\fig{outputMutateBuilding}{Improved 3D model}{0.6}
}

%-------------------------------------------------------------------

\frame
{
	\frametitle{Future research}
	\fig{typesOfRoofs}{Different types of roofs}{0.4}
}

\frame
{
	\frametitle{Future research}
	\fig{gableRoofWood.eps}{The parts of the 3D model for the Gable roof}{0.6}
}

%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------

\section{Window detection}
%-------------------------------------------------------------------
\frame
{
\figsHor{p_window_nice_geometry1.eps}{p_window_nice_geometry2.eps}
}

\frame
{
	\frametitle{Outline window detection}
	%---------------- notes ------------------------------
	%recall that this thesis was about annotation of urban scenes
	%we annotated the 3D structure of the scene
	%next topic is window detection
	%---------------- notes ------------------------------
	\begin{itemize}
	\item Method I: Connected corner approach 
		\begin{itemize}
		\item invariant to viewing direction
		\end{itemize}
	\item Facade rectification 
	\item Method II: Histogram based approach
		\begin{itemize}
		\item frontal view only
		\item uses histograms of Houghlines
		\end{itemize}
	\end{itemize}
	% this will become clear when I explain it 
}

%-------------------------------------------------------------------
\frame
{
	\frametitle{Method I: Connected corner approach}
}


%-------------------------------------------------------------------
\frame
{
	\frametitle{Situation}
	\begin{itemize}
	%---------------- notes ------------------------------
	%notes Window contains sub windows enclosed in rectangular window frames 
	%As the color of the window frames differ from the glass, the amount of
	%horizontal and vertical edges is large at these locations.  
	%Some horizontal and vertical edges come from the same window frame, therefore they often
	%---------------- notes ------------------------------
	\item <+-| alert@+> Window = frame + glass (+ sub windows)
	%----------------- notes ------------------------
	% dutch word is kozijn
	%------------------------------------------------
	\item <+-| alert@+> Color difference, frame, glass
	\item <+-| alert@+> Produce edges in horizontal and vertical direction
	\end{itemize}
}

\frame
{
	\frametitle{Original}
	\fig{w_Dirk6_ImOri.eps} 
	%--------------- notes --------------------------------
	% scene contains an everyday scene:
	% angle in viewing direction
	% different window types
	% light spot
	% occluding tree
	% bicycles 
	%--------------- notes --------------------------------
}

\frame
{
	\frametitle{Edge detection}
	\fig{w_Dirk6_ImEdge.eps}{Edge detection}{0.45}
}


%------------- notes -------------------
% how do we detect windows from this image
%---------------------------------------

%-------------------------------------------------------------------
\frame
{
	\frametitle{The idea}
	\begin{itemize}
	\item <+-| alert@+> \emph{"horizontal and vertical edges that come from the same (sub)window frame share a corner"}
	\end{itemize}
}


%-------------------------------------------------------------------
\frame
{
	\frametitle{Edge detection and Houghline extraction}
	%---------------- notes ------------------------------
	%edge detection -> straight lines 
	%---------------- notes ------------------------------
	\begin{itemize}
	\item <+-| alert@+> Two groups of straight lines (Houghlines)
	\begin{itemize}
		\item $\theta$-constraint
		\item <+-| alert@+> Horizontal, $\theta = [-30..0..30)$ degrees
		%---------------- notes ------------------------------
		% theta = 0, horizontal   
		%---------------- notes ------------------------------
		\item <+-| alert@+> Vertical, $\theta = [80..90..100)$ degrees
	\end{itemize}
	\item <+-| alert@+> Why the use of angle ranges?
	\begin{itemize}
		\item <+-| alert@+> Camera not exactly upright
		% (camera's roll factor)
		\item <+-| alert@+> Facade view contains an angle, perspective distortion 
		%(camera's yaw factor)
	\end{itemize}
	\end{itemize}

}

\frame
{
	\frametitle{Original}
	\fig{w_Dirk6_ImOri.eps} 
	%--------------- notes --------------------------------
	% scene contains an everyday scene:
	% angle in viewing direction
	% different window types
	% light spot
	% occluding tree
	% bicycles 
	%--------------- notes --------------------------------
}

\frame
{
	\frametitle{Edge detection}
	\fig{w_Dirk6_ImEdge.eps}{Edge detection}{0.45}
}

\frame
{
	\frametitle{Result of $\theta$ constrained Hough transform}
	\fig{w_Dirk6_ImHoughResult.eps}
}

\frame
{
	\frametitle{Connected corner}
	\begin{itemize}
	\item <+-| alert@+> horizontal and vertical edge share same corner e.g. L
	\item <+-| alert@+> If $P_i$, $P_e$ are close enough form connected corner
	\item <+-| alert@+> Tolerate gab or extension
	\figCustom{cCornerTypes}{width=250px} %{First row: different type of connected corner candidates. Second row: the
	%--------------- notes --------------------------------
	% to form and clean up the connected corners..
	%--------------- notes --------------------------------
	\end{itemize}
}

\frame
{
	\fig{w_Dirk6_ImHoughResult.eps}
}

\frame
{
	\frametitle{Connected Corners}
	\fig{w_Dirk6_ImcCorner_cCorner.eps} %{Found connected corners}{0.45}
	%--------------- notes --------------------------------
	% most of the connected corners are found to form and clean up the connected corners..
	%--------------- notes --------------------------------

}

\frame
{
	\frametitle{Window area extraction}
	\begin{itemize}
	\item <+-| alert@+> How to extract the window areas?
	\item <+-| alert@+> Mirror from diagonal through endpoints
	\item <+-| alert@+> L-shapes becomes quadrangle shaped window areas
	\end{itemize}
	%--------------- notes --------------------------------
	% whiteboard : draw L and rectangle with mirror line
	% if we overlap these areas we get this result
	%--------------- notes --------------------------------
}

\frame
{
	\frametitle{Results}
	\fig{w_Dirk6_ImcCorner_windowFilled.eps} %{Window regions}{0.45}
}

\frame
{
	\frametitle{Future research}
	\begin{itemize}
	\item <+-| alert@+> L-shapes, U-shapes
		%--------------- notes --------------------------------
		% whiteboard : draw L and rectangle with mirror line
		% if we overlap these areas we get this result
		%--------------- notes --------------------------------
	\item <+-| alert@+> Analysis of substructure of windows
	\begin{itemize}
		%\item <+-| alert@+> Define \emph{contained in} relations
		\item <+-| alert@+> Cluster connected corner on location and length
		\item <+-| alert@+> Close connected corners of same size will be grouped
		\item <+-| alert@+> Maximum inter-cluster distance correlates with size sub window
		\item <+-| alert@+> Assume nr of sub windows to estimate 
		\begin{itemize}
			\item max and min size of window
			\item number of clusters
			\item max inter-cluster distance
		\end{itemize}
	\end{itemize}
	\end{itemize}
}





%\subsection{Method II: Histogram based approach} 
\frame
{
	\frametitle{Method II: window detection}
	\begin{itemize}
	\item <+-| alert@+> Assumes that the windows are
	\begin{itemize}
		\item <+-| alert@+> orthogonal
		%--------------- notes --------------------------------
		% orthogonal sides
		% angle between the horizontal and vertical parts is 90 degrees
		%--------------- notes --------------------------------
		\item <+-| alert@+> aligned
		%--------------- notes --------------------------------
		% certain pattern in the windows
		% they are lined up in both horizontal as vertical direction
		% e.g. a row of windows share the same height
		%--------------- notes --------------------------------
	\end{itemize}
	\end{itemize}
		%--------------- notes --------------------------------
		%I exploited these properties to build a robust window detector
		%--------------- notes --------------------------------

}

%\subsection{Facade rectification}
\frame
{
	\frametitle{Aligned but not orthogonal}
	\fig{w_spil6ImOri.eps} %{Dataset: Anne1, Original, (unrectified) image}{0.45}
}

\frame
{
	\frametitle{Facade rectification}
	\begin{itemize}
		\item <+-| alert@+> 3D model of building gives plane that corresponds to wall
		\item <+-| alert@+> plane's normal vector:  $b$
		\item <+-| alert@+> (FIT3D) Camera's heading: $a$
		\item <+-| alert@+> if $a || b$, view is frontal
		\item <+-| alert@+> Rotation matrix R is calculated and applied
			\begin{itemize}
			\item <+-| alert@+> orthogonal rotation vector $a$ x $b$
			\item <+-| alert@+> angle $\theta$
			\end{itemize}
		%--------------- notes --------------------------------
		%by calculating angle diff of $a$ and $b$
		%apply on all points
		%--------------- notes --------------------------------
		\figCustom{axisAnglePresentation.eps}{scale=0.1}%{$\vec{a}$ is the camera's heading and $\vec{b}$
	\end{itemize}
}


\frame
{
	\frametitle{Unrectified facade}
	\fig{w_spil6ImOri.eps} %{Dataset: Anne1, Original, (unrectified) image}{0.45}
}

\frame
{
	\frametitle{Rectified facade}
	\fig{w_Spil1TransCrop1_ImOri.eps} %{Dataset: Anne1, Rectified image}{0.45}
}

%--------------- notes --------------------------------
% we can also apply this on our other dataset
%--------------- notes --------------------------------

\frame
{
\fig{w_Dirk4_ImOri_Unrectified.eps} %{Dataset: Dirk, Original, (unrectified) image, }{0.45}
}

\frame
{
\fig{w_Dirk4Trans_ImOri.eps} %{Dataset: Dirk}{0.55}
}


\frame
{
	\frametitle{Extracting the window alignment}
	\begin{itemize}
	\item Window alignment line
	\begin{itemize}
		\item <+-| alert@+> \emph{"A horizontal or vertical line that aligns multiple windows"}
	\end{itemize}
	\item <+-| alert@+> Alignment lines provide grid of blocks
	\item <+-| alert@+> Window, non-window areas
	\end{itemize}
	
}


\frame
{
	\frametitle{Window alignment lines example}
\fig{w_Spil1TransCrop1_ImHibaap.eps} %{Dataset: Anne1, Regular window alignment (parallel projection): Based on a smoothed histogram (red line) that displays the amount of overlapping Hough lines, for the column division the horizontal Hough lines are counted (at each $y$ position), for the row division the vertical Hough lines are counted (at each $x$ position) }{0.45}
}

\frame
{
	\frametitle{The idea}
	\begin{itemize}
	\item <+-| alert@+> Amount of Houghlines high at window locations
	\end{itemize}
	\fig{w_Spil1TransCrop1_ImHoughResult.eps}%{Dataset: Anne1, (Projected) Hough lines on rectified image}{0.45}
}

\frame
{
	\frametitle{The algorithm for vertical alignment (columns)}
	%----------------------- notes --------------------
	%----------------------- notes --------------------
	\begin{itemize}
	\item <+-| alert@+> Isolate \emph{vertical} Hough lines
	\item <+-| alert@+> Extract pixel coordinates of endpoints 
		\begin{itemize}
		\item <+-| alert@+> set of (2 dimensional) coordinates (x,y)
		\end{itemize}
	%----------------------- notes --------------------
	% where h is the number of horizontal lines
	%----------------------- notes --------------------
	\item <+-| alert@+> Discard least informative dimension
		\begin{itemize}
		\item <+-| alert@+> the height is irrelevant for column division
		\item <+-| alert@+> project to x-axis (by discard y-value) 
		\item <+-| alert@+> gives a set of (1 dimensional) x-values 
		\end{itemize}
	%----------------------- notes --------------------
	% where each scalar value presents a x coordinate
	%----------------------- notes --------------------
	\item <+-| alert@+> Create histogram: count number of values on each x-position
	%----------------------- notes --------------------
	% where w is the width of the image
	%----------------------- notes --------------------
	\end{itemize}
}

\frame
{
	\frametitle{Window alignment lines}
	%----------------------- notes --------------------
	% Peaks are located at position where an increased number of Houghlines start or end
	%----------------------- notes --------------------
	\fig{w_Spil1TransCrop1_ImHibaap.eps} %{Dataset: Anne1, Regular window alignment (parallel projection): Based on a smoothed histogram (red line) that displays the amount of overlapping Hough lines, for the column division the horizontal Hough lines are counted (at each $y$ position), for the row division the vertical Hough lines are counted (at each $x$ position) }{0.45}
}

\frame
{
	\frametitle{Window alignment lines}
	%----------------------- notes --------------------
	% Peaks are located at position where an increased number of Houghlines start or end
	%----------------------- notes --------------------
	\figCustom{w_Spil1TransCrop1_ImHibaapZoom.eps}{width=200px} %{Dataset: Anne1, Regular window alignment (parallel projection): Based on a smoothed histogram (red line) that displays the amount of overlapping Hough lines, for the column division the horizontal Hough lines are counted (at each $y$ position), for the row division the vertical Hough lines are counted (at each $x$ position) }{0.45}
}


% \frame
% {
% 	\frametitle{Peak (area) extraction}
% 	\begin{itemize}
% 	\item <+-| alert@+> Smooth histogram function 
% 	\item <+-| alert@+> Relative threshold, 0.5 * max peak, defines peak areas
% 	\item <+-| alert@+> Locate maximum for each peak area
% 	\item <+-| alert@+> Draw alignment line on peaks
% 	%----------------------- notes --------------------
% 	% exploit concave shape of the peak area
% 	%----------------------- notes --------------------
% 	\fig{w_Spil1TransImproved_XvCrop.eps} %{Dataset: Anne2, Regular window alignment: based on a histogram that displays amount of overlapping vertical Hough lines at each $x$ position}{0.7}
% 	\end{itemize}
% }


\frame
{
	\frametitle{Reasons for improvement}
	\begin{itemize}
	\item Window alignment at wrong locations
	\item Right side of window frame of first 4 columns not found
	\item Main reason: perspective distortion creates occlusion effect
	\fig{w_Spil1TransImproved_XvCrop.eps} %{Dataset: Anne2, Regular window alignment: based on a histogram that displays amount of overlapping vertical Hough lines at each $x$ position}{0.7}
	\end{itemize}
}

\frame
{
	\fig{w_spil6ImOri.eps} %{Dataset: Anne1, Original, (unrectified) image}{0.45}
}

\frame
{
	\frametitle{Alternative window alignment}
	\begin{itemize}
	\item <+-| alert@+> Previous method: 
		\begin{itemize}
		\item <+-| alert@+> Line segment endpoints
		\item <+-| alert@+> Vertical lines - window columns
		\item <+-| alert@+> Histogram peak detection
		\end{itemize}
	\item <+-| alert@+> Alternative method: 
		\begin{itemize}
		\item <+-| alert@+> Entire line segment
		%------------- notes ----------------
		% this means longer lines have more influence which is desirable
		%------------- notes ----------------
		\item <+-| alert@+> Horizontal lines - window columns
		\item <+-| alert@+> Histogram shape analysis
		\end{itemize}
	\end{itemize}
}

\frame
{
	\frametitle{Alternative window alignment}
	\begin{itemize}
	\item <+-| alert@+> Idea
		\begin{itemize}
		\item <+-| alert@+> occlusion doesnt affect horizontal edges 
		\item <+-| alert@+> on vertical alignment positions appears a big increase/decrease of horizontal lines 
		\item <+-| alert@+> take this increase/decrease into account
			\begin{itemize}
			\item <+-| alert@+> instead of amount of Houghlines
			\end{itemize}
		\item <+-| alert@+> use entire line
			\begin{itemize}
			\item <+-| alert@+> instead of only the endpoints
			\end{itemize}
	%------------- notes ----------------
	% as the window starts/end
	% name subwindows
	%------------- notes ----------------
		\end{itemize}
	\item <+-| alert@+> $X_{h}$ defines number of overlapping horizontal lines on each x position
	\item <+-| alert@+> peak function \[D = abs( X_{h}')\]
	%------------- notes ----------------
	% deravitive because increase/decrease
	% abs to be big at both 
	%------------- notes ----------------
	\end{itemize}
}

\frame
{
\fig{w_Spil1TransImproved_Xh.eps}%{Dataset: Anne2, Alternative window alignment (orthogonal projection): Based on the shape of the smoothed histogram function. 
}




\frame
{
	\frametitle{Fusing the window alignment methods}
	\begin{itemize}
	\item <+-| alert@+> Plot both methods
	\item <+-| alert@+> Increase threshold (less but more certain alignment lines)
	\item <+-| alert@+> Peak found by both methods?
		\begin{itemize}
		\item <+-| alert@+> Merge peaks that are close
		\end{itemize}
	\end{itemize}
}

\frame
{
	\fig{w_Spil1TransImproved_Xv.eps} %{Dataset: Anne2, Regular window alignment: based on a histogram that displays amount of overlapping vertical Hough lines at each $x$ position}{0.7}
%------------- notes -------------------
% note that the right side of the windows is now correct detected
%------------- notes -------------------
}


\frame
{
	\fig{w_Spil1TransImproved_Xvh.eps} %{Dataset: Anne2, Regular and alternative window alignment combined}{0.7}
}

\frame
{
	\frametitle{Other dataset}
}

\frame
{
	\fig{w_Dirk4_ImOri_Unrectified.eps} %{Dataset: Dirk, Original, (unrectified) image, }{0.45}
}

\frame
{
	\fig{w_Dirk4Trans_ImOri.eps}% {Dataset: Dirk}{0.55}
}


\frame
{
	\fig{w_Dirk4Trans_ImHibaap_Xvh.eps} %{Dataset: Dirk, Regular and alternative window alignment combined}{0.45}
}


\frame
{
	\frametitle{Window classification}
	\begin{itemize}
	\item <+-| alert@+> Alignment lines give grid of blocks
	\item <+-| alert@+> Classify blocks 
		\begin{itemize}
		\item <+-| alert@+> window or non-window block
		\end{itemize}
	\item <+-| alert@+> Two methods:
		\begin{itemize}
		\item <+-| alert@+> Basic classification 
			\begin{itemize}
			\item <+-| alert@+> based on amount of Houghline endpoints
			\end{itemize}
		\item <+-| alert@+> Improved classification 
			\begin{itemize}
			\item <+-| alert@+> based on shape of histogram function
			%------------ notes ---------------
			% similar to window alignment method we just discussed 
			%------------ notes ---------------
			\end{itemize}
		\end{itemize}
	\end{itemize}
}

\frame
{
	\frametitle{Basic window classification assumptions}
	\begin{itemize}
	\item <+-| alert@+> Assumptions:
		\begin{itemize}
		\item <+-| alert@+> a block contains a high amount of Houghlines: window
		\item <+-| alert@+> a block contains low amount of Houghlines: non-window
		\end{itemize}
	\item <+-| alert@+> The windows are aligned 
		\begin{itemize}
		\item <+-| alert@+> a row or column contains either zero windows or multiple windows 
		\end{itemize}
	\end{itemize}
}

\frame
{
	\frametitle{Basic window classification algorithm}
	\begin{itemize}
	%------------ notes ---------------
	% instead of individual block classifaction..
	% more accurate
	%------------ notes ---------------
	\item <+-| alert@+> Exploit window alignment: 
		\begin{itemize}
		\item classify full block rows and block columns
		\end{itemize}
	\item <+-| alert@+> Count the nr of Houghline endpoints in each blockrow
	\item <+-| alert@+> discard blockrow size influence (normalize):
	\item <+-| alert@+> \[\forall Ri\in \{1..numRows\} : R_i = \frac{HoughlinePxCount}{R_i^{width} \cdot R_i^{height}}\]
	\item <+-| alert@+> Output: $||R||$ scalar values that ranks a row being window area or not
	\end{itemize}
	

}

\frame
{
	\fig{w_Spil1TransCrop1_ImClassRectBarh.eps} %{Classification values for window
}

\frame
{
	\fig{w_Spil1TransCrop1_ImOri.eps} 
}

\frame
{
	\fig{w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps} %{Dataset: Anne1, Basic window classification method:Measure of certainty, 
}



\frame
{
	\frametitle{From certainty to classification}
	\begin{itemize}
	% as we cope with two clusters non window and window areas
	\item <+-| alert@+> $k$-means classification on R and C values

	%------------- notes -----------------
	% how do we determine which value is classified as high
	%------------- notes -----------------
	\end{itemize}
}

\frame
{
	\figCustom{w_Dirk4Trans_ImClassRectI_kmeansLuck_barsV.eps}{width=250px}
}

\frame
{
	\figCustom{w_Dirk4Trans_ImClassRectI_kmeansLuck_barsH.eps}{width=250px}
}

\frame
{
	\fig{w_Spil1TransCrop1_ImClassRect.eps} %{Dataset: Anne1, Basic window classification method: The extracted windows, red:the grouping}{0.65}
}

\frame
{
	\frametitle{Improved window classification}
	\begin{itemize}
	\item <+-| alert@+> Use idea of alternative alignment
	\item <+-| alert@+> Amount of Houghlines 
		\begin{itemize}
		\item <+-| alert@+> increases towards center of window area
		\item <+-| alert@+> decreases towards center of non-window area
		\end{itemize}
	\item <+-| alert@+> Shape concave - window area
	\item <+-| alert@+> Shape convex - non-window area
	\end{itemize}
}
\frame
{
	\fig{w_Spil1TransImproved_WAlinesMerged.eps} %{Dataset: Anne2, Improved window
}

% \frame
% {
% 	\frametitle{Algorithm}
% 	\begin{itemize}
% 	\item <+-| alert@+> Detect concave and convex shape of histogram function $Xh$
% 	% derivative
% 	\item <+-| alert@+> $X_{h}'$ 
% 	\item <+-| alert@+> Sign change of derivative indicates peak or valley
% 	\item <+-| alert@+> (+,-) $\rightarrow$ concave $\rightarrow$ window area
% 	\item <+-| alert@+> (-,+) $\rightarrow$ convex $\rightarrow$ non-window area
% 	\end{itemize}
% }

\frame
{
	\frametitle{Result}
	\fig{w_Spil1TransImproved_windowsGrouped.eps} %{Dataset: Anne2, Improved window classification method: The extracted windows, red:the grouping}{0.7}
}

% \frame
% {
% 	\frametitle{Test on scene that violates assumption}
% 	% ------------ notes ---------------------
% 	% when something works you relax the assumptions
% 	% ----------------------------------------
% 
% 	\begin{itemize}
% 		\item Windows are \emph{partially} aligned
% 			\begin{itemize}
% 			\item unaligned doors
% 			\item inter row size difference 
% 			\end{itemize}
% 		\item Windows differ in size and type
% 	\end{itemize}
% 	\figCustom{w_Dirk4Trans_ImOri.eps}{width=200px} %{Dataset: Dirk, Improved window
% }
% \frame
% {
% 	\fig{w_Dirk4Trans_ImHibaap_WAlinesMerged.eps} %{Dataset: Dirk, Improved window
% }
% 
% \frame
% {
% 	\fig{w_Dirk4Trans_ImClassRect.eps} %{Dataset: Dirk, Improved window classification method: The extracted windows, red:the grouping}{0.40}
% }

\frame
{
	\frametitle{Conclusion:Connected corner}
}

\frame
{
	\frametitle{Conclusion:Connected corner}
	\fig{w_Dirk6_ImcCorner_cCorner.eps} % {Found connected corners}{0.45}
}

\frame
{
	\frametitle{Conclusion:Connected corner}
	\fig{w_Dirk6_ImcCorner_windowFilled.eps} %{Window regions}{0.45}
}

%------------------- notes ----------------------
% start with individual conclusions
%------------------------------------------------

\frame
{
	\frametitle{Conclusion:Connected corner}
	\begin{itemize}
	\item <+-| alert@+> Detection rate 97\%
	\item <+-| alert@+> Suitable for scenes with variation in window size/type
	\item <+-| alert@+> Small requirement on input data
		\begin{itemize}
		\item <+-| alert@+> neither 3D information about the building 
		\item <+-| alert@+> nor rectification is needed
		\end{itemize}
	\item <+-| alert@+> Future research
		\begin{itemize}			
		\item <+-| alert@+> U shapes 
		\item <+-| alert@+> Analysis of sub window structure
		\end{itemize}			
	\end{itemize}
}

\frame
{
	\frametitle{Conclusion:histogram based window alignment}		

	\begin{itemize}
	\item <+-| alert@+> Interpretation amount of Houghlines = strong approach towards window
	detection
	\item <+-| alert@+> Two window detection methods
	\item <+-| alert@+> Alternative window alignment performs better, used:
		\begin{itemize}
		\item <+-| alert@+> a strong combination horizontal and vertical Houghlines histograms
		\item <+-| alert@+> high order (derivative) shape interpretation of the histogram function
		\end{itemize}
	%todo tabel
	\end{itemize}

}
\frame
{
	\frametitle{Conclusion:window classification}		
	\begin{itemize}
	\item <+-| alert@+> Method I: based on amount of Houghlines
		\begin{itemize}
		\item <+-| alert@+> performs quite good on the dataset we used
		\item <+-| alert@+> very sensitive to alignment errors
		\item <+-| alert@+> (errors in alignment propagate to classification)
		\end{itemize}
	\item <+-| alert@+> Method II:based on shape of Houghlines histogram
		\begin{itemize}
		\item <+-| alert@+> based on strong increase/decrease of Houghlines
		\item <+-| alert@+> very robust, at least 97\% detection rate on all datasets, why?
		\item <+-| alert@+> robust to alignment errors (works on center area)
		\item <+-| alert@+> higher order interpretation of histogram function
		% ------------ notes ----------------
		%	as long al the middle points fall in window non-window area
		% ------------------------------------
		\item <+-| alert@+> the increase/decrease is relative
		\item <+-| alert@+> robust to variation in window type and illumination conditions
		\end{itemize}
	\end{itemize}
}

\frame
{
	\frametitle{Conclusion}		

	\begin{itemize}
	\item <+-| alert@+> We retrieved different semantics of urban scenes
	\item <+-| alert@+> Similar to humans 
		\begin{itemize}
		\item <+-| alert@+> using 3D information, straight lines, right angles, etc.
		\end{itemize}
	\item <+-| alert@+> We achieved:
		\begin{itemize}
		\item <+-| alert@+> The extraction of a skyline in different urban scenes
		\item <+-| alert@+> The 3D reconstruction of a building
		\item <+-| alert@+> The extraction of windows
		\item <+-| alert@+> For a set of urban scenes with different properties 
		\end{itemize}
	\end{itemize}
%---------------- notes ------------------
% although this is nothing compared to what humans see 
% we made a valuable effort towards human-like interpretation of urban scenes
%-----------------------------------------
}


%
%	\subsubsection{Introduction}
%
%	\subsubsection{3D plane based rectification} 
%
%	\subsubsection{Results} % facade rectification
%
%	\subsection{Datasets}
%
%	\subsubsection{Anne1}
%
%	\subsubsection{Anne2}
%
%	\subsubsection{Dirk}
%
%	\subsection{Method II: Histogram based approach} 
%
%	\subsubsection{Introduction}
%
%	\subsubsection{Extracting the window alignment}
%
%	\subsubsection{Basic window classification (based on line amount)}
%
%	\subsubsection{Improved window classification (based on shape of the histogram function)}
%
%	%\subsubsection{Comparison of basic and improved window classification}
%
%	\subsection{Conclusion}
%
%	\subsubsection{Method I:Connected corner approach} % cCorner
%
%	\subsubsection{Method II: histogram based approach}









%\section{Interactive demo}
%	osgviewer demo of 3d point cloud and images
%	demo of matlab progje


%\subsection{Questions}
\frame{
\frametitle{Questions}
\fig{questions.eps}
}

\frame{
\frametitle{Questions}
\begin{LARGE}
\begin{center}
 Questions?
\end{center}
\end{LARGE}
}

\end{document}
