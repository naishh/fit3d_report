% DRAFT
\section{Preliminaries on Computer Vision}
In this chapter we discuss the basic computer vision techniques that are used for
the skyline detection, and window detection.  Furthermore we discuss 3rd
party software, the \emph{FIT3D toolbox} \cite{Fit3d} which is used in 3D building extraction and facade rectification.

\subsection{Coordination systems}
%todo doorlezen
In this thesis three different coordinate reference systems are used. 
The first one is located at the image level and describes the
location of the pixel in 2D. It is called the Image Coordinate Frame (ICF), in
Figure \ref{fig:coordinateSystems.eps} ($x_0,y_0$) span this frame.
The second is the Camera Coordinate
Frame (CCF) and it involves the projection of the image point (x,y) to the
retina of the camera in 3D.  The origin of the CCF is equal to the center of the
camera. If the camera rotates, it rotates around this point. The coordinates are
expressed in (XYZ). \\
The last system is the World Coordinate Frame (WCF) and it is used to describe the positions of the 3D model of the
scene and the position of the cameras in the world in (ijk). An overview of the
systems can be found in Table \ref{tab:coord}.

\fig{coordinateSystems.eps}{Coordinate systems}{1}
\begin{table}[ht]
	\caption{Coordinate reference systems with their properties}
	\label{tab:coord}
	\begin{tabular}{|l||c|c|c|}
	\hline
	Reference system name		&	abreviation	&	dimensions	&	axis\\
	\hline
	\hline
	Image Coordinate Frame	&   ICF			&	2D			& 	x,y\\
	\hline
	Camera Coordinate Frame	&   CCF			&	3D			& 	X,Y,Z\\
	\hline
	World Coordinate Frame	&   WCF			&	3D			& 	i,j,k\\
	\hline
	\end{tabular}
\end{table}
%todo use costins work and refer 
%todo use FIT3D toolbox paper


%  The camera can be described in the same coordinate system as the 3D point we are
%  looking for, we call this the world coordinate system. 
%  The camera center is a value in $\mathbb{R}$3 that represents the camera's position in the world
%  coordinate system. If the camera rotates, it rotates around this point.
%  \TODO{ elaborate}
%  
%  
%  %TODO IMPORTANT!!!!!!!!!!!!!!!!!!!!!!!!
%\subsection{Getting the camera centers and viewing directions}

\subsection{Camera calibration}
\label{sec:cameraCalibration}
Camera calibration is done to determine
the relationship between what appears on the image (or camera retina) and where
 it is located in the 3D world. This involves calculating the camera's intrinsic
 and extrinsic parameters.

\subparagraph{Intrinsic parameters}
The intrinsic parameters contain information about the internal parameters
of the camera.  These are focal length, pixel aspect ratio, and principal point.
%todo explain principal point

These parameters come together in a calibration matrix K.  The Floriande
dataset (originated from the Fit3D toolbox \cite{Fit3d}) comes with a
calibration matrix K.  For
the other datasets we calculated K with the Bouguet toolbox
\cite{bouguet}.
This method involves taking images of a chessboard in different positions and
orientations. An algorithm detects the grid on the chessboard and monitors its
transformations under the different images.  If enough images are given (at
least 10) and the images contain enough variety in chessboard pose, the Bouguet
toolbox can calculate the intrinsic parameters quitte accurate.
More details about this method can be found in \cite{bouguet}.

\subparagraph{Extrinsic parameters}
%todo praten over coordinaat stelsels
%\paragraph{Getting the camera centers and viewing directions}
%todo introduce viewing direction
The extrinsic parameters present the center of the locations of the camera
and the camera's headings. These are unique for every view.
%todo explain rotation, translation
In some systems these are recorded by measuring the cameras position and
headings at the scene. In other systems this is computed afterwards (from the
images).  We calculated the values also afterwards and used existing software for
this: \emph{FIT3D toolbox} \cite{Fit3d}, details of this process is
explained next.


\subsection{FIT3D toolbox \ref{fit3d}}
\label{sec:prelimFIT3D}
%todo give fit3d a proper intro
The \emph{FIT3D toolbox} \ref{fit3d} is used for several aims in this thesis.
It is used in the window detection module to rectify the facades.
and the skyline detection used \emph{FIT3D} to extract a 3D model.

In order to extract this 3D model a series of frames (originating from different
views) is used to estimate the relation of the camera coordinates to the
world coordinates, Next, the result is used to extract a point cloud of matching
features. Finally this point cloud is converted to planes which correspond with the walls of the building.

Because the toolbox plays an assisting role we explain the steps briefly.
Detailed knowledge about the methods can be found in \ref{fit3d}.

%3D model reconstruction
%	2d model reconstruction
\subsubsection{Multiple views}
\emph{FIT3D} uses multiple views to gather information about the 3D structure of the
building. The toolbox comes with a prepared dataset of 7 consecutive images (steady (zoom, lightning,
etc.) parameters) of a scene.  \emph{FIT3D} doesn't require the input images to be
chronological however they need to have sufficient overlap. 
\fig{fit3dImgSequence.eps}{Example series of 7 consecutive frames, (dataset:
\emph{FIT3D toolbox}\cite{FIT3D})}{0.3}
%/media/Storage/scriptie/fit3d/generate3dModel/3dcFiles


%todo explain what a camera coordinate is (homoge px coord)
\subsubsection{Relating the camera coordinates to the world coordinates}
%Getting the camera's centers and camera's heading (Extrinsic parameters)}
The different views are used to estimate the relation of the camera frame
coordinates to the world coordinates, (the extrinsic parameters).  In other
words the different positions of the camera centers and camera's heading are
estimated.  This is done by calculating the relative motion between the
different views.\\

The frame to frame motion is calculated by extracting about 25k SIFT features of
each frame.  Next, SIFT descriptors are used to describe and match the features
within the consecutive frames.  Not all features will overlap or match in the
frames therefor RANSAC is used to robustly remove the outliers.  After this an
\emph{8-point algorithm} together with a voting mechanism is used to extract the
relative camera motion.\\

The frames are matched one by one which returns an estimation of the camera
motion. Because this estimation is not accurate enough, a 3-frame match is done 
This result is more accurate but comes with a certain
amount of re-projection error which is minimized using a numerical
iterative method called \emph{bundle adjustment}.  \\

From every frame the camera motion is stored relative to the first frame.
The motion is stored as a rotation and translation in homogeneous
form (3x4) $[R|t]$. This is gathered for every frame in a (7x3x4) projection matrix $P$.
$P$ can be used to translate camera coordinates of a specific view 
 to 3D world coordinates and vice versa.

\subsubsection{3D point cloud extraction}
The next step is to use this projection matrix $P$ to obtain a set of 3D points.
These correspond to the matching SIFT features of the different views.  This
is achieved using a \emph{linear triangulation} method. \\
The result is an accurate 3D point cloud of the building, see Figure
\ref{fig:fit3dImgSequence.eps}.
Next a RANSAC based plane fitter is used to accurately fit planes through
the 3D points, see Figure \ref{fig:fit3d_planes.eps}.\\
\fig{fit3d_planes.eps}{Fitted planes}{0.45}

In this thesis the plane fitting step is skipped and a new method is
used to obtain the 3D model. This is explained in \ref{sec:generate3dModel}.

% werkwijze:
% FIT3D paper doornemen
% wat heb ik gedaan wat doet FIT3D?
% 	in code kijken
% generate illustrative images
% 

%----------------------------------------------------------------------



%  
%  
%  \subsection{Homogeneous coordinate}
%  \TODO{ explain how to add extra homogeneous dimension (x,y,1) and why this is}
%  
%  
%\subsection{Planes and walls}
%\paragraph{Planes and walls}
%  \label{sec:planeswalls}
%  \TODO{ How to span plane by wall coords}
%  
%\subsection{Axis angle representation}
%\paragraph{Axis angle representation}
%  \label{sec:axisAngle}
%  \TODO{Axis angle representation, wikipedia}
%  
%\subsection{Gaussian blur}
%\paragraph{Gaussian smoothing}
%  %matlab documentation, wikipedia
