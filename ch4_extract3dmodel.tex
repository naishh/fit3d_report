% todo 
% be more clear about the projection, its the first time the reader sees it 



\section{Extracting the 3D building}
%todo plaatje van scene met een 3d model erin
%todo plaatje 2d model
\label{sec:generate3dModel}
% motivation
\subsection{Introduction}
In the previous chapter we explained our skyline detection algorithm which
extracted the skyline of a scene. The output is a set of 2D points which was  
collected for a sequence of images.
The aim of this chapter is to use this set of points together with an
 aerial 2D model of the building and the 3D pointcloud obtained by the FIT3D
 toolbox\cite{fit3d} (\ref{sec:prelimFIT3D}) to generate a 3D model of the
 building.


\paragraph{Research question}
%todo uitgebreider over fit3d linken 
Is it possible to use a set of (noisy) skyline points together with an aerial
2D model and a 3D pointcloud obtained by \emph{FIT3D toolbox\cite{fit3d}} to
generate a 3D model of the building?\\

We present a stepwise solution.\\ 
First \emph{Openstreetmap} is used to obtain a 2D topview of the building (the
different parts of the 2D model represent the walls of the building). Next the
3D pointcloud obtained by the \emph{FIT3D toolbox\cite{fit3d}} is used to align this 2D
model in the scene.  After this the set of points returned by the skyline
detector is transfered to a set of lines. Then each line segment is assigned to
a part (wall) of the aligned 2D model.  After this the line segments are
projected to vertical planes spaned by the 2D model.  The result is used to
estimate the height values of the walls of the 2D model. The 2D model is
transformed according this height values to a 3D
model. We will now elaborate on each step.\\


\subsection{Method}
\subsubsection{Extracting the 2D model}
The basis of the generated 3D model is its ground-plane: a 2D model originated from
\emph{Openstreetmap}.
\fig{openstreetmap}{Openstreetmap with annotated buildings}{0.3}
\emph{Openstreetmap} (Figure \ref{fig:openstreetmap}) is a freely accessible 2D map generated by
users all over the world. It contains information about streets, building
contours, building functions, museums, etc.  We are interested in the building
contours therefor we take a snapshot of a particular area and extract this building
contour.  This is a set of ordered points where each point corresponds to a
corner of the building.  Next we link these points to line segments which
represent (the topview of) the walls of the building.

\subsubsection{Aligning the 2D model}
\fig{pc_3d.eps}{ 3D point cloud of the walls of the building}{0.5}
We want to align the 2D model extracted from \emph{Openstreetmap} in the scene at the right location with the right aspect ratios. 
From \emph{FIT3D\cite{fit3d}} we have the 3D point cloud of the building in world coordinates
(Figure \ref{fig:pc_3d.eps}).
The first challenge is to obtain a topview of the 3D point cloud.
How can we determine the direction of the top view? We want to project in the
direction that is most parallel to the walls and orthogonal to the ground-plane of the 2D model.

\paragraph{Gravity aligned walls assumption}
	\emph{The walls of the building are aligned in the opposite direction of the gravity
	which is orthogonal to the 2D basis from \textbf{Openstreetmap}}. This means
	we assume the images taken upright: the camera's $roll=0$ (Figure
	\ref{fig:cameraPitch.eps}).\\

Now we have defined the wall direction, we obtain the topview of the 3D point
cloud by discarding the y-dimension of the points.  Note that this is equivalent to a
projection to the x,z plane. The result is a set of 2D points that represent the
topview of the 3D point cloud (Figure \ref{fig:pc_topview_linefit.eps}).

We determine the walls by fitting line segments in the point cloud.  We
annotated these line segments manually.  (If the system needs to operate
automatically, RANSAC can be used to fit lines in this point cloud.)

\fig{pc_topview.eps}{The projected 3D point cloud of the walls of the
building}{0.4}
\fig{pc_topview_linefit.eps}{The top view building walls represented the fitted line
segments, M1}{0.5}
\fig{pc_topview2DModel.eps}{M2, the 2D model extracted from \emph{Openstreetmap}
aligned width the point cloud}{0.5}
\clearpage

The next step is to align the 2D \emph{Openstreetmap} model $M1$ with these line segments of the
projected pointcloud $M2$.  First the endpoints of the line segments, which
correspond to the wall corners, are extracted in both models.
Not that the walls of $M2$ that are located at the back of the building are often missing
because they are occluded by the front walls. We only consider the corners that
are present in both models.
Next the correspondences between these corners is annotated manually. 
This is used to generate a set of linear equations
which are solved in closed form \cite{hartley}. The result is a matrix $A$
which represents the rotation, translation and scaling that is needed for a
coordinate of $M1$ to be transformed to $M2$.
Finally $A$ is applied on all cornerpoints of $M1$ which results in an aligned 2D
model (Figure \ref{fig:pc_topview2DModel.eps}).

%affinetransform
%http://stackoverflow.com/questions/1856210/trying-to-derive-a-2d-transformation-matrix-using-only-the-images
%http://docs.oracle.com/javase/1.4.2/docs/api/java/awt/geom/AffineTransform.html


%todo convention matrix bold?
%------------------------------------------------------------------------
%todo example image of 3d model overlay
% of osgviewer screendpumps pakken en het erinplakken
% of building.osg in osgviewer plakken
% of in matlab de boel tegelijk plotten


%discuss disadvantage (skew Y assumption)
% see images in 
%  /media/Storage/scriptie/fit3d/results/anglebug
% formula of 3 coords of wall with an Y coord 



\subsubsection{Transfering the aligned 2D model to 3D}
Because the 2D model is based on aerial images it contains no
information regarding the height of each wall. 
In the next section we explain how we obtain the precise height values.\\

For the sake of presentation we use an average building height to generate a
rough estimate of the 3D model.  The 3D model is generated by taking 2D model
and extend it in the opposite gravity direction.  An example of the 3D model can
be seen in Figure \ref{fig:3dModel}.

\fig{3dModel}{An example of a basic 3D model, generated by extending a basis
(2D) model from \emph{Openstreetmap} to an average building height}{0.5}
%todo fit3d linken

%todo plaatje maken van aligened model: probleem plotBuilding is niet aligned
%met generate../align3dmodel



\subsubsection{Extracting line segments}
\label{extractinglinesegments}
	Because we want to estimate the height of the building walls of the 3D model we
	need to know how high the walls in the 2D images are.  To estimate this we
	first determine which part of the skyline is part of the building contour.
	Straight lines in the skyline area are likely to come from the building
	contour.  If we have a method that extracts straight line segments from the
	skyline, we can use these lines segments to estimate the height of the walls
	of the 3D model.  In this section we explain how we extracting this straight
	line segments.  \\

\paragraph{Assumptions}
	Many urban areas contain buildings with a flat roof. Therefore the contour
	of the building is always formed by a set of straight line segments.
	Furthermore the contour of the building is always aligned with the upper
	side of a building wall.  If we assume a flat roof we can find the height of
	the building walls without having to concern for (complex) roof types.

	\subparagraph{Flat roof assumption}
	\emph{We assume each building has a flat roof, implicating that the building
	contour is aligned with the topside of a building wall.  The building walls
	may have different heights but the roof should be flat.}\\


\paragraph{Hough transform}
	As was discussed in chapter \ref{sec:ch2}, a widely used method for extracting line segments is the Hough transform \cite{Hough}.
	We regard this as a suitable method because it is
	used a lot for this kind of problems. This is probably because it is unique
	in its low complexity (compared to other methods like
	\emph{RANSAC}, who often use an iterative approach).
	For a detailed explanation of the Hough transform can be found in section
	\ref{sec:prelimHough}.\\

	The input of the Hough transform that is build in in \emph{MATLAB} is a binary
	image. This is in our case the output of the skyline detector (chapter
	\ref{sec:skylinedetection}).\\
	If a pixel is classified as a skyline pixel (a pixel that lies on the
	skyline according the skyline detector), the Hough transform increases
	a vote value for every valid line $(r,\theta)$ pair that crosses this
	particular pixel.  
	Lines $(r,\theta)$ pairs that receive a large amount of votes
	contain a large amount of skyline pixels.\\
	Because the algorithm detects straight lines containing only skyline pixels
	it returns only the straight parts of the skyline.
	As these straight skyline parts resemble the building contour
	we found exactly what we where looking for.\\

	Results of the Hough transform on the output of the skyline detector are
	displayed and evaluated in the Result section (\ref{sec:ResultImprove}).

\subsubsection{Project the skyline to the 3D model}
\label{sec:project}

	% intro
	The Hough transform of the previous section returned a set of 2D line
	segments which likely present parts of the building contour.  
	As we want to estimate the building wall heights we need to determine the wall
	of the building that is most likely responsible for that line segment.
	We can solve this problem by projecting the line segments to a specific
	part of the 3D model. To 
	This is done in three steps, first the camera is calibrated, next we
	project the line segments to the the building and finally we 
	determine the specific building part that is associate with a line segment.


	To project the line segments to the 3D model we need to know where the
	camera was positioned and heading when it took the photo (extrinsic
	parameters). Furthermore we need to know in what way this camera transformed
	the image (zoom factors, lens distortion etc.) (intrinsic parameters).
	This process is referred to as Camera Calibration and is explained in 
	(\ref{sec:cameraCalibration})
	 
	\paragraph{From image point (2D) to possible points in scene (3D)} 

	What can we do if we computed the camera calibration parameters?
	The line segment that was returned by the Hough transform consists of two
	endpoints $v$ and $w$. These endpoints are in 2D but are recorded in a 3D
	scene and therefore present a 3D point in space. We don't know which point
	this is as for example we don't know the distance from the 3D point to the camera that took
	the picture. 
	However, because we calibrated the camera (\ref{sec:cameraCalibration}) we
	can reduce these possible points in 3D space to a line. Next we explain how
	we calculate this for one 2D image point (for example a line segment endpoint).\\

	We know:
	\begin{itemize}
		\item $\vec{x} = (x,y)$, the image point (in the camera coordinates
		(XYZ))
		\item $\vec{x}_{h} = (x,y,1)'$, the homogeneous coordinate of the image point
		\item $\vec{c}$ and $\vec{h}$, the camera's extrinsic parameters (the
		camera's center and heading in world coordinates (ijk))
		\item K, the camera's intrinsic parameters
		\item P, the projection from camera coordinates (XYZ) to world
		coordinates (ijk). It contains implicitly the intrinsic and
		extrinsic parameters (the camera's center $\vec{c}$ and heading
		$\vec{h}$) and is applied to transfer the camera image points $\vec{x}$ (XYZ) to world
		coordinates (ijk)
	\end{itemize}

	The two coordinates that span the line of possible points in 3D space are
	calculated as follows:
	\begin{itemize}
		\item $\vec{c}$, the location of the center of the camera in 
		world coordinates (ijk)
		%todo %(camera centers are annotated for every image)
		\item $\vec{x}_{ijk} = P K' \vec{x}_{h}$, the image point that lies on the retina of
		the camera expressed in world coordinates (ijk)\\
	\end{itemize}
	This is illustrated in Figure \ref{fig:coordinateSystemsCopy.eps}.

	\fig{coordinateSystemsCopy.eps}{The blue line spanned by the camera center $\vec{c}$ and the image point
	$\vec{x}$ tranfered to world coordinates represent the possible 3D points in
	space. $\vec{X}$ is a random possible point on the line.}{1}
	\clearpage

	%todo image
	%todo zee zisserman pagenumbers in feedbackmap

	%So the problem is boiled down in finding the Camera centers and viewing
	%directions and finding the
	%Calibration matrix.\\

	Using the two required coordinates we set up an equation of the
	line of possible points in 3D.  

	 \[ l = \vec{c} + (\vec{x}_{ijk}-\vec{c})t, t \in \mathbb{R} \]

	for example if $t=100$ then the point in the real world lies at 100 times
		the distance from the camera center to its retina.
	In Figure \ref{coordinateSystemsCopy.eps} t is about 4 (for point M).

	Above calculations are done for each line segment endpoint.
	Now we know which possible points in 3D the skyline parts,
	we can use this to find the correspondence between a lines and the walls.


\subsubsection{Associating line segments with building walls}
%todo I miss an important issue here, namely that I assume every wall to appear
%in equal size but this is not the fact, e.g. a projection of the same line on a
%different wall, that stands further away from the cc will appear bigger
%therefor the is not very 'eerlijk'
% say something about equal size assumption, where i throw away the dimension
% that is pointing from the camera cc
% solution: dichterbij heeft voorrang
	\paragraph{Building wall appearance assumption:}
	\emph{We assume that every straight line segment of the skyline represent (a part of) the upper side of a specific wall of the building.}
	Unfortunately we don't know which line is associated with which building
	wall. In this section we determine this association.

	First we prepare the 3D model. Next we project the line segments to all planes of the 3D building by
	taking their endpoints and using the technique of the previous section (\ref{sec:project})
	Finally we determine the most likely plane based on the largest line-wall overlap. 

	\paragraph{Preparing the 3D model}
	%The building is first divided into different walls.  Every wall of the building spans a plane. 
	%Intersections are then calculated between the lines and the planes of the building walls.\\
	The 3D building model consists of different walls. A wall is described by
	two ground coordinates, a height, and a direction.
	The height is based on an average building height, the direction is always the
	y-direction (see \emph{Gravity aligned walls assumption}).\\
	First we transform the walls into (infinite) planes.  This is done for two
	reasons: first this transformation is required to calculate the intersection
	properly. Second, because the 3D model is an estimate, the walls maybe just
	to small which could result in a missing intersection. \\


	\paragraph{Intersect with all walls}
	Now we have the building walls transformed to planes we take the endpoints of the
	lines and project them to all the planes of the building.  

	Each 2D endpoint has a line of possible 3D points which we calculated in the
	previous section. This was the line spanned by the camera center and the
	image point in world coordinates.
	This line is intersected with all planes of the building walls. Every 2D
	endpoint is now associated with multiple intersections resulting in 2 x
	$l$ x $w$ points in 3D (grouped by the line segments), 2 means
	\#endpoints of the line, $l$ is the number of lines and $w$ is the number of
	walls.\\

	We now calculated every possible projection to (/intersection with) every plane. How do we
	determine which plane represents the most likely wall?  
	We only calculated the projection to the
	infinite planes spanned by the walls hence we don't know which line lies on
	the wall and which falls outside the wall. To solve this problem let's
	zoom in to the situation:
	If we project $l$ to the plane spanned
	by a wall $W$ we get a line $l_{proj_W}$ in $\mathbb{R}3$.  If we assume
	$l$ to come from the contour of wall $W$, then $l_{proj_W}$ should have a large
	intersection with this wall $W$. 
	Let's call this the line-wall overlap value, $lwo$.  
	Note that the projection of $l$ with the
	other walls should have a small $lwo$ value.\\
	%todo2. see figure, create two Figure's (2d and 3d) with a single Hough line
	%(projected)

	\paragraph{Largest line-wall overlap assumption:}
	\emph{A line segment is associated with the wall with the largest projection
	overlap.}\\
	Having defined the assumptions, the situation and the idea behind the
	line-wall association, we can now explain the line-wall matching algorithm.\\ 

	A line segment is projected to all walls and the amount of line-wall
	overlap, $lwo$ is calculated. The wall with the largest overlap with the specific line
segment is classified as the most likely wall for that line segment.
	Next the line segments are projected to their most likely wall and the
	algorithm outputs this set of lines in $\mathbb{R}3$. 
	

	%\paragraph{Line wall overlap type}
	This line-wall overlap is calculated in a different steps.
	First, different types of overlap are explained. Secondly the algorithm
	determines the \emph{overlap type}, then the overlap amount is determined and
	finally the amount of overlap is normalized.\\

	$l_{proj_W}$ can overlap $W$ in four different scenarios, this is explained
	in Figure 10. The wall $W$ is spanned by $abcd$, and $l_{proj_W}$ is spanned
	by $vw$.
	
		
	\fignocaption{overlaytypes}{}{0.4}
	\clearpage

% captions of the Figure
%		1) no overlap (see fig 10a)\\
%		2) partial overlap (fig 10b)\\
%		3) full overlap ($l_{proj_w}$ is included in $w$)(fig 10c)\\
%		4) full overlap ($l_{proj_w}$ overextends $w$) (fig 10d)\\

	The type of overlap is defined by exposing the endpoints of the line
	segments to an \emph{in polygon} test, where the polygon represents a 
	wall of the building (e.g. $abcd$ in Figure 10).
	%todo REF MATLAB?
	%we use the MATLAB build in polygon as in section (%todo)

	Table \ref{tab:lwatypes} represents the types of overlap with the corresponding number of points
	that pass the \emph{in polygon} test and their possible line-wall overlap
	value.\\ 

	\begin{table}[ht]
		\caption{Types of overlap with corresponding number of points in polygon}
		\label{tab:lwatypes}

		\begin{tabular}{|l||c|c|c|}
		\hline
		Type of line-wall overlap 			&	Points in polygon 			& Line-wall overlap & Figure \\
		\hline
		\hline
		No overlap					&	0					& 0		& 10a\\
		\hline
		Partial overlap 				&	1					& [0..1]	& 10b\\
		\hline
		Full overlap (included)		&	2					& 1		& 10c\\
		\hline
		Full overlap (overextended)		&  	0					& 1 		& 10d\\
		\hline
		\end{tabular}
	\end{table}

	\paragraph{No overlap}
	If the point in polygon test returns 0, the line-wall overlap calculation
	is skipped and 0 is returned. The remaining overlap types, partial and full,
	are treated individually:\\


	\paragraph{Partial overlap}
	Let's first consider the partial overlap type (Figure 10b), the \emph{in polygon} test
	returned 1, that means that one of the line segments endpoint lies inside
	and one lies outside the wall.\\
	To calculate the amount of line-wall overlap, the line segment is cropped to the
	part that overlaps the wall and the length is measured.\\
	The cropped line has two coordinates, first of course the point that passed
	the \emph{in polygon} test and secondly the intersection of the line
	segment with one of the vertical wall sides ($da$ or $cb$ from Figure 10b).\\
	We assume the walls to be of infinite height, therefore the 
	partial overlapping line segment always intersects one of the
	vertical wall sides.\\
	To determine which of the two vertical wall sides is crossed, we determine
	on which side the point that doesn't lie in the polygon (v) is on.  This is
	done by an angle comparison.\\
	First, two groups of two vectors are defined: $dv$, $dc$ and $cw$, $cd$ (see Figure 10b).
	We measure the angles between the vectors and call them $\angle d$, and
	$\angle c$. 
	Because one of the line segment endpoints lies outside
	the wall $\angle d$ or $\angle c$ is obtuse, in this case $\angle d$ is obtuse.
	(Note that this holds because the walls are orthogonal to the basis
	which we assumed in the \emph{Gravity aligned walls assumption}\\
	To be more precise: 
	\begin{itemize}
	\item If $\angle d$ is obtuse, the left vertical wall side $da$, is
	crossed. \\
	\item If $\angle c$ is obtuse, the right vertical wall side $cb$, is
	crossed. \\
	\end{itemize}
	The angles are acute or obtuse if the dot product of the vectors involved
	are respectively positive or negative. The advantage of this method is that
	it's simple and has low computational costs.\\
	
	\emph{Line-wall overlap calculation}\\
	The amount of line-wall overlap is calculated by cutting of the
	point where $l$ intersects the determined vertical wall side ($da$ or
	$cb$) and measuring its remaining length.\\

	\paragraph{Full or no overlap}
	Now let's consider the overlap types where the \emph{in polygon} test
	returned 0.
	As you can see in Figure 10a and 10d this resulted in either full or no overlap.
	Again we analyze the vector angles to determine the remaining overlap-type.
	If only one of the angles is obtuse with no points in the polygon, like in Figure 10a,
	the whole line segment lies outside the wall: an overlap value of zero
	is returned.\\
	Otherwise, if both angles $\angle d$ and $\angle c$ are obtuse or acute (Figure 10d),
	both endpoints lie on a different side of the wall, and they cross the wall somewhere in
	between. Full overlap is concluded here. \\
	The amount of overlap is now calculated by measuring the length
	of the line segment which is cut down by his intersections with $da$ and
	$cb$. In this case this is the same as line $dc$, but its easy to see that
	this is not the case when $vw$ is not parallel to $dc$.\\
	
	\paragraph{Line-wall overlap normalization}
	Finally the line-wall overlap is normalized by the line segments length:\\
	\begin{equation}
		\alpha_l = \frac{lwo}{|l|}
	\end{equation}
	Where $\alpha_l$ is the normalized line-wall overlap, $lwo$ is the unnormalized
	amount of line-wall overlap, and ($|l|$) is the total length of the line
	segment (uncut).\\
	The intuition behind this is that line segments that are likely to
	present a wall not only have a large overlap but also have a small part
	that has no overlap, the missing overlap should have a negative effect. By
	calculating the relative overlap, both amounts of overlap and -missing
	overlap are taken into account.\\
	The maximum of the normalized line-wall overlap is used to associate a
	line segment with its most likely wall.
	To summarize, the overlap type is defined by calculating the numbers of in
	polygon points and evaluating two dot products. Next the line segment is cut off
	depending on the overlap type and the line is normalized. \\
	
	Now we have determined the normalized line-wall overlap, we use this to
	search for the correct line-wall association. This is achieved by
	associating a line segment with the wall that has the largest line-wall
	overlap.\\

\subsubsection{Improving the 3D model by wall height estimation}
	In the previous section we associated the line segments with their most
	likely wall. In this section this information is used to estimate the
	heights of the walls of the 3D model. \\
	Now all line segments are associated with a certain wall, we re-project the
	line segment from the different views on their associated wall. The
	re-projection is done by intersecting both endpoints of the line segment to
	the plane that is spanned by the associated wall.\\
	Next the 3D intersection points are collected and averaged, this gives us
	an average of the midpoints of the projected line segments. We do this for
	every wall separately, returning the average height of the line segments.
	These averages are then used as the new heights of the walls of the
	building.  Note that this is only permitted in presence of the \emph{flat roof assumption}.\\
	The new individual heights are used in the 3D model by adjusting the
	location of the existing upper corner points of the walls. We copy the
	bottom left and right corner points and add the estimated height from the
	previous section to its y-value. The y-value is the direction of the
	gravity which is permitted by the \emph{Gravity aligned walls assumption}.

\newpage
\subsection{Results}
\label{sec:ResultImprove}
\fig{outputHoughlines2d}{Three best ranked lines of the Hough transform on the
skyline detector match the building walls}{0.4}
\fig{outputHoughlines3d}{Houghlines projected on the most likely wall}{0.5}
\clearpage
\fig{outputMutateBuilding}{Improved 3D model}{0.6}
Figure \ref{fig:outputHoughlines2d} shows the top 3 longest Houghlines. The
endpoints are denoted with a black and blue cross. All three line segments lie on the
building contour.  The left line segment covers only a part of the building wall. The
middle line segment covers the full wall contour. The left and middle line segment are connected. The
right line segment covers the wall until the tree occludes.\\

Figure \ref{fig:outputHoughlines3d} 
 displays the line segments projected onto their associated walls.
Note that the line segments are originated from different views. 
%fit3d plaatje sequence
 
For a clear view we have only selected the lines that were associated with three
specific walls of the building of Figure \ref{fig:outputHoughlines2d}.  
For each different view we draw a red cross that represents the average middle
point for that view for each wall, some crosses overlap.\\

Figure \ref{fig:outputMutateBuilding} 
displays the updated 3D model. The
corner points of the walls are adjusted according the calculated wall heights.
The green plane displays the modified wall. The left and middle wall are extended
whereas the right wall is shortened.\\


\subsection{Discussion}
%todo positief lullen over resultaat
As can be seen in Figure \ref{fig:outputHoughlines2d}, 
the top three Houghlines correspond the the three most prominent building walls
What also can be seen is that the left line segment doesn't cover the whole
building wall. This is caused by the use of strict parameters in the Hough transform
(like a small line thickness parameter).  If some ascending skyline pixels fall just outside
the Houghlines, a gap is created and the line segment is cut down at that point.
This is however not a big problem because the lines are long enough to produce a
good wall height estimate. Furthermore there are 5 other lines
(originated from different views) that support the estimate for this wall.





\subsection{Conclusion}
Let's answer our research question.
\paragraph{Research question}
Is it possible to use a set of (noisy) skyline points together with an aerial
2D model and a 3D pointcloud obtained by \emph{FIT3D toolbox\cite{fit3d}} to generate a 3D model of the building?

Yes this is possible, we showed that a Houghline transform is a useful method to detect
skyline outliers and find prominent structure in the contour of a building with a flat
roof. We introduced a method to pair up line segments with their associated
walls. This was used to produce new wall heights which were propagated to the 3D
model.  Existing and novel AI computer vision techniques were powerfully
combined resulting in an significant improvement of a 3D model based on only a
few calibrated 2D images. 
%todo conclusion is no summary... conclude something..

\subsection{Future research}
\subsubsection{Gravity aligned walls assumption}
In this project we assumed the walls to align with the gravity.
This means the camera must be up right: his parameter \emph{roll} must be
exactly zero when capturing the images. 
In practical use this is not true. We demonstrate this by plotting the 3D point cloud
with the 3D model in Figure .
Although this assumption let us focus on the important issues it would be nice
to incorporate gravity estimation in future research.
Costin Ionita wrote a part of his master thesis about gravity estimation in
\cite{costin}.\\

%todo generate a figure with 2 Hough lines on wall of tree
%As can be seen in Figure %\ref{fig:}
\subsubsection{Double wall height influence}
Sometimes two line segments appear on the same single wall. This means that they have a double
influence on the average wall height, which is unjustified. 
A simple solution would be to add a normalization pre-process step, so each view
has only one wall height vote per wall. A more decent solution would be to
merge the two (or more) line segments to a single line segment. 
Lines that are close and parallel could be merged and averaged.
Lines that lie in each others direction could be merged by increasing the 
Hough transforms \emph{FillGap} parameter.  E.g. for the right wall of the building in 
Figure \ref{fig:outputHoughlines3d} the \emph{FillGap} parameter needs to be at
least as big as the occluding tree.

\subsubsection{Complexity}
In this thesis little is discussed about the computational costs. Because the 
computations are done efficiently (e.g. using matrix multiplications
in MATLAB) and off line, the calculation are done in reasonable time.
However, if we want to make the application real time, the next speedup would be useful.\\
To determine the best line-wall association the line segments are now projected to
every wall and for every wall the amount of line-wall overlap is calculated. This
is computational very expensive and looks a bit like an overkill.\\

It would be a significant speedup to reduce the set of walls to only the walls
that contain the middle point of the line segments. To be more concrete the
middle point needs to be calculated by averaging the line segments endpoints,
this middle point is used in the \emph{in polygon} test for every wall.  Next the
line-wall association algorithm only treats the walls that pass this test.

The downside of this method is that it makes the system less accurate because it
will resulting in more false negatives. A line segment that overlaps the wall with only 1/3 could be an
important candidate for the height estimation but because the speedup method it is discarded.
What can be concluded is that there is a trade of in the accurateness of the
height estimation and the computational costs.


\subsubsection{Alternative roofs}
%todo vet goed dat ik dus mijn algo different wall heights aan kan, promoten!

%-----done:
We assumed a flat roof, this doesn't mean that our method is unusable if
we discard this assumption.
E.g. without adaptations the method could be used to determine the (maximum)
building height which is a useful application.\\

If we discard the flat roof assumption the building is allowed to have any
shape. In this situation it should also be possible to extract a
full 3D model.  We will now consider other roof types and discuss what
adaptations the system should require to handle these.  In Figure
\ref{fig:typesOfRoofs}, 6 different roof shapes are displayed.\\
%-----

\fig{typesOfRoofs}{Different types of roofs}{0.4}
Consider the \emph{Gable Roof}, it is a roof consisting of two planes
which are not parallel with the facade of the building. This makes the problem
of extracting the 3D model more complex, but not infeasible. \\
Because we assume that the roof images are taken from the ground, the skyline
detector will always detect the top of the building. In case of a flat roof
this is also the top of the building walls. In case of an
alternative roof, this will be just the top of the building. The building walls however
could lie a lot lower, therefore something else needs to be developed to find the wall
heights. It would be useful to develop a method that can detect which roof type we
are dealing with, what the wall heights are, and finally generate an entire 3D
model.\\
Some ideas about this are now proposed:\\
\begin{itemize}
	\item Use an object detector to detect doors, windows and dormers so the 
	number of floors, the location of the wall-roof separation and the exclusion of
	some roof types (e.g. a dormer is never located on a flat roof) could be determined.\\
	\item Use the Hough transform to search for horizontal lines to detect the
	wall-roof separation, and use the the ground plane and the top roof line to
	guide the search.  Some building have a gutter, because of this the number
	of horizontal lines on the wall-roof separation will be larger which could
	be of great use.\\
	\item Use geographic information (a database of roof types) with gps location
	to classify the roof type. \\
	\item The skyline detector detects the building height, if we
	could use predefined information about the ratio between the wall height and
	total height of the building, the wall heights could be estimated.\\
\end{itemize}
Assuming we determined the roof type,the building height and wall heights, the 3D model could 
easily be generated. For the \emph{Gable} roof for example this will involve
connecting two surfaces from the upper side of the walls with the high roof line (returned by the
skyline detector). For the other roof types, the building height and wall
height together with a template structure of the roof could be used to generate the 3D
model.


%\section{My old method of line-wall association}
%\label{sec:oldmethodlwa}
%We take the line segments endpoints and project it onto the building walls. The wall with the shortest
%distance to the camera center will be assigned to the line segment. Points that
%lie outside the polygon are punished.\\
%
%And to update the specific wall we first need to know with a high probability of
%being correct which wall the line segment presents.
%But this method introduces a problem: some of the line segments have endpoints that lie at the corner of the building. These line segments could easily be associated with the neighboring wall. Because the 3D model is a rough estimate this could lead to bad results.
%In the corner case it is not clear to which wall the line segment belongs because both endpoints do not agree on the same wall. To solve this problem some heuristic methods are developed and tested. The following heuristic is both simple and effective.
%The heuristic uses the importance of the middle point of the line segment. This middle point has a low change of being on a building corner and the on average biggest change of being on the wall we are looking for.
%Therefor we discard the endpoints and use the middle point the endpoints to determine the right wall.
%This middle point is intersected with all planes spanned by the walls. The line segment is stored to the wall with the shortest distance.
%The output of this part of the algorithm is for every wall a bunch of associated line segments originated from different views.\\
%
%in section Results was this text:\\
%The left and middle line segment of Figure \ref{fig:outputHoughlines2d} 
%are
%a good example of the corner problem. Both endpoints that lie on the corner could easily be associated
%with the wrong wall (even if the rough 3D model is very accurate). Fortunately
%we use the middle point of the line segment to determine the correct wall. This
%works well as its 100\% accurate (for this dataset).
%
