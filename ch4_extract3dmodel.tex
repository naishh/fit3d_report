% todo 
% be more clear about the projection, its the first time the reader sees it 



\section{Extracting the 3D building}
%todo plaatje van scene met een 3D model erin
%todo plaatje 2d model
\label{sec:generate3dModel}
% motivation
\subsection{Introduction}
In the previous chapter we explained our skyline detection algorithm which
extracted the skyline of a scene. The output is a set of 2D points which was  
collected for a sequence of images.
The aim of this Chapter is to use this set of points together with an
 aerial 2D model of the building and the 3D point cloud obtained by the \emph{FIT3D
 toolbox\cite{FIT3D}} (\ref{sec:prelimFIT3D}) to generate a 3D model of the
 building.


\paragraph{Research question}
%todo uitgebreider over FIT3D linken 
Is it possible to use a set of (noisy) skyline points together with an aerial
2D model and a 3D point cloud obtained by the \emph{FIT3D toolbox\cite{FIT3D}} to
generate a 3D model of the building?\\

We present a stepwise solution: first \emph{Openstreetmap\cite{Openstreetmap}} is used to obtain a 2D top view of
the outline of the building. The line segments of the 2D model represent the walls of the building. 
Next the 3D point cloud obtained by the \emph{FIT3D toolbox\cite{FIT3D}} is used to align this 2D
model in the scene.  After this, the set of points returned by the skyline
detector is transfered to a set of lines. Then each line segment is assigned to
a part (wall) of the aligned 2D model.  Next the line segments are
projected to vertical planes spanned by the 2D model.  The result is used to
estimate the height values of the walls of the 2D model. The 2D model is
transformed according to this height values to a 3D
model. We will now elaborate on each step.\\


\subsection{Method}
\subsubsection{Extracting the 2D model}
\fig{openstreetmap}{Openstreetmap\cite{Openstreetmap} with annotated buildings}{0.3}
The basis of the generated 3D model is a map containing the 2D outline polygon
of the building originated from \emph{Openstreetmap\cite{Openstreetmap}} 
(Figure \ref{fig:openstreetmap}) which is a freely accessible 2D map generated by
users all over the world. It contains information about streets, building
contours, building functions, museums, etc.  We are interested in the building
contours therefore we take a snapshot of a particular area and extract this building
contour.  This is a set of ordered points where each point corresponds to a
corner of the building.  Next we link these points to line segments which
represent (the top view of) the walls of the building.

\subsubsection{Aligning the 2D model}
\fig{pc_3d.eps}{ 3D point cloud of the walls of the building}{0.5}
We want to align the 2D model in the scene which means that we have to
position the 2D model in the world coordinate using a translation, rotation and
scaling.

From \emph{FIT3D\cite{FIT3D}} we have the 3D point cloud of the building in world coordinates
(Figure \ref{fig:pc_3d.eps}).
The first challenge is to obtain a top view of the 3D point cloud.
How can we determine the direction of the top view? We want to project in the
direction that is most parallel to the walls and orthogonal to the 2D model.

\paragraph{Gravity aligned walls assumption}
	\emph{The walls of the building are aligned with the gravity direction
	(y-axis) which is orthogonal to the 2D model from \emph{Openstreetmap\cite{Openstreetmap}}}. This means
	we assume the images are taken upright: the camera's $roll=0$ (Figure
	\ref{fig:cameraPitch.eps}).\\

Now we have defined the wall direction, we obtain the top view of the 3D point
cloud by discarding the y-dimension of the points.  Note that this is equivalent to a
projection to the x,z plane. The result is a set of 2D points that represent the
top view of the 3D point cloud (Figure \ref{fig:pc_topview_linefit.eps}).

We determine the walls by fitting line segments in the point cloud.  We
annotated these line segments manually.  If the system needs to operate
automatically, RANSAC can be used to fit lines in this point cloud.

\fig{pc_topview.eps}{The projected 3D point cloud of the walls of the
building}{0.5}
\fig{pc_topview_linefit.eps}{M2, The fitted line segments define (a top view of
) the building walls}{0.5}
\fig{pc_topview2DModel.eps}{The 2D model M1 aligned with the fitted line
segments (M2)}{0.5}
\clearpage

The next step is to align the 2D \emph{Openstreetmap\cite{Openstreetmap}} model $M1$ with these line segments of the
projected point cloud $M2$ in the world coordinate frame.  First the endpoints of the line segments, which
correspond to the wall corners, are extracted in both models.
Note that the walls of $M2$, that are located at the back of the building, are missing
because they are occluded by the front walls. We only consider the corners that
are present in both models.

Next the correspondences between these corners is annotated manually. 
This is used to generate a set of linear equations
which are solved in closed form \cite{hartley}. The result is a matrix $A$
which represents the rotation, translation and scaling that is needed for a
coordinate of $M1$ to be transformed to $M2$ in the world coordinate frame.
Finally $A$ is applied on all corner points of $M1$ which results in an aligned 2D
model (Figure \ref{fig:pc_topview2DModel.eps}).

%affinetransform
%http://stackoverflow.com/questions/1856210/trying-to-derive-a-2d-transformation-matrix-using-only-the-images
%http://docs.oracle.com/javase/1.4.2/docs/api/java/awt/geom/AffineTransform.html


%todo convention matrix bold?
%------------------------------------------------------------------------
%todo example image of 3D model overlay
% of osgviewer screendpumps pakken en het erinplakken
% of building.osg in osgviewer plakken
% of in matlab de boel tegelijk plotten


%discuss disadvantage (skew Y assumption)
% see images in 
%  /media/Storage/scriptie/FIT3D/results/anglebug
% formula of 3 coords of wall with an Y coord 



\subsubsection{Transferring the aligned 2D model to 3D}
Because the 2D model is based on aerial images it contains no
information regarding the height of each wall. 
In the next section we explain how we obtain the precise height values.\\

For the sake of presentation we use an average building height to generate a
rough estimate of the 3D model.  The 3D model is generated by taking the 2D model
and extend it in the orthogonal direction.  An example of the 3D model can
be seen in Figure \ref{fig:3dModel}.

\fig{3dModel}{The basic 3D model, generated by extending the 
(2D) model from \emph{Openstreetmap\cite{Openstreetmap}} to an average building height}{0.5}
%todo FIT3D linken

%todo plaatje maken van aligened model: probleem plotBuilding is niet aligned
%met generate../align3dmodel



\subsubsection{Extracting line segments}
\label{extractinglinesegments}
	Because we want to estimate the height of the building walls of the 3D
	model, we need to know how high the walls in the images are.  To estimate
	this, we first determine which part of the skyline is part of the building
	contour.  We use the idea that straight lines in the skyline area are likely
	to come from the building contour. In this section we explain how we extract
	these straight line segments.

\paragraph{Assumptions}
	Many urban areas contain buildings with a flat roof. Therefore the contour
	of the building is always formed by a set of straight line segments.
	Furthermore the building contour is often aligned with the topside of a building wall.
	If we assume a flat roof, we can use the skyline to estimate the height of
	the building walls without having to concern for (complex) roof types.

	\subparagraph{Flat roof assumption}
	\emph{We assume each building has a flat roof, implicating that the building
	contour is aligned with the topside of a building wall.  The building walls
	may have different heights but the roof should be flat.}\\


\paragraph{Hough transform}
	As was discussed in Chapter \ref{sec:ch2}, a widely used method for extracting line segments is the Hough transform \cite{Hough}.
	We regard this as a suitable method because it is
	used a lot for this kind of problems. This is probably because it is unique
	in its low complexity (compared to other methods like
	\emph{RANSAC}, who often use an iterative approach).
	For a detailed explanation of the Hough transform, see section \ref{sec:prelimHough}.\\

	The input of the Hough transform that is build-in in \emph{MATLAB\cite{matlab}} is a binary
	image. This is in our case the output of the skyline detector (Chapter
	\ref{sec:skylinedetection}).\\
	If a pixel is classified as a skyline pixel (a pixel that lies on the
	skyline according the skyline detector), the Hough transform increases
	a vote value for every valid line $(r,\theta)$ pair that crosses this
	particular pixel.  
	Lines $(r,\theta)$ pairs that receive a large amount of votes
	contain a large amount of skyline pixels.\\
	Because the algorithm detects straight lines containing only skyline pixels
	it returns only the straight parts of the skyline.
	As these straight skyline parts are likely to come frome the building
	contour, we found exactly what we where looking for.
	Results of the Hough transform on the output of the skyline detector are
	displayed and evaluated in the Result section (\ref{sec:ResultImprove}).

\subsubsection{Project the skyline to the 3D model}
\label{sec:project}

	% intro
	The Hough transform of the previous section returned a set of 2D line
	segments which likely present parts of the building contour.  
	As we want to estimate the building wall heights we need to correspond these
	line segments to the walls.  This is done by projecting the line segments to a specific part of the 3D model.  
	We present a stepwise solution: first the camera is calibrated, next we
	project the line segments to the the building and finally we 
	determine the specific building part that is associate with a line segment.

	To project the line segments to the 3D model we need to know the
	camera's position and rotation when it took the photo (extrinsic
	parameters). Furthermore we need to know in what way this camera transformed
	the image (e.g. lens distortion) (intrinsic parameters).
	This process is referred to as camera calibration and is explained in 
	(\ref{sec:cameraCalibration})
	 
	\paragraph{From image point (2D) to possible points in scene (3D)} 

	What can we do if we computed the camera calibration parameters?
	The line segment that was returned by the Hough transform consists of two
	endpoints $v$ and $w$. These endpoints are in 2D but are recorded in a 3D
	scene and therefore present a 3D point in space. We don't know which point
	this is as for example we don't know the distance from the 3D point to the camera that took
	the picture. 
	However, because we calibrated the camera (\ref{sec:cameraCalibration}) we
	can reduce these possible points in 3D space to a line. Next we explain how
	we calculate this for one 2D image point (for example a line segment endpoint).\\

	From the input images and the calibration process
	(\ref{sec:cameraCalibration}) we have:
	\begin{itemize}
		\item $\vec{x} = (x,y)$, the image point (in the camera coordinates
		(XYZ))
		\item $\vec{x}_{h} = (x,y,1)'$, the homogeneous coordinate of the image point
		\item $\vec{c_{ijk}}$ and $R$, the camera's extrinsic parameters (the
		center and rotation of the camera in world coordinates (ijk))
		\item $K$, the camera's intrinsic parameters
		\item $P$, the projection from camera coordinates (XYZ) to world
		coordinates (ijk). It contains the 
		extrinsic parameters (the camera's center $\vec{c_{ijk}}$ and rotation 
		$R$).
	\end{itemize}

	The image point in the image coordinate frame corresponds to a line of
	possible points in the world coordinate frame.  The two coordinates that
	span this line are calculated as follows:
	\begin{itemize}
		\item $\vec{c_{ijk}}$, the location of the center of the camera in 
		world coordinates (ijk)
		%todo %(camera centers are annotated for every image)
		\item $\vec{x}_{ijk} = P K' \vec{x}_{h}$, the image point expressed in world coordinates (ijk)\\
	\end{itemize}
	This is illustrated in Figure \ref{fig:coordinateSystemsCopy.eps}.

	\fig{coordinateSystemsCopy.eps}{The blue line spanned by the camera center
	$\vec{c}$ and the image point $\vec{x}$ transfered to world coordinates represent the possible 3D points in
	space. $\vec{X}$ is a random possible point on the line.}{1}
	\clearpage

	%todo image
	%todo zee zisserman pagenumbers in feedbackmap

	%So the problem is boiled down in finding the Camera centers and viewing
	%directions and finding the
	%Calibration matrix.\\

	Using the two required coordinates we set up an equation of the
	line of possible points in 3D.  

	 \[ l = \vec{c_{ijk}} + (\vec{x}_{ijk}-\vec{c_{ijk}})t, t \in \mathbb{R} \]

	for example if $t=100$ then the point in the real world lies at 100 times
		the distance from the camera center to its retina.
	In Figure \ref{fig:coordinateSystemsCopy.eps} t is about 4 (for point M).

	Above calculations are done for all the line segment's endpoints.
	This results in a line of possible points in 3D space for each endpoint.
	Next we explain how we use this to find the line-wall correspondences.


\subsubsection{Associating line segments with building walls}
\label{sec:linewallass}
%todo I miss an important issue here, namely that I assume every wall to appear
%in equal size but this is not the fact, e.g. a projection of the same line on a
%different wall, that stands further away from the cc will appear bigger
%therefor the is not very 'eerlijk'
% say something about equal size assumption, where i throw away the dimension
% that is pointing from the camera cc
% solution: dichterbij heeft voorrang
	\paragraph{Building wall appearance assumption:}
	\emph{We assume that every straight line segment of the skyline represent (a
	part of) the upper side of a specific wall of the building.}\\

	Unfortunately we don't know which line is associated with which building
	wall. In this section we determine this association.

	First we describe the walls with planes. Next, we project the line segments to all planes of the 3D building by
	taking their endpoints and using the technique of the previous section (\ref{sec:project})
	Finally we determine the most likely wall based on the largest line-wall overlap. 

	\paragraph{Walls to planes}
	%The building is first divided into different walls.  Every wall of the building spans a plane. 
	%Intersections are then calculated between the lines and the planes of the building walls.\\
	The 3D building model consists of different walls. A wall is described by
	two ground points, a height, and a direction.
	The height is based on an average building height, the direction is always the
	y-direction (see \emph{Gravity aligned walls assumption}). We transform the walls into (infinite) planes.  This is done for two
	reasons: first this transformation is required to calculate the intersection
	properly. Second, because the 3D model is an estimate, the walls maybe just
	to small which could result in a missing intersection. \\


	\paragraph{Intersect with all walls}
	Now we have the building walls transformed to planes, we take the endpoints of the
	lines and project them to all the planes of the building for further
	selection.  

	Each 2D endpoint has a line of possible 3D points which we calculated in the
	previous section. This was the line spanned by the camera center and the
	image point in world coordinates.
	This line is intersected with all planes of the building walls. 
	
	Every 2D endpoint is now associated with multiple intersections resulting in 2 x
	$l$ x $w$ points in 3D (grouped by the line segments), 2 means
	\#endpoints of the line, $l$ is the number of lines and $w$ is the number of
	walls.\\

	We now calculated every possible intersection with every plane. How do we
	determine which plane represents a line segments most likely wall?  
	Recall that the wall is a subset of the plane.  We only calculated the projection to the
	planes spanned by the walls hence we don't know which line lies on
	the wall and which falls outside the wall. To solve this problem let's
	zoom in to the situation:

	If we project a skyline part $l$ in 2D containing two points to the plane spanned
	by a wall $W$ we get two intersections points that present the projected
	line $l_{proj_W}$ in 3D. If we assume
	$l$ to come from the contour of wall $W$, then $l_{proj_W}$ should have a large
	overlap with this wall $W$.  We call this the line-wall overlap value, $lwo$.  
	Besides the large overlap with $W$ we expect a small or zero $lwo$ for
	the other walls, see Figure 10a and 10b.\\
	%todo2. see figure, create two Figure's (2d and 3d) with a single Hough line
	%(projected)

	\paragraph{Largest line-wall overlap assumption:}
	\emph{A line segment is associated with the wall with the largest projection
	overlap.}\\

	Having defined the assumptions, the situation and the idea behind the
	line-wall association, we can now explain the line-wall matching algorithm.\\ 

	A line segment is projected to all walls and the amount of line-wall
	overlap, $lwo$ is calculated. The wall with the largest overlap with the specific line
segment is classified as the most likely wall for that line segment.
	Next the line segments are projected to their most likely wall and the
	algorithm outputs this set of lines in $\mathbb{R}3$. 
	

	%\paragraph{Line wall overlap type}
	This line-wall overlap is calculated in different steps.
	Before we explain the steps we discuss the different types of overlap. Next
	we explain how the algorithm determines the \emph{overlap type}. Finally we
	we calculate the overlap amount and normalize this value.\\

	$l_{proj_W}$ can overlap $W$ in four different ways, this is illustrated 
	in Figure 10. The wall $W$ is spanned by $abcd$, and $l_{proj_W}$ is spanned
	by $vw$.
	%todo future research parallel walls take the closest one 
	
		
	\fignocaption{overlaytypes}{}{0.4}
	\clearpage

% captions of the Figure
%		1) no overlap (see fig 10a)\\
%		2) partial overlap (fig 10b)\\
%		3) full overlap ($l_{proj_w}$ is included in $w$)(fig 10c)\\
%		4) full overlap ($l_{proj_w}$ overextends $w$) (fig 10d)\\

	The type of overlap is defined by exposing the endpoints of the line
	segments to an \emph{in polygon} test, where the polygon represents a 
	wall of the building (e.g. $abcd$ in Figure 10).
	%todo REF MATLAB\cite{matlab}?
	%we use the MATLAB\cite{matlab} build in polygon as in section (%todo)

	Table \ref{tab:lwatypes} represents the types of overlap with the corresponding number of points
	that pass the \emph{in polygon} test and their possible line-wall overlap
	value.\\ 

	\begin{table}[ht]
		\caption{Types of overlap with corresponding number of points in polygon}
		\label{tab:lwatypes}

		\begin{tabular}{|l||c|c|c|}
		\hline
		Type of line-wall overlap 			&	Points in polygon 			& Line-wall overlap & Figure \\
		\hline
		\hline
		No overlap					&	0					& 0		& 10a\\
		\hline
		Partial overlap 				&	1					& [0..1]	& 10b\\
		\hline
		Full overlap (included)		&	2					& 1		& 10c\\
		\hline
		Full overlap (overextended)		&  	0					& 1 		& 10d\\
		\hline
		\end{tabular}
	\end{table}

	\paragraph{No overlap}
	If the point in polygon test returns 0, the line-wall overlap calculation
	is skipped and 0 is returned. The remaining overlap types, partial and full,
	are treated individually:\\


	\paragraph{Partial overlap}
	Let's first consider the partial overlap type (Figure 10b), the \emph{in polygon} test
	returned 1, that means that one of the line segments endpoint lies inside
	and one lies outside the wall.\\
	To determine the amount of line-wall overlap, the part of the line segment
	that overlaps the wall is calculated. The length of this part is measured
	and stored as its $lwo$ value.\\
	The trimmed line has two coordinates: 1) the point that passed
	the \emph{in polygon} test and 2) the intersection of the line
	segment with one of the vertical wall sides ($da$ or $cb$ from Figure 10b).\\
	How do we determine which vertical wall side is crossed?
	We use the fact that one of the line segments endpoints lies outside the
	polygon next to the vertical wall side.
 	This point is easily determined by an angle comparison:
	first, two groups of two vectors are defined: $dv$, $dc$ and $cw$, $cd$ (see Figure 10b).
	We measure the angles between the vectors and call them $\angle d$, and
	$\angle c$.  Because one of the line segment endpoints lies outside
	the wall, either $\angle d$ or $\angle c$ is obtuse. In this example $\angle
	d$ is obtuse therefor the left wall side is crossed.
	Note that this only holds because the walls are orthogonal to the basis
	which we assumed in the \emph{Gravity aligned walls assumption}.\\

	Now we know that:
	\begin{itemize}
	\item If $\angle d$ is obtuse, the left vertical wall side $da$, is
	crossed. \\
	\item If $\angle c$ is obtuse, the right vertical wall side $cb$, is
	crossed. \\
	\end{itemize}
	The angles are acute or obtuse if the dot product of the vectors involved
	are respectively positive or negative. 
	Note the advantage of this method: 
	it is simple and has low computational costs.
	
	\paragraph{Line-wall overlap calculation}
	The amount of line-wall overlap is calculated by cutting of the
	point where $l$ intersects the determined vertical wall side ($da$ or
	$cb$) and measuring its remaining length.\\

	\paragraph{Full or no overlap}
	Now let's consider the overlap types where the \emph{in polygon} test
	returned 0.
	As you can see in Figure 10a and 10d this resulted in either full or no overlap.
	Again, we analyze the vector angles to determine the remaining overlap type.
	If only one of the angles is obtuse and no points lie in the polygon
	(Figure 10a), the entire line segment lies outside the wall and $lwo = 0$.\\
	Otherwise, if both angles $\angle d$ and $\angle c$ are obtuse or acute (Figure 10d),
	both endpoints lie on a different side of the wall, and they cross the wall somewhere in
	between. Full overlap is concluded here. \\
	
	The amount of overlap is now calculated by measuring the length
	of the line segment which is cut down by his intersections with $da$ and
	$cb$. In this case this is equal to the length of the line $dc$, however, it
	is easy to see that this only holds of $vw$ is parallel to $dc$.
	
	\paragraph{Line-wall overlap normalization}
	Finally the line-wall overlap is normalized by the line segments length:\\
	\begin{equation}
		lwo' =  \frac{lwo}{|l|}
	\end{equation}
	Where $lwo'$ is the normalized line-wall overlap, $lwo$ is the length of
	the trimmed line segment, and $|l|$ is the total length of the line.\\
	The intuition behind this is that line segments that are likely to
	present a wall not only have a large overlap but also have a small part
	that has no overlap, the missing overlap should have a negative effect. By
	calculating the relative overlap $\frac{lwo}{|l|}$, both amounts of overlap and missing
	overlap are taken into account.\\

	Normalizing the line-wall overlap on the line length implies that the length
	of the line does not influence anymore.  This can be seen as a
	disadvantage because small (noisy) line segments with 100\% overlap will
	outperform large robust lines with little overlap.  This problem is solved
	by discarding all line segments smaller than a \emph{MinimumLength} value.
	This is done during pre-processing in the Hough transform module. 

	Next the normalized line-wall overlap is used to search for the correct
	line-wall association. This is achieved by associating a line segment with
	the wall that has the largest line-wall overlap.\\

	To summarize, the overlap type is determined by calculating the numbers of in
	polygon points and evaluating two dot products. Next the line segment is cut off
	depending on the overlap type and the line is normalized. 
	The wall where the line segment scores the maximum $lwo'$ value is
	associated with this line segment.


\newpage
\subsubsection{Improving the 3D model by wall height estimation}
	In the previous section we associated the line segments with their most
	likely wall. In this section this information is used to estimate the
	heights of the walls of the 3D model. 

	Every different view of the building produced a collection of lines that are
	associated with a certain wall. For example if we consider 4 views and for
	wall $W$ the views have respectively 2,4,4,1 lines that are associated with
	$W$, we have a total of 11 line segments that correspond with wall $W$.\\

	Next we re-project the line segment from the different views on their
	associated walls.  The re-projection is done as explained in
	(\ref{sec:linewallass}) by intersecting both endpoints
	of the line segment to the plane that is spanned by the associated wall.

	\fig{skyline_proj.eps}{The Houghlines collected from different views projected on their corresponding
	wall}{0.25}
	\clearpage

	Next the average height of the projected line segments is calculated for
	each wall. This is done by collecting and averaging the y-value of
	the middle points of the linesegments and this is done for each wall seperately.
	Next, these averages are
	used to set the new heights of the planes of the 3D model.  

	The new individual heights are applied to the 3D model by adjusting the
	height of the existing upper corner points of the walls. We copy the bottom
	left and right corner points and add the estimated height from the previous
	section to its y-value. The y-value is the direction of the gravity which is
	obtained through the \emph{Gravity aligned walls assumption}.\\

\newpage
\subsection{Results}
\label{sec:ResultImprove}
\fig{outputHoughlines2d}{Three best ranked lines of the Hough transform on the
skyline detector match the three most prominent displayed building walls}{0.4}
\fig{outputHoughlines3d}{The Houghlines collected from different views re-projected
on the 3D model according to the line-wall correspondence.}{0.5}
\clearpage
\fig{floriande_back.eps}{The walls of the building differ in height: the back
part of the building is higher than the middle part.}{0.32}
\fig{outputMutateBuilding}{Improved 3D model}{0.6}
\clearpage

Figure \ref{fig:outputHoughlines2d} shows the top 3 longest Houghlines of a
single view. The endpoints are denoted with a black and blue cross. All three line segments lie on the
building contour.  The left line segment covers only a part of the building wall. The
middle line segment covers the full wall contour. The left and middle line segment are connected. The
right line segment covers the wall until it occludes the tree.\\

Figure \ref{fig:outputHoughlines3d} displays the line segments (collected from
Figure \ref{fig:outputHoughlines2d} and 6 other views) projected onto their associated walls.
For a clear illustration we selected just three (prominent) walls.
For each view a different red cross is drawn at each wall. It illustrates the
average height of the corresponding lines corresponding to that wall.
 
Figure \ref{fig:outputMutateBuilding} displays the updated 3D model. The corner
points of the walls are adjusted according the calculated wall heights.  The
green plane displays the modified wall. The left and middle wall are extended
whereas the right wall is shortened.\\


\subsection{Discussion}
%todo positief lullen over resultaat
As can be seen in Figure \ref{fig:outputHoughlines2d}, 
the top three Houghlines correspond to the three most prominent building walls.
What also can be seen is that the left line segment doesn't cover the entire
building wall. This is caused by the use of
a small line thickness parameter
in the Hough transform: if some ascending skyline points fall just outside
a Houghlines, a gap is created and the line segment is cut down at that point.
As we use a large minimum length the residual line segment is to small and is
discarded.  However, this is not a big problem because the lines are long enough to produce a
good wall height estimate. Furthermore, in this example, there are 5 other lines
(originated from the different views) that support the height estimate for this
particular wall.\\

The different red crosses in Figure \ref{fig:outputHoughlines3d} which illustrate the average height per view 
agree in height. This means the camera calibration, edge detection and Houghline
extraction where accurately processed and our method is consistent. \\

Although the building appears to have similar building heights, the middle part
of the building is smaller then the front and back part of the building, see
Figure \ref{fig:outputHoughlines2d} and \ref{fig:floriande_back.eps}.
This agrees with the updated 3D model in Figure \ref{fig:outputMutateBuilding}.



\subsection{Conclusion}
Let's answer our research question.
\paragraph{Research question}
Is it possible to use a set of (noisy) skyline points together with an aerial
2D model and a 3D point cloud obtained by \emph{FIT3D toolbox\cite{FIT3D}} to generate a 3D model of the building?

Yes this is possible, we showed that a Houghline transform is a useful method to
discard skyline outliers and find prominent structure in the contour of a
building with a flat roof. We introduced a method to extract a 2D model of a
building from \emph{Openstreetmap\cite{Openstreetmap}} and aligned it to the
scene using the \emph{FIT3D toolbox\cite{FIT3D}} Next, we extracted the skyline
and transformed this to Houghlines which we paired up with their associated
walls. This was used to produce new wall heights which were propagated to the 3D
model.  Existing and novel AI computer vision techniques were powerfully
combined resulting in an reasonable 3D model based on only a few 2D images. 
%todo conclusion is no summary... conclude something..

\subsection{Future research}
\subsubsection{Gravity aligned walls assumption}
In this project we assumed the walls to align with the gravity.
This means the camera must be up right: his parameter \emph{roll} must be
exactly zero when capturing the images. 
In practical use this is not true. We demonstrate this by plotting the 3D point cloud
with the 3D model in Figure .
Although this assumption let us focus on the important issues it would be nice
to incorporate gravity estimation in future research.
Costin Ionita wrote a part of his master thesis about gravity estimation in
\cite{costin}.\\

%todo generate a figure with 2 Hough lines on wall of tree
%As can be seen in Figure %\ref{fig:}
\subsubsection{Double wall height influence}
Sometimes two line segments appear on the same single wall. This means that they have a double
influence on the average wall height, which is unjustified. 
A simple solution would be to add a normalization pre-process step, so each view
has only one wall height vote per wall. A more decent solution would be to
merge the two (or more) line segments to a single line segment. 
Lines that are close and parallel could be merged and averaged.
Lines that lie in each others direction could be merged by increasing the 
Hough transforms \emph{FillGap} parameter.  E.g. for the right wall of the building in 
Figure \ref{fig:outputHoughlines3d} the \emph{FillGap} parameter needs to be at
least as big as the occluding tree.

\subsubsection{Complexity}
In this thesis little is discussed about the computational costs. Because the 
computations are done efficiently (e.g. using matrix multiplications
in MATLAB\cite{matlab}) and off line, the calculation are done in reasonable time.
However, if we want to make the application real time, the next speedup would be useful.\\
To determine the best line-wall association the line segments are now projected to
every wall and for every wall the amount of line-wall overlap is calculated. This
is computational very expensive and looks a bit like an overkill.\\

It would be a significant speedup to reduce the set of walls to only the walls
that contain the middle point of the line segments. To be more concrete the
middle point needs to be calculated by averaging the line segments endpoints,
this middle point is used in the \emph{in polygon} test for every wall.  Next the
line-wall association algorithm only treats the walls that pass this test.

The downside of this method is that it makes the system less accurate because it
will resulting in more false negatives. A line segment that overlaps the wall with only 1/3 could be an
important candidate for the height estimation but because the speedup method it is discarded.
What can be concluded is that there is a trade of in the accurateness of the
height estimation and the computational costs.


\subsubsection{Alternative roofs}
We assumed a flat roof, this doesn't mean that our methods are unusable if
we discard this assumption.
E.g. without adaptations the existing methods could be used to determine the (maximum)
building height. 
If we discard the flat roof assumption the building is allowed to have any
shape. In this situation it should also be possible to extract a
full 3D model.  We will now consider other roof types and discuss what
adaptations the system should require to handle these.  In Figure
\ref{fig:typesOfRoofs}, 6 different roof shapes are displayed.\\

\fig{typesOfRoofs}{Different types of roofs}{0.4}
Consider the \emph{Gable Roof}, it is a roof consisting of two planes
which are not parallel with the facade of the building. This makes the problem
of extracting the 3D model more complex, but not impossible. \\
Because we assume that the roof images are taken from the ground, the skyline
detector will always detect the top of the building. In case of a flat roof
this is also the top of the building walls. In case of an
alternative roof, this will be just the top of the building. The building walls however
could lie a lot lower, therefore something else needs to be developed to find the wall
heights. It would be useful to develop a method that can detect the shape of the roof 
estimate the wall heights and finally generate an entire 3D model.\\
A idea about this is now proposed:\\

The 3D model of a building with a non-flat roof type will consist of a cuboid 
with a custom roof on top of it.  First we would generate the custom roof 
using the shape of the skyline.  Next we would determine the height of the
cuboid. Finally we connect both parts and have a full 3D model.\\

The skyline detector discussed in Chapter 3 could be used to extract the contour
of the roof. Next we would extract a basic 3D model (discussed in Chapter 4) which
is used to transform the roof contour to a frontal view.  Houghline
extraction could be used to extract straight lines which correspond to the base of
the primitive planes of the roof.  For example the Gable roof will return two
straight lines connected in the middle. Next we extend the lines to planes.
We would extract a point cloud of the side wall using the \emph{FIT3D toolbox\cite{FIT3D}} 
to determine the alignment of the planes.\\

Next, a shape is cut out of the plane according to the side view of the building contour.
For example the Hipped roof (Figure \ref{rooftypes}) would imply we have to cut
out a trapezium shape whereas most other roofs require a rectangular shape.
Note the power of combining the frontal and side view to determine the roof planes.
Let us consider the Gable roof, we extract two lines and extend them to skew
rectangular surfaces which are connected in the middle.\\

Next the height of the cuboid needs to be determined.
This could be done by detecting the roof-wall separation on the side view of the
building.  This can be done by detecting horizontal lines using the Hough
transform.
To discard outliers we would only search between the ground plane and the top of
the roof.  The location of the top of the roof could be determined by the highest skyline point that
is present in both frontal and side view. The ground plane could be estimated
using Costins \ref{costin} method.
The roof color often differs from the wall color which produce strong edges. Also
some building contain a gutter connected to the roof. The gutter will also
produce a long strong edge.  These strong edges can be used to find the cubic height
and the lower bound location of the roof.
Finally we connect the roof and the cuboid to obtain a full 3D model.
\\


%\section{My old method of line-wall association}
%\label{sec:oldmethodlwa}
%We take the line segments endpoints and project it onto the building walls. The wall with the shortest
%distance to the camera center will be assigned to the line segment. Points that
%lie outside the polygon are punished.\\
%
%And to update the specific wall we first need to know with a high probability of
%being correct which wall the line segment presents.
%But this method introduces a problem: some of the line segments have endpoints that lie at the corner of the building. These line segments could easily be associated with the neighboring wall. Because the 3D model is a rough estimate this could lead to bad results.
%In the corner case it is not clear to which wall the line segment belongs because both endpoints do not agree on the same wall. To solve this problem some heuristic methods are developed and tested. The following heuristic is both simple and effective.
%The heuristic uses the importance of the middle point of the line segment. This middle point has a low change of being on a building corner and the on average biggest change of being on the wall we are looking for.
%Therefor we discard the endpoints and use the middle point the endpoints to determine the right wall.
%This middle point is intersected with all planes spanned by the walls. The line segment is stored to the wall with the shortest distance.
%The output of this part of the algorithm is for every wall a bunch of associated line segments originated from different views.\\
%
%in section Results was this text:\\
%The left and middle line segment of Figure \ref{fig:outputHoughlines2d} 
%are
%a good example of the corner problem. Both endpoints that lie on the corner could easily be associated
%with the wrong wall (even if the rough 3D model is very accurate). Fortunately
%we use the middle point of the line segment to determine the correct wall. This
%works well as its 100\% accurate (for this dataset).
%
