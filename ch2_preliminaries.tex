% DRAFT
\section{Preliminaries on Computer Vision}
\label{sec:ch2}
In this chapter we discuss the basic computer vision techniques that are used for
the skyline detection, and window detection.  Furthermore we discuss 3rd
party software, the \emph{FIT3D toolbox} \cite{FIT3D} which is used in 3D building extraction and facade rectification.

\subsection{Hough transform}
\label{sec:prelimHough}
\subsubsection{Theory}
	A widely used method for extracting line segments is the Hough transform
	\cite{Hough}.
	In the Hough transform, the main idea is to consider the characteristics of a
	straight line not as its image points $(x1, y1)$, $(x2, y2)$, but in
	terms of the parameters of the straight line formula $y = mx + b$. i.e., the
	slope parameter $m$ and the intercept parameter $b$.


	The Hough transform transforms the line $y = mx + b$ 
	to a point $(b,m)$ in parameter space.
	With this representation it is impossible to describe a vertical line as 
	the slope $m$ is infinite.
	Therefore it is better to use a different pair of parameters, denoted $r$ and $\theta$ .  These are the Polar Coordinates.

	The parameter $r$ represents the distance between the origin and the line  and $\theta$ is the angle of the vector orthogonal to the line.
	Using this parameterization, the equation of the line can be written as

\[	    y = \left(-{\cos\theta\over\sin\theta}\right)x + \left({r\over{\sin\theta}}\right) \]

	\[r = x \cos \theta+y\sin \theta \]
	
	This means that a straight line in $(x,y)$ space appears as a sinusoidal
	curve in the Hough parameter $(r,\theta)$ space.  Let's see an example, the
	following image is transformed into the space $(r,\theta)$.
	
	%todo introduce the accumulator array,
	%todo transform gif to eps 
	\fig{HoughTransform_edge.eps}{An input image, consisting of eight straight lines, for the Hough transform}{0.5}
	\figsHor{HoughTransform_peaks}{HoughTransform_peaks1.eps}{HoughTransform_peaks2.eps}{Hough transform} {$(r, \theta)$ values}{$(r, \theta)$ accumulator array (quantized)}

	 
	 As you can see for every edge point 
	 in Figure \ref{fig:HoughTransform_edge.eps} 
	 a curve is generated in $(r,\theta)$ space in Figure 
\ref{fig:HoughTransform_peaks1.eps}.
	 On eight positions (white dots) the number of intersecting sinusoidal
	 curves is high, these position correspond to the eight separate straight
	 line segments in Figure \ref{fig:HoughTransform_edge.eps} .
	
	This makes the problem of detecting straight lines in finding peaks in the 
	Hough parameter $(r,\theta)$ space.

\subsubsection{Implementation}
	The input of a Hough transform is a binary image. In our research it is the output of 
	the skyline detector (\ref{sec:skylinedetection}). In the case of window
	detection (\ref{sec:windowDetection}) it is the output of an edge image.\\

	The Hough transform develops an accumulator array of a quantized parameter space $(r, \theta)$.

	It loops through the binary image and for each positive value 
	it generates all possible lines, quantized $(r, \theta)$ pairs, that intersect with this point.
	For each candidate it increases a vote in the accumulator array.
	Lines $(r, \theta)$ that receive a large amount of votes
	i.e. the dots in Figure \ref{fig:HoughTransform_peaks1.eps} are the found straight lines in the $(x,y)$ space.

	These positions are found by looking for local maxima in the accumulator array.

\subsubsection{$\theta$ constrained Hough transform}
The accumulator array consist of two dimensions $r$ and $\theta$.
$\theta$ typically ranges from $[-90..90)$ resulting in 180 unique bins.
Note that a line with $(r, \theta) = (t,j)$ can also be represent by the identical $(-t, j-90)$, e.g. $(4, 135)$ == $(-4, 45)$.
This makes it possible to represent every line with the interval $[-90..90)$.\\

Sometimes we want to find lines that have a certain angle.
For example the skyline of a building will appear about horizontal. If we
want to detect windows we would like to detect edges in the horizontal and vertical directions.
This can easily achieved by adjusting the $\theta$ range.
For example if one would detect lines in the horizontal direction of
a photograph of a building taken by a user, $\theta = [-10..0..10]$.Although
only $\theta = 0$ presents an exact horizontal line we broaden the interval
because the user hardly ever holds the camera exactly orthogonal.
	
\subsubsection{Matlab parameters}
We used a standard Matlab implementation of the Hough transform for straight lines.  This implementation comes with some interesting parameters:\\

	The \emph{minimum length} parameter specifies the minimum length that a line must have to be valid. This is especially interesting if we want to detect a large straight skyline or if we want to discard lines that are to small to form for example a window.\\

	Furthermore it contains the parameter \emph{FillGap} that specifies the distance
	between two line segments associated with the same $(r, \theta)$ pair.
	When this inter line segment distance is less then the \emph{FillGap} parameter, it merges the line segments into a single line segment. In our application this parameter is of particular interest when we want to merge lines that are interrupted by for example an occluding tree or street lamp.\\


% \subsubsection{Other shapes}
% The Hough transform is not bound to finding straight lines. In fact it could be
% applied to any space which can be represented by a set of parameters.  For
% instance, a circle can be transformed into a set of three parameters, $(x,y,r)$
% representing its center and radius. This makes the Hough space three
% dimensional.  For more complicated shapes, the 
% \emph{Generalized Hough transform} is used.
% It uses a feature that votes for a particular position, orientation and/or scaling of a certain predefined shape.  The shapes are predefined in a look-up table.\\

\subsection{Coordination systems}
%todo doorlezen
In this thesis three different coordinate reference systems are used. 
The first one is located at the image level and describes the
location of the pixel in 2D. It is called the Image Coordinate Frame (ICF), in
Figure \ref{fig:coordinateSystems.eps} ($x_0,y_0$) span this frame.
The second is the Camera Coordinate
Frame (CCF) and it involves the projection of the image point (x,y) to the
retina of the camera in 3D.  The origin of the CCF is equal to the center of the
camera. If the camera rotates, it rotates around this point. The coordinates are
expressed in (XYZ). \\
The last system is the World Coordinate Frame (WCF) and it is used to describe the positions of the 3D model of the
scene and the position of the cameras in the world in (ijk). An overview of the
systems can be found in Table \ref{tab:coord}.

\fig{coordinateSystems.eps}{Coordinate systems}{1}
\begin{table}[ht]
	\caption{Coordinate reference systems with their properties}
	\label{tab:coord}
	\begin{tabular}{|l||c|c|c|}
	\hline
	Reference system name		&	abreviation	&	dimensions	&	axis\\
	\hline
	\hline
	Image Coordinate Frame	&   ICF			&	2D			& 	x,y\\
	\hline
	Camera Coordinate Frame	&   CCF			&	3D			& 	X,Y,Z\\
	\hline
	World Coordinate Frame	&   WCF			&	3D			& 	i,j,k\\
	\hline
	\end{tabular}
\end{table}
%todo use costins work and refer 
%todo use FIT3D toolbox paper


%  The camera can be described in the same coordinate system as the 3D point we are
%  looking for, we call this the world coordinate system. 
%  The camera center is a value in $\mathbb{R}$3 that represents the camera's position in the world
%  coordinate system. If the camera rotates, it rotates around this point.
%  \TODO{ elaborate}
%  
%  
%  %TODO IMPORTANT!!!!!!!!!!!!!!!!!!!!!!!!
%\subsection{Getting the camera centers and viewing directions}

\subsection{Camera calibration}
\label{sec:cameraCalibration}
Camera calibration is done to determine
the relationship between what appears on the image (or camera retina) and where
 it is located in the 3D world. This involves calculating the camera's intrinsic
 and extrinsic parameters.

\subparagraph{Intrinsic parameters}
The intrinsic parameters contain information about the internal parameters
of the camera.  These are focal length, pixel aspect ratio, and principal point.
%todo explain principal point

These parameters come together in a calibration matrix K.  The Floriande
dataset (originated from the Fit3D toolbox \cite{FIT3D}) comes with a
calibration matrix K.  For
the other datasets we calculated K with the Bouguet toolbox
\cite{bouguet}.
This method involves taking images of a chessboard in different positions and
orientations. An algorithm detects the grid on the chessboard and monitors its
transformations under the different images.  If enough images are given (at
least 10) and the images contain enough variety in chessboard pose, the Bouguet
toolbox can calculate the intrinsic parameters quitte accurate.
More details about this method can be found in \cite{bouguet}.

\subparagraph{Extrinsic parameters}
%todo praten over coordinaat stelsels
%\paragraph{Getting the camera centers and viewing directions}
%todo introduce viewing direction
The extrinsic parameters present the center of the locations of the camera
and the camera's headings. These are unique for every view.
%todo explain rotation, translation
In some systems these are recorded by measuring the cameras position and
headings at the scene. In other systems this is computed afterwards (from the
images).  We calculated the values also afterwards and used existing software for
this: \emph{FIT3D toolbox} \cite{FIT3D}, details of this process is
explained next.


\subsection{FIT3D toolbox \ref{FIT3D}}
\label{sec:prelimFIT3D}
%todo give FIT3D a proper intro
The \emph{FIT3D toolbox} \ref{FIT3D} is used for several aims in this thesis.
It is used in the window detection module to rectify the facades.
and the skyline detection used \emph{FIT3D} to extract a 3D model.

In order to extract this 3D model a series of frames (originating from different
views) is used to estimate the relation of the camera coordinates to the
world coordinates, Next, the result is used to extract a point cloud of matching
features. Finally this point cloud is converted to planes which correspond with the walls of the building.

Because the toolbox plays an assisting role we explain the steps briefly.
Detailed knowledge about the methods can be found in \ref{FIT3D}.

%3D model reconstruction
%	2d model reconstruction
\subsubsection{Multiple views}
\emph{FIT3D} uses multiple views to gather information about the 3D structure of the
building. The toolbox comes with a prepared dataset of 7 consecutive images (steady (zoom, lightning,
etc.) parameters) of a scene.  \emph{FIT3D} doesn't require the input images to be
chronological however they need to have sufficient overlap. 
\fig{FIT3DImgSequence.eps}{Example series of 7 consecutive frames, (dataset:
\emph{FIT3D toolbox}\cite{FIT3D})}{0.3}
%/media/Storage/scriptie/FIT3D/generate3dModel/3dcFiles


%todo explain what a camera coordinate is (homoge px coord)
\subsubsection{Relating the camera coordinates to the world coordinates}
%Getting the camera's centers and camera's heading (Extrinsic parameters)}
The different views are used to estimate the relation of the camera frame
coordinates to the world coordinates, (the extrinsic parameters).  In other
words the different positions of the camera centers and camera's heading are
estimated.  This is done by calculating the relative motion between the
different views.\\

The frame to frame motion is calculated by extracting about 25k SIFT features of
each frame.  Next, SIFT descriptors are used to describe and match the features
within the consecutive frames.  Not all features will overlap or match in the
frames therefor RANSAC is used to robustly remove the outliers.  After this an
\emph{8-point algorithm} together with a voting mechanism is used to extract the
relative camera motion.\\

The frames are matched one by one which returns an estimation of the camera
motion. Because this estimation is not accurate enough, a 3-frame match is done 
This result is more accurate but comes with a certain
amount of re-projection error which is minimized using a numerical
iterative method called \emph{bundle adjustment}.  \\

From every frame the camera motion is stored relative to the first frame.
The motion is stored as a rotation and translation in homogeneous
form (3x4) $[R|t]$. This is gathered for every frame in a (7x3x4) projection matrix $P$.
$P$ can be used to translate camera coordinates of a specific view 
 to 3D world coordinates and vice versa.

\subsubsection{3D point cloud extraction}
The next step is to use this projection matrix $P$ to obtain a set of 3D points.
These correspond to the matching SIFT features of the different views.  This
is achieved using a \emph{linear triangulation} method. \\
The result is an accurate 3D point cloud of the building, see Figure
\ref{fig:FIT3DImgSequence.eps}.
Next a RANSAC based plane fitter is used to accurately fit planes through
the 3D points, see Figure \ref{fig:FIT3D_planes.eps}.\\
\fig{FIT3D_planes.eps}{Fitted planes}{0.45}

In this thesis the plane fitting step is skipped and a new method is
used to obtain the 3D model. This is explained in \ref{sec:generate3dModel}.

% werkwijze:
% FIT3D paper doornemen
% wat heb ik gedaan wat doet FIT3D?
% 	in code kijken
% generate illustrative images
% 

%----------------------------------------------------------------------



%  
%  
%  \subsection{Homogeneous coordinate}
%  \TODO{ explain how to add extra homogeneous dimension (x,y,1) and why this is}
%  
%  
%\subsection{Planes and walls}
%\paragraph{Planes and walls}
%  \label{sec:planeswalls}
%  \TODO{ How to span plane by wall coords}
%  
%\subsection{Axis angle representation}
%\paragraph{Axis angle representation}
%  \label{sec:axisAngle}
%  \TODO{Axis angle representation, wikipedia}
%  
%\subsection{Gaussian blur}
%\paragraph{Gaussian smoothing}
%  %matlab documentation, wikipedia
