% DRAFT
\section{Preliminaries on Computer Vision}
In this chapter we discuss the basic computer vision techniques that are used for
the skyline detection, 3D building extraction and window detection.

\subsection{Hough transform}
\label{sec:prelimHough}
\subsubsection{Theory}
	A widely used method for extracting line segments is the Hough transform
	\cite{Hough}.
	In the Hough transform, the main idea is to consider the characteristics of a
	straight line not as its image points $(x1, y1)$, $(x2, y2)$, but in
	terms of the parameters of the straight line formula $y = mx + b$. i.e., the
	slope parameter $m$ and the intercept parameter $b$.


	The Hough transform transforms the line $y = mx + b$ 
	to a point $(b,m)$ in parameter space.
	With this representation it is impossible to describe a vertical line as 
	the slope $m$ is infinite.
	Therefore it is better to use a different pair of parameters, denoted $r$ and $\theta$ .  These are the Polar Coordinates.

	The parameter $r$ represents the distance between the origin and the line  and $\theta$ is the angle of the vector orthogonal to the line.
	Using this parameterization, the equation of the line can be written as

\[	    y = \left(-{\cos\theta\over\sin\theta}\right)x + \left({r\over{\sin\theta}}\right) \]

	\[r = x \cos \theta+y\sin \theta \]
	
	This means that a straight line in $(x,y)$ space appears as a sinusoidal
	curve in the Hough parameter $(r,\theta)$ space.  Let's see an example, the
	following image is transformed into the space $(r,\theta)$.
	
	%todo introduce the accumulator array,
	%todo transform gif to eps 
	\fig{HoughTransform_edge.eps}{An input image, consisting of eight straight lines, for the Hough transform}{0.5}
	\figsHor{HoughTransform_peaks}{HoughTransform_peaks1.eps}{HoughTransform_peaks2.eps}{Hough transform} {$(r, \theta)$ values}{$(r, \theta)$ accumulator array (quantized)}

	 
	 As you can see for every edge point 
	 in Figure \ref{fig:HoughTransform_edge.eps} 
	 a curve is generated in $(r,\theta)$ space in Figure 
\ref{fig:HoughTransform_peaks1.eps}.
	 On eight positions (white dots) the number of intersecting sinusoidal
	 curves is high, these position correspond to the eight separate straight
	 line segments in Figure \ref{fig:HoughTransform_edge.eps} .
	
	This makes the problem of detecting straight lines in finding peaks in the 
	Hough parameter $(r,\theta)$ space.

\subsubsection{Implementation}
	The input of a Hough transform is a binary image. In our research it is the output of 
	the skyline detector (\ref{sec:skylinedetection}). In the case of window
	detection (\ref{sec:windowDetection}) it is the output of an edge image.\\

	The Hough transform develops an accumulator array of a quantized parameter space $(r, \theta)$.

	It loops through the binary image and for each positive value 
	it generates all possible lines, quantized $(r, \theta)$ pairs, that intersect with this point.
	For each candidate it increases a vote in the accumulator array.
	Lines $(r, \theta)$ that receive a large amount of votes
	i.e. the dots in Figure \ref{fig:HoughTransform_peaks1.eps} are the found straight lines in the $(x,y)$ space.

	These positions are found by looking for local maxima in the accumulator array.

\subsubsection{$\theta$ constrained Hough transform}
The accumulator array consist of two dimensions $r$ and $\theta$.
$\theta$ typically ranges from $[-90..90)$ resulting in 180 unique bins.
Note that a line with $(r, \theta) = (t,j)$ can also be represent by the identical $(-t, j-90)$, e.g. $(4, 135)$ == $(-4, 45)$.
This makes it possible to represent every line with the interval $[-90..90)$.\\

Sometimes we want to find lines that have a certain angle.
For example the skyline of a building will appear about horizontal. If we
want to detect windows we would like to detect edges in the horizontal and vertical directions.
This can easily achieved by adjusting the $\theta$ range.
For example if one would detect lines in the horizontal direction of
a photograph of a building taken by a user, $\theta = [-10..0..10]$.Although
only $\theta = 0$ presents an exact horizontal line we broaden the interval
because the user hardly ever holds the camera exactly orthogonal.
	
\subsubsection{Matlab parameters}
We used a standard Matlab implementation of the Hough transform for straight lines.  This implementation comes with some interesting parameters:\\

	The \emph{minimum length} parameter specifies the minimum length that a line must have to be valid. This is especially interesting if we want to detect a large straight skyline or if we want to discard lines that are to small to form for example a window.\\

	Furthermore it contains the parameter \emph{FillGap} that specifies the distance
	between two line segments associated with the same $(r, \theta)$ pair.
	When this inter line segment distance is less then the \emph{FillGap} parameter, it merges the line segments into a single line segment. In our application this parameter is of particular interest when we want to merge lines that are interrupted by for example an occluding tree or street lamp.\\


\subsubsection{Other shapes}
The Hough transform is not bound to finding straight lines. In fact it could be
applied to any space which can be represented by a set of parameters.  For
instance, a circle can be transformed into a set of three parameters, $(x,y,r)$
representing its center and radius. This makes the Hough space three
dimensional.  For more complicated shapes, the 
\emph{Generalized Hough transform} is used.
It uses a feature that votes for a particular position, orientation and/or scaling of a certain predefined shape.  The shapes are predefined in a look-up table.\\

[Draftnote: topcis below are not finished at the time of writing]
%\subsection{Coordination systems}
\paragraph{Coordination systems}
%todo use costins work and refer 
%todo use fit3d toolbox paper


%  The camera can be described in the same coordinate system as the 3D point we are
%  looking for, we call this the world coordinate system. 
%  The camera center is a value in $\mathbb{R}$3 that represents the camera's position in the world
%  coordinate system. If the camera rotates, it rotates around this point.
%  \TODO{ elaborate}
%  
%  
%  %TODO IMPORTANT!!!!!!!!!!!!!!!!!!!!!!!!
%\subsection{Getting the camera centers and viewing directions}


top down explanation
	frame to frame motion
	scale ratio calculation


\subsection{FIT3d toolbox \ref{fit3d}}
\label{sec:prelimFit3d}
%intro
The FIT3d toolbox \ref{fit3d} is used for several aims in this thesis.
The skyline detection used fit3d to extract a 3d model 
and the window detection module used this 3d model to rectify the facades.
%todo One of the planes of the 3D model was used to rectify the
%facade,

Extracting a 3d model is done in fit3d in a few steps which we explain briefly.
More detailed knowledge about the methods can be found in \ref{fit3d}.

%3D model reconstruction
%	2d model reconstruction

\subsubsection{Getting the camera centers and viewing directions}
% explain the notion motion


% refer to coordinate systems
% refer to intrinsic and extrinsic coords
We started by taking a series of 7 consecutive images (steady (zoom, lightning,
etc.) parameters) of a scene.  The images don't need to be chronological but need to have sufficient overlap. 

%/media/Storage/scriptie/fit3d/generate3dModel/3dcFiles
\fig{fit3dImgSequence.eps}{Example series of 7 consecutive frames, (dataset: FIT3D toolbox\cite{Fit3d})}{0.3}

We calculated the motion of the camera between the frames in a few steps
First we extract about 25k SIFT features of each frame.  Then we use
SIFT descriptors to describe and match the features within the consecutive
frames.  Not all features will overlap or match in the frames therefor RANSAC is used to
robustly remove the outliers.  After this an \emph{8-point algorithm} together with a
voting mechanism is used to extract the camera motion. For details please read
\cite{Fit3d}.

The frames where matched one by one which returns an estimation of the camera
motion that is not accurate enough.  Therefore a 3-frame match is done which
gives more accurate results.  Unfortunately this result comes with a certain
amount of re-projection error, this error is minimized using a numerical
iterative method called \emph{bundle adjustment}.  The final result is a very
accurate estimation of the camera motion.




\subsubsection{3D point cloud extraction and plane fitting}
The next step is to use this camera motion to obtain a set of 3D points
(corresponding to the matching image features).  This is achieved using a
\emph{linear triangulation} method. 

Next a RANSAC based plane fitter is used to accurately fit a plane through
the 3D points. 
\emph{todo example figure}


werkwijze:
fit3d paper doornemen
mail isaac doornemen
wat heb ik gedaan wat doet fit3d?
	in code kijken
generate illustrative images


steps:
relatieve cc's
	sift features point cloud
	get P from fit3d 

absolute cc's (align 3d model)

	project point cloud in Y direction
	make 2d model by hand  (example images)
		or use google maps and fit it on it

	



%todo plaatje van scene met een 3d model erin


\subsubsection{Aligning the 2d model}
%todo plaatje 2d model





%  \label{sec:cameracenters}
%  \TODO{ Explain here about fit3d toolbox}
%  
%\subsection{Calibrating the camera with the Bouguet toolbox}
\paragraph{Calibrating the camera with the Bouguet toolbox}
%  \label{sec:bouguet}
%  TODO
%  \TODO{TODO see ref 15 and fit3d toolbox p4}
%  
%  	%\paragraph{Getting the calibration matrix}
%  	%TODO inspiration
%  	% isaacs paper 
%  	% google 
%  	% matlab help bouget 
%  	%
%  
%  
%  \subsection{Homogeneous coordinate}
%  \TODO{ explain how to add extra homogeneous dimension (x,y,1) and why this is}
%  
%  
%\subsection{Planes and walls}
\paragraph{Planes and walls}
%  \label{sec:planeswalls}
%  \TODO{ How to span plane by wall coords}
%  
%\subsection{Axis angle representation}
\paragraph{Axis angle representation}
%  \label{sec:axisAngle}
%  \TODO{Axis angle representation, wikipedia}
%  
%\subsection{Gaussian blur}
\paragraph{Gaussian blur}
%  %matlab documentation, wikipedia
