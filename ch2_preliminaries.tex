% DRAFT
\section{Preliminaries on Computer Vision}
In this chapter we discuss the basic computer vision techniques that are used for
the skyline detection, 3D building extraction and window detection.

\subsection{Hough transform}
\label{sec:prelimHough}
\subsubsection{Theory}
	A widely used method for extracting line segments is the Hough transform
	\cite{Hough}.
	In the Hough transform, the main idea is to consider the characteristics of a
	straight line not as its image points $(x1, y1)$, $(x2, y2)$, but in
	terms of the parameters of the straight line formula $y = mx + b$. i.e., the
	slope parameter $m$ and the intercept parameter $b$.


	The Hough transform transforms the line $y = mx + b$ 
	to a point $(b,m)$ in parameter space.
	With this representation it is impossible to describe a vertical line as 
	the slope $m$ is infinite.
	Therefore it is better to use a different pair of parameters, denoted $r$ and $\theta$ .  These are the Polar Coordinates.

	The parameter $r$ represents the distance between the origin and the line  and $\theta$ is the angle of the vector orthogonal to the line.
	Using this parameterization, the equation of the line can be written as

\[	    y = \left(-{\cos\theta\over\sin\theta}\right)x + \left({r\over{\sin\theta}}\right) \]

	\[r = x \cos \theta+y\sin \theta \]
	
	This means that a straight line in $(x,y)$ space appears as a sinusoidal
	curve in the Hough parameter $(r,\theta)$ space.  Let's see an example, the
	following image is transformed into the space $(r,\theta)$.
	
	%todo introduce the accumulator array,
	%todo transform gif to eps 
	\fig{HoughTransform_edge.eps}{An input image, consisting of eight straight lines, for the Hough transform}{0.5}
	\figsHor{HoughTransform_peaks}{HoughTransform_peaks1.eps}{HoughTransform_peaks2.eps}{Hough transform} {$(r, \theta)$ values}{$(r, \theta)$ accumulator array (quantized)}

	 
	 As you can see for every edge point 
	 in Figure \ref{fig:HoughTransform_edge.eps} 
	 a curve is generated in $(r,\theta)$ space in Figure 
\ref{fig:HoughTransform_peaks1.eps}.
	 On eight positions (white dots) the number of intersecting sinusoidal
	 curves is high, these position correspond to the eight separate straight
	 line segments in Figure \ref{fig:HoughTransform_edge.eps} .
	
	This makes the problem of detecting straight lines in finding peaks in the 
	Hough parameter $(r,\theta)$ space.

\subsubsection{Implementation}
	The input of a Hough transform is a binary image. In our research it is the output of 
	the skyline detector (\ref{sec:skylinedetection}). In the case of window
	detection (\ref{sec:windowDetection}) it is the output of an edge image.\\

	The Hough transform develops an accumulator array of a quantized parameter space $(r, \theta)$.

	It loops through the binary image and for each positive value 
	it generates all possible lines, quantized $(r, \theta)$ pairs, that intersect with this point.
	For each candidate it increases a vote in the accumulator array.
	Lines $(r, \theta)$ that receive a large amount of votes
	i.e. the dots in Figure \ref{fig:HoughTransform_peaks1.eps} are the found straight lines in the $(x,y)$ space.

	These positions are found by looking for local maxima in the accumulator array.

\subsubsection{$\theta$ constrained Hough transform}
The accumulator array consist of two dimensions $r$ and $\theta$.
$\theta$ typically ranges from $[-90..90)$ resulting in 180 unique bins.
Note that a line with $(r, \theta) = (t,j)$ can also be represent by the identical $(-t, j-90)$, e.g. $(4, 135)$ == $(-4, 45)$.
This makes it possible to represent every line with the interval $[-90..90)$.\\

Sometimes we want to find lines that have a certain angle.
For example the skyline of a building will appear about horizontal. If we
want to detect windows we would like to detect edges in the horizontal and vertical directions.
This can easily achieved by adjusting the $\theta$ range.
For example if one would detect lines in the horizontal direction of
a photograph of a building taken by a user, $\theta = [-10..0..10]$.Although
only $\theta = 0$ presents an exact horizontal line we broaden the interval
because the user hardly ever holds the camera exactly orthogonal.
	
\subsubsection{Matlab parameters}
We used a standard Matlab implementation of the Hough transform for straight lines.  This implementation comes with some interesting parameters:\\

	The \emph{minimum length} parameter specifies the minimum length that a line must have to be valid. This is especially interesting if we want to detect a large straight skyline or if we want to discard lines that are to small to form for example a window.\\

	Furthermore it contains the parameter \emph{FillGap} that specifies the distance
	between two line segments associated with the same $(r, \theta)$ pair.
	When this inter line segment distance is less then the \emph{FillGap} parameter, it merges the line segments into a single line segment. In our application this parameter is of particular interest when we want to merge lines that are interrupted by for example an occluding tree or street lamp.\\


\subsubsection{Other shapes}
The Hough transform is not bound to finding straight lines. In fact it could be
applied to any space which can be represented by a set of parameters.  For
instance, a circle can be transformed into a set of three parameters, $(x,y,r)$
representing its center and radius. This makes the Hough space three
dimensional.  For more complicated shapes, the 
\emph{Generalized Hough transform} is used.
It uses a feature that votes for a particular position, orientation and/or scaling of a certain predefined shape.  The shapes are predefined in a look-up table.\\

[Draftnote: topcis below are not finished at the time of writing]
%\subsection{Coordination systems}
\paragraph{Coordination systems}
%todo use costins work and refer 
%todo use FIT3D toolbox paper
%todo plaatje camera


%  The camera can be described in the same coordinate system as the 3D point we are
%  looking for, we call this the world coordinate system. 
%  The camera center is a value in $\mathbb{R}$3 that represents the camera's position in the world
%  coordinate system. If the camera rotates, it rotates around this point.
%  \TODO{ elaborate}
%  
%  
%  %TODO IMPORTANT!!!!!!!!!!!!!!!!!!!!!!!!
%\subsection{Getting the camera centers and viewing directions}


\subsection{FIT3D toolbox \ref{fit3d}}
\label{sec:prelimFIT3D}
%todo give fit3d a proper intro
The FIT3D toolbox \ref{fit3d} is used for several aims in this thesis.
It is used in the window detection module to rectify the facades.
and the skyline detection used FIT3D to extract a 3D model.

In order to extract this 3D model a series of frames (originating from different
views) is used to estimate the relation of the camera frame coordinates to the
world coordinates, Next, the result is used to extract a point cloud of matching
features. Finally this point cloud is converted to planes which correspond with the walls of the building.

Because the toolbox plays an assisting role we explain the steps briefly.
Detailed knowledge about the methods can be found in \ref{fit3d}.

%3D model reconstruction
%	2d model reconstruction
\subsubsection{Multiple views}
FIT3D uses multiple views to gather information about the 3D structure of the
building. The toolbox comes with a prepared dataset of 7 consecutive images (steady (zoom, lightning,
etc.) parameters) of a scene.  FIT3D doesn't require the input images to be
chronological however they need to have sufficient overlap. 
\fig{fit3dImgSequence.eps}{Example series of 7 consecutive frames, (dataset: FIT3D toolbox\cite{FIT3D})}{0.3}
%/media/Storage/scriptie/fit3d/generate3dModel/3dcFiles


%todo explain what a camera coordinate is (homoge px coord)
\subsubsection{Relating the camera frame coordinates to the world coordinates}
%Getting the camera's centers and camera's heading (Extrinsic parameters)}
The different views are used to estimate the relation of the camera frame
coordinates to the world coordinates, (the extrinsic parameters).  In other
words the different positions of the camera centers and camera's heading are
estimated.  This is done by calculating the relative motion between the
different views.\\

The frame to frame motion is calculated by extracting about 25k SIFT features of
each frame.  Next, SIFT descriptors are used to describe and match the features
within the consecutive frames.  Not all features will overlap or match in the
frames therefor RANSAC is used to robustly remove the outliers.  After this an
\emph{8-point algorithm} together with a voting mechanism is used to extract the
relative camera motion.\\

The frames are matched one by one which returns an estimation of the camera
motion. Because this estimation is not accurate enough, a 3-frame match is done 
This result is more accurate but comes with a certain
amount of re-projection error which is minimized using a numerical
iterative method called \emph{bundle adjustment}.  \\

From every frame the camera motion is stored relative to the first frame.
The motion is stored as a rotation and translation in homogeneous
form (3x4) $[R|t]$. This is gathered for every frame in a (7x3x4) projection matrix $P$.
$P$ can be used to translate camera coordinates of a specific view 
 to 3D world coordinates and vice versa.

\subsubsection{3D point cloud extraction}
The next step is to use this projection matrix $P$ to obtain a set of 3D points.
These correspond to the matching SIFT features of the different views.  This
is achieved using a \emph{linear triangulation} method. \\
The result is an accurate 3D point cloud of the building, see Figure
\ref{fig:fit3dImgSequence.eps}.
Next a RANSAC based plane fitter is used to accurately fit planes through
the 3D points, see Figure \ref{fig:fit3d_planes.eps}.\\
\fig{fit3d_planes.eps}

In this thesis the plane fitting step is skipped and a new method is
used to obtain the 3D model. This is explained in \ref{sec:generate3dModel}.

% werkwijze:
% FIT3D paper doornemen
% wat heb ik gedaan wat doet FIT3D?
% 	in code kijken
% generate illustrative images
% 
% 
% steps:
% relatieve cc's

%----------------------------------------------------------------------



%\subsection{Calibrating the camera with the Bouguet toolbox}
\paragraph{Calibrating the camera with the Bouguet toolbox}
%  \label{sec:bouguet}
%  TODO
%  \TODO{TODO see ref 15 and FIT3D toolbox p4}
%  
%  	%\paragraph{Getting the calibration matrix}
%  	%TODO inspiration
%  	% isaacs paper 
%  	% google 
%  	% matlab help bouget 
%  	%
%  
%  
%  \subsection{Homogeneous coordinate}
%  \TODO{ explain how to add extra homogeneous dimension (x,y,1) and why this is}
%  
%  
%\subsection{Planes and walls}
\paragraph{Planes and walls}
%  \label{sec:planeswalls}
%  \TODO{ How to span plane by wall coords}
%  
%\subsection{Axis angle representation}
\paragraph{Axis angle representation}
%  \label{sec:axisAngle}
%  \TODO{Axis angle representation, wikipedia}
%  
%\subsection{Gaussian blur}
\paragraph{Gaussian blur}
%  %matlab documentation, wikipedia
