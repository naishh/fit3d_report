% set ignorecase

\section{Window detection}
\label{sec:windowDetection}
\subsection{Introduction}
Semantic interpretation of urban scenes is used in a wide range of applications.
We deal with an important aspect of semantic urban scene interpretation, Window detection. 

% thesis outline:
This chapter is organised as follows, we start with an variety applications that
use semantic interpretation of urban scenes.  Then we discuss related work and
put our work in context.  Next we describe our first window detection approach
that is invariant to viewing direction.  After this we present a facade
rectification method. Next this method is used for our second method that assumes
orthogonal and aligned windows.  Finally we show and discuss results. 

\subsection{Application examples}
\paragraph{3d City models} 
	Manual creation of 3d models is a time consuming and expensive procedure.
	Therefore semantic models are used for semi automatic 3d
	reconstruction/modelling.
	 %[Procedural Modeling of Buildings].  
	The semantic understanding is also used in 3d city models which are
	generated from aerial or satellite imagery.  The detected (doors and)
	windows are mapped to the model to increase the level of detail. 
	Some other applications can automatically extract a CAD-like model of
	the building surface.

\paragraph{Historical buildings documentation and deformation analysis}
	In some field of research, Historical buildings are documented. The complex
	structures that are contained in the facades are recorded and reconstructed.
	Window detection plays a central role in this. 
	Another field of research is the analysis of building deformation in areas
	containing old buildings.  Window detection provides information about the
	region of interest that could be tracked over time for an accurate
	deformation analysis.
	%[A SEMI-AUTOMATIC IMAGE-BASED MEASUREMENT SYSTEM]


\paragraph{Interactive 3d models}
	There are some virtual training applications that are designed for
	emergency response who require interaction with a 3d model.  
	For the simulation to be realistic it is important to have a model that is
	of high visual quality and has sufficient semantic detail (i.e. contains
	windows).  This is also the case for a fly-through visualization of a street with
	buildings.
	Other applications that require semantic 3d models are virtual tourism,
	visual impact analysis, driving simulation and military simulation systems.
	\fig{p_simulation_people.eps}{Simulation environment}{0.3}

\paragraph{Augmented reality}
	Some mobile platforms apply augmented reality using facade and window
	detection to make an accurate overlay of the building. An example overlay is the
	same building but 200 years earlier.  Semantical information is used to not
	only identify a respective building, but also find his exact location in the
	image.  The accuracy and realistic level of the 3d model are vital for a
	successful simulation.  And because the applications are mobile, very fast
	building understanding algorithms are required.  
	Window detection plays an
	important role in these processes as the size and location of the windows
	supply an effective descriptor that can be used for robust and fast building
	identification.  Furthermore it provides an accuratly alignment of the
	overlay.

\paragraph{Building recognition and urban planning}
	Building recognition is used in the field of urban planning where the semantic 3d
	models are used to provide important references to the city scenes from the
	street level.
	Building recognition is done using large image datasets where the
	buildings are mostly described by local information descriptors.  
	Some approaches try to describe the 3D building with laser range data. Some methods fuse the laser data with
	ground images. However those generated 3D models are a mesh structure which doesn't make the facade structure explicit.
	For a more accurate disambiguation, other types of contextual information are
	desired.  The semantical interpretation of the facade can provide this need.
	In this context, window detection can be used as a strong discriminator.\\

We can conclude that window detection plays an important role in the
interpretation of urban scenes and is applied in a wide range of domains.  



\TODO{in state of art een lijn creeren, opbouwen, in context plaatsen, theoretisch afwimplene}
\subsection{State of art window detection} % related work
A large amount of research is done on semantical interpretation of urban scenes. 
First we briefly discuss the research that is done on window detection using
approaches that differ from our approach.  After this, we discuss state of the art
window detection that has a big overlap with our approach in detail.\\

\subsubsection{Alternative approaches on window detection}
Muller et all \cite{Muller_procedural} detect regularity and symmetry in the building. The
symmetry is detected in the vertical (floors) and horizontal (window
rows) direction.
They use shape grammars to divide the building wall in tiles, windows, doors etc.
The results are used to derive a 3d model of high 3d visual quality.
\figsSmall{label_window_nice_geometry}{p_window_nice_geometry1.eps}{p_window_nice_geometry2.eps}{Results of Muller et all}{original texture}{reconstruction}
Although they achieved some interesting results their method has some
disadvantages.  As their method is fully based on detecting symmetrie, they have
to assume repeating and aligned windows: this constraints the variety of scenes
the system can handle.
Furthermore they match template window objects which they predefine, this 
constraints the variety of windowtypes that could be matched.  At last 
they use expensive algorithms that make it impossible for the system
to run in real time.  \\

Using a thermal camera, Sirmacek \cite{Sirmacek_thermal}
detects heat leakage on building walls as an indicator for doors or windows.
Windows are detected with L-shaped features as set of \emph{steerable filters}.
The windows are grouped using \emph{perceptual organization rules}:
they search and group intersecting L-shapes to close a window shape. A shape is
determined closed if it can seperate an inside region from the outside, i.e.
determine if it is a window.  \\

Ali et all \cite{Ali_facades} describe the windows with \emph{Haar} like
features, the features are combined using a cascading classifier. The cascading
classifier, which acts like a decision tree, is learned using the \emph{Ada
boost} algorithm.  They also use window detection to determine the region of
the facade.\\

Although this method is used a lot in computer vision, it is not the most
promissing approach to window detection because it is supervised.  This means
that it requires a large dataset in which every window must be accurately
annotated.  As the cascader uses a fixed swiping window it is sensitive to
scale: all window sizes must be equal and a size range must be given to the system.
Furthermore the system is always overfitted to the learning data
, making it hard to generalise (detect windows that are
not included in the dataset).  A more general descriptor of the window that is
size invariant is desirable.\\

\TODO{research question}

\TODO{loopt niet heeeelemaal lekker}
To investigate this, we developed two methods on which one method doesn't
require repeating windows nor aligned windows.  One of the main targets of our research
is a low requirements on the input data.  First our system doesn't need 
a large annotated dataset. Furthermore we took the images with a mobile phone
and decided to extract the windows from the image space only, this makes us
independent of additional expensive data like heat or laser range images.
It doesn't mean the previous work on window detection using laser or heat
images isn't of good use.  Instead we learned a lot from the previous research
as they have to match the laser or heat data to the real image space.  This
matching process involves a description (semantical annotation) of the facade.
Let's explain a method of that kind and discuss other approaches that are more
similar to our approach.\\

\subsubsection{Similar approaches}
Pu and Vosselman \cite{Pu_refiningbuilding} combine laser range images with
ground images to reconstruct facade details.  They solve inconsistency between
laser and image data and improve the alignment of a 3d model with a matching
algorithm.  In one of the matching strategies they compare the edges of a 3d
model to extracted Hough lines of both ground and laser range images.  They
match the lines by comparing the angle, location and length differences. These
criteria are also used in our approach.\\

They also detect windows and use them to provide a significant better alignment
of the 3d model.  As windows have a high reflection, they form hole like shapes
in the laser range images.  These holes are directly used to extract the
windows, unfortunatelly the results where far from accurate.\\

The work of Pu and Vosselman \cite{Pu_refiningbuilding} provides a useful
practical application of window detection and it amplifies the need for a robust
window detection technique that is independent of laser range data.\\


Recky et all \cite{Recky_kmeans} developed a window detector that is build on
the primary work of Lee and Nevatia \cite{Lee_extraction} (which is discussed
next).  In order to make clear orthogonal projections they rectify the facade. 
To determine the alignment of the windows the edges are projected into their orthogonal direction. 
For example the horizontal edges are projected in the vertical direction to 
to establish the vertical division of the windows.\\
\fig{p_projection_profiles.eps}{Projection profiles of Lee and Nevatias work}{0.4}
A function is developed that counts the amount of projected edges on each
location, this is the \emph{Projection profile}, see Figure
\ref{fig:p_projection_profiles.eps}.
A threshold is applied on the projection profile to indicate the window
boundaries that is used for the window alignment.\\

In the next step they use color to disambiguate the window areas from nonwindow
areas.  To be more precise, they convert the image to CIE-Lab color space and
use k-means to classify the windows.  Although this method is robust, both color
transformation and k-means clustering are very computational expensive.
Furthermore the classification based on color is sensitive to change in
illumination conditions.\\


As in the work of Recky et all \cite{Recky_kmeans} Lee et all
\cite{Lee_extraction}
perform orthogonal edge projection to find the window alignment.  As different
shapes of windows can exist in the same column/row, they only use the window alignment as
a hypothesis.  Then, using this hypothesis, they perform a refinement for each
window independently. 
Although this comes with accurate results, the iterative refinement is
a computational expensive procedure. As we want to run our system in realtime
this method is not suitable for our application. \\

\subsubsection{Our work in context}
The state of the art window alignment procedure in \cite{Recky_kmeans} and \cite{Lee_extraction}
is very robust.  Therefore we decided to use this method as a basis and improved
the alignment algorithm, furthermore we build a different window classification method.  

Our improvement on the alignment procedure is as follows.
In the previous work \cite{Recky_kmeans} and \cite{Lee_extraction} they use
a single projection profile for each direction.  We improved this 
process by fusing two (more advanced) projection profiles for each direction.
E.g. for the determination of the horizontal division of the windows we fuse both
horizontal and vertical projection profiles.\\

Furthermore we build two alternative window classification procedure which are 
based on a higher level of shape interpretation of these projection profiles.
As the classification is based on the projection profiles (edge information) we
don't require expensive color transformations and we only apply (rectification)
transformations on line segment endpoints. This makes our algorithm invariant to
change in illumination and perform in real-time.  
	










\subsection{Method I: Connected corner approach} 
\subsubsection{Situation and assumptions}
A window consists of a complex structure involving a lot of connected horizontal
and vertical lines, we use this property to detect the windows. 
We introduce the concept \emph{connected corner}, this is a corner that is 
connected to a horizontal and vertical line.  
The search for these connected corners is based on edge information.
The connected corners give a good indication of the position of the windows, 

In this approach the viewing direction is not required to be frontal.
The windows could be arbitrarily located and they don't need
to be aligned to each other neither to the X and Y axis of the image.
As such the windows are detected individually.

\subsubsection{Edge detection and Houghline extraction}
Edge detection is done as it is described in section \ref{sec:edge}.
From the edge images two groups of Houghlines are extracted. 
The groups fall in the two window directions horizontal and vertical.
This is done by controling the allowed angles, $\theta$ bin ranges, in the Hough transform.
The horizontal group has a range of $\theta = [-30..0..30)$ degrees, where $\theta = 0$ presents a horizontal line. 
The vertical group has a range of $\theta = [80..90..100)$ degrees. 

We use these ranges because 1) the user hardly ever holds the camera exactly
orthogonal.  2) we work with unrectified facades, meaning we deal with
perspective distortion.  To be more concrete, if the user takes a photo (Figure
\ref{fig:cameraPitch.eps} with a certain Yaw \= 0, the horizontal lines become
skew.  The range of the vertical group is smaller then the horizontal group as
the user often takes photo with a low pitch value and a high yaw.
\fig{cameraPitch.eps}{Pitch roll and yaw of the camera}{0.5}

The results of the edge detection and the Hough transform of two images can be seen in Figure \ref{fig:w_Dirk6_ImEdge.eps} and
 \ref{fig:w_Dirk6_ImHoughResult.eps}.

\TODO{maak analogie met L en H patronen hoe we die herkennen in de hersenen}

\subsubsection{Connected corners extraction}
\fig{cCornerTypes}{First row: different type of connected corner candidates. Second row: the
result the clean connected corner}{0.4} As windows contain complex structures
the amount of horizontal and vertical Houghlines is large at these locations.
A horizontal and vertical line is often connected in a corner of a window.  In
this approach we pair up these horizontal and vertical lines to determine
\emph{connected corners} that indicate a window.

Often a connected corner contains a small gap or an extension which we tolerate,
these cases are illustrated in Figure \ref{fig:cCornerTypes} in the top row.
A horizontal gap, a vertical and horizontal gap and a vertical elongation. The
cleaned up corners are given in the bottom row.  When the horizontal and
vertical lines intersect, the gap distance is $D=0$.  When the lines do not
intersect, the distance $D$ between the intersection point $P_i$ and the endpoint $P_e$ of the
line is measured $D = ||P_i-P_e||$, this is illustrated as dotted lines in Figure
\ref{fig:cCornerTypes}.  Next, $D$ is compared to a \emph{maximum intersection
distance} threshold $midT$.  And if $D<=midT$, the intersection is close enough
to form a connected corner.\\

After two Houghlines are classified as a connected corner, they are extended or
trimmed, depending on the situation. The results are shown in the second row in
Figure \ref{fig:cCornerTypes}.
In Figure \ref{fig:cCornerTypes}(I)  the horizontal line is extended.  Figure
\ref{fig:cCornerTypes}(II) shows that the vertical line is trimmed.  In Figure
\ref{fig:cCornerTypes}(III) both lines are extended.  At last, Figure
\ref{fig:cCornerTypes}(IV) shows how both lines are trimmed.


\subsubsection{Window area extraction}
To retrieve the actual windows, each connected corner is mirrored along its
diagonal. The connected corner now contains four sides which form a 
quadrangle window area.
All quadrangles are filled and displayed in Figure
\ref{fig:w_Dirk6_ImcCorner_windowFilled.eps}, this result is discussed in section
\ref{sec:results}.


\subsubsection{Results}
% old dataset:
%\fig{w_spil6cCornerImEdge.eps}{Edge detection}{0.6}
%\fig{w_spil6cCornerImHoughResult.eps}{Result of $\theta$ constrained Hough transform}{0.6}
%\fig{w_spil6cCornercCorner.eps}{Found connected corners}{0.6}
%\fig{w_spil6cCornerWindows.eps}{Connected corner as windows}{0.6}
%\fig{cCornerSpilTrans1.eps}{Found connected corners on the rectified image}{0.45}

%of the image, see Figure \ref{fig:w_Spil1TransCrop1_ImOri.eps}.

\newpage
\fig{w_Dirk6_ImEdge.eps}{Edge detection}{0.6}
\fig{w_Dirk6_ImHoughResult.eps}{Result of $\theta$ constrained Hough transform}{0.6}
\fig{w_Dirk6_ImcCorner_cCorner.eps}{Found connected corners}{0.6}
\fig{w_Dirk6_ImcCorner_windowFilled.eps}{Window regions}{0.6}

Figure \ref{fig:w_Dirk6_ImcCorner_windowFilled.eps} displays the result of the
connected corner aproach on an unrectified scene.  It contains 110 windows of
which are 109 detected, this is 99\%. Furthermore there are some False Positive
areas, this is about 3 \%.
Sometimes a window is not detected, for example the window on the right top
isn't detected, this is because its smaller then the minimum window width.\\

\subsubsection{Conclusion}
As can be concluded from the results, the connected corner method is suitable
for, and robust to, scenes with a variation in window sizes and types. This
	makes the connected corner approach suitable for a wide range of window
	scenes where no or few prior information about the windows is known.
	Furthermore as no rectification step is needed, the system has a low input
	requirement that is independent of calibrated input data and 3d information
	about the building.

\subsubsection{Future research} %\subsubsection{Method I: Connected corner approach} 
We only developed L-shaped connected corners, it would be nice to connect more
parts of the window to form U shaped connected corners or even complete rectangles.
The later is difficult because the edges are often incomplete due to for example occlusion.\\

Furthermore the next step in this study would be to group the connected corners
to windows and sub windows.  The big window that contains sub windows could be
found by calculating the convex hull of the red areas in Figure
\ref{fig:w_Dirk6_ImcCorner_windowFilled.eps}.  The sub windows could be found
using a clustering algorithm that groups the connected corners to a window. For
this method it would be useful to assume the window size as this correlates
directly to the inter-cluster distance.  It would also be nice to incorporate
not only the center of the connected corner as a parameter of the cluster space
but also the length and position of the of the connected corners' horizontal
and vertical line parts.  The inter cluster distance and the number of grouped
connected corner could form a good source for the certainty of the sub
window.\\







\subsection{Facade rectification}
\subsubsection{Introduction}
In order to apply our second method of window detection
we need the windows on the facade to be orthogonal and aligned.
Therefore we rectify the facade, which can be achieved in a simple or complex way.\\

The simple rectification method uses point to point correspondences. This 
requires annotation of the corner points of the facade that are mapped with the
corners of a rectangle. This mapping is used to calculate a transformation matrix. 
 The downside of this method is that it isn't very accurate.
This is because it only uses four point to point correspondences 
 and doesnt take the camera lens distortion into account.
 Another downside is that it requires (manual) annotation of the corner points.
A more accurate and fully automatic method is desired .\\

The second method involves the extraction of a 3D plane of the facade. This 
method is more complex and gives more accurate results. It involves a
comprehensive process and lots of research is done in this area. 
Given the related work, the automity of this module and our focus on the annotational part,
we used existing software to apply the window rectification.
I. Estebans \emph{FIT3D toolbox} \cite{Fit3d} comes with an addon which extracts a 3D model from a
series of frames.  One of the planes of the 3D model was used to rectify the
facade, making it ready for robust window detection. 

Using this addon of the \emph{FIT3D toolbox} \cite{Fit3d} we calculated the motion between a
series of frames in order to extract a point cloud of matching features. This
point cloud is used to extract a plane.  which is used to rectify the facade by
a projection process.  We now explain the steps in more detail.

\subsubsection{3D plane extraction}
We started by taking a series of 7 consecutive consequent (steady zoom, lightning, etc. parameters) images of a scene.
The images are chronological and have sufficient overlap. 

%/media/Storage/scriptie/fit3d/generate3dModel/3dcFiles
\fig{fit3dImgSequence.eps}{Example series of 7 consecutive frames, (dataset: FIT3D toolbox\cite{Fit3d})}{0.3}

We calculated the motion of the camera between the frames in a few steps
First we extract about 25k SIFT features of each frame.  Then we use
SIFT descriptors to describe and match the features within the consecutive
frames.  Not all features will overlap or match in the frames therefor RANSAC is used to
robustly remove the outliers.  After this an \emph{8-point algorithm} together with a
voting mechanism is used to extract the camera motion. For details please read
\cite{Fit3d}.

The frames where matched one by one which returns an estimation of the camera
motion that is not accurate enough.  Therefore a 3-frame match is done which
gives more accurate results.  Unfortunately this result comes with a certain
amount of reprojection error, this error is minimized using a numerical
iterative method called \emph{bundle adjustment}.  The final result is a very
accurate estimation of the camera motion.

The next step is to use this camera motion to obtain a set of 3D points
(corresponding to the matching image features).  This is achieved using \emph{linear triangulation}
method. 

Next a RANSAC based plane fitter is used to accurately fit a plane through
the 3D points. 
\emph{todo example figure}

\subsubsection{Efficient Projecting} 
Now we extracted the 3D plane of the facade, the next challenge is to use this
in order to rectify the facade.\\

It would be straight forward to rectify the full image. However this is
computational very expensive as each pixel needs to be projected. To keep the
computational cost to a minimum we project only the necessary data. Since we
are using Houghlines we project only the endpoints of the found Houghlines. 
(This is allowed because the projective transformation we apply preserves the
straightness of the lines. Note that this means we apply the edge detection and
Houghline extraction on the unrectified image.)\\

If $h$ is the number of Houghlines, the number of projections is $2h$
When we rectify the full image the number of projection is $w$ x $h$, where $w$,$h$ are the width and height of
the image. To give an indication, for the \emph{Anne1} dataset 
this means we apply 600 projections in stead of 1572864: a factor of almost 3k faster.\\

The Houghline endpoints are projected to the 3D plane we extracted in the same
way as we explained in chapter \ref{chap:skylinedetection}. To wrap up, we send
rays from the camera center trough the houghline endpoints and calculated the
intersection with the 3D plane.  The result is a 3D point cloud where each
point is labeled to their corresponding Houghline.\\

The next step is to transform the facade (and therefore the Houghlines) are
seen upfront. Instead of transforming the facade we rotate and translate the camera. 
This means the viewing direction (z-axis) of the camera needs
to be equivalent to the normal of the facade plane. Lets denote the
unit vector spanning the original viewing direction as $z$ and the
unit vector spanning the desired viewing direction/normal of the facade $z'$.
We calculated a rotation matrix from the axis-angle representation.\\

\TODO{ref axis angle wikipedia}
This presentation uses a unit vector $u$ indicating the direction of a directed axis, and an
angle describing the magnitude of the rotation about this axis.
This axis is orthogonal to $z$ and $z'$ and can therefore be
calculated by $u = cross(z,z')$ where cross defines the cross product of
two vectors.
The magnitude of the rotation $\alpha$ is equivalent to the angle $z$ between $z'$ given $u$. 
E.g. if the images are taken almost upfront (the facade is almost rectified) $\alpha$ is low.
$\alpha$ and $u$ are used to create a rotation matrix $R$ which is applied to the 3D point cloud.
The result is a set of rectified 3D points that are grouped tot their Houghlines.

\subsubsection{Results} % facade rectification
We show the result of two datasets, Anne1 and Dirk.
Note that we also rectified the image pixels itself, this is only for
the purpose of displaying the projected Houghlines in context.


\newpage
\fig{w_spil6ImOri.eps}{Dataset: Anne1, Original, (unrectified) image}{0.45}
%\fig{w_Spil1TransCrop1_ImOri.eps}{Dataset: Anne1, Rectified image}{0.45}
\fig{w_Spil1TransCrop1_ImHoughResult.eps}{Dataset: Anne1, Projected Houghlines on rectified image}{0.45}

\newpage
\fig{w_Dirk4_ImOri_Unrectified.eps}{Dataset: Dirk, Original, (unrectified) image, }{0.45}
\fig{w_Dirk4Trans_ImHoughResult.eps}{Dataset: Dirk, Projected Houghlines on rectified image}{0.45}
%-----------
\TODO{..}
%, the windows of this dataset differ in size, type, shape and are partially unaligned}

\TODO{Q: Do I have to discuss and conclude these results?}




\subsection{Method II: Histogram based approach} 
\subsubsection{Introduction}
From the previous section we saw that from a series of images, a 3D model of a
building can be extracted. Furthermore we saw that using this 3D model the
scene could be converted to a frontal view of a building, where a building wall
appears orthogonal.  This frontal view enables us to assume orthogonality and
alignment of the windows. 
We exploit this properties to build a robust window detector. First we determine
the alignment of the windows and then we label and group the areas that
contain the windows. 

\TODO{linken}

The extraction of the windows is done in different steps, first we rectify the
image as described in the previous section.  Then the alignment of the windows
is determined, this is based on a histogram of the Houghlines'. We use this
alignment to divide the image in window or not window regions.  Finally these
regions are classified and combined which gives us the windows.\\
We present a regular and alternative window alignment method
followed by two different kind of window classifications. 


\paragraph{Situation and assumptions}
To be more precise in our assumptions, we assume the windows have orthogonal
sides.  Furthermore we assume that the windows are aligned. This means that a
row of windows share the same height and $y$ position. For a column of windows
the width and $x$ position has to be equal.  Note that this doesn't mean that
all windows have the share the same size.



\subsubsection{Extracting the window alignment}
\paragraph{Regular window alignment}
%explain pipe line
%(color transform)
%edge extraction
%Houghline extraction
We introduce the concept alignment line. We define this as a horizontal or
vertical line that aligns multiple windows. In Figure
\ref{fig:w_Spil1TransCrop1_ImHibaap.eps}
we show the alignment lines as two groups, horizontal (red) and
vertical (green) alignment lines.  The combination of both groups give a grid of
blocks that we classify as window or non-window areas.\\

% MOTIVATION
How do we determine this alignment lines? We make use of the fact that among a
horizontal alignment line a lot of horizontal Houghlines are present, see
Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}. For the vertical alignment lines
the number of vertical Houghlines is high, see green lines in Figure
\ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}.\\

We begin by extracting the pixel coordinates of Hough transformed line
segments. We store them in two groups, horizontal and vertical.% (crosses in Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}). 
We discard the dimension that is least informative by projecting the coordinates to
the axis that is orthogonal to its group. 
This means that for each horizontal Houghline the coordinates on the line are projected to the X
axis and for each vertical Houghline the coordinates are projected to the Y
axis. We have now transformed the data in two groups of 1 dimensional
coordinates which represent the projected position of the Houghlines.\\

Next we calculate two histograms H(orizontal) and V(ertical), containing respectively
$w$ and $h$ bins where $w x h$ is the dimension of the image.  The histograms
are presented as small yellow bars in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.

The peaks are located at the positions where an increased number of Houghlines
start or end.  These are the interesting positions as they are highly correlated
to the alignment lines of the windows. 

It is easy to see that the number of peaks is far more then the desired number of alignment lines.
Therefore we smooth the values using a moving average filter.
The result, red lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}
, is a smooth \emph{projection profile} which contains the right number of peaks. The peaks
are located at the average positions of the window edges. Next step is to
calculate the peak areas and after this the peak positions. 

Before we find the peak positions we extract the peak \emph{areas} by thresholding the
function. To make the threshold invariant to the values, we set the threshold to 0.5 $\cdot$ max Peak. 
(This value works for most datasets but is a parameter that can be changed).
%The two thresholds are presented as black dotted lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.\\
Next we create a binary list of peaks P, P returns 1 for positions that are contained in
a peak, i.e. are above the threshold, and 0 otherwise.
We detect the peak areas by searching for the positions where P = 1
(where the function passes the threshold line). 
If we loop through the values of P we detect a peak-start on position $s$ if ${P(s-1),P(s)}={0,1}$
and a peak-end on $e$ if ${P(e-1),P(e)}={1,0}$. 
I.e. if P = 0011000011100, then two peaks are present. The first peak covers positions $(3,4)$, 
the second peak covers $(9,10,11)$.\\

Having segmented the peak areas, the next step is to extract the peak positions. 
Each peak area has only one peak and, since we used an average smoothing filter, the shape of 
the peaks are often concave. Therefore we extract the peaks by locating the max of each peak area. 
These locations are used to draw the window alignment lines, they can be seen
as dotted red lines and dotted green lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.

\paragraph{Alternative window alignment}
As you can see in Figure \ref{fig:w_Spil1TransImproved_Xv.eps}
a few window alignment lines are not found and a few lines are found at wrong locations.
The right side of the window frame of the first 4 columns of windows is not found.
This means we have to find another way to detect the window alignment on these positions.\\

For the vertical alignment we only took vertical lines into account.
In this method we examine the projection profile of the \emph{horizontal} Houghlines
projected on the X axis, $X_h$, Figure \ref{fig:w_Spil1TransImproved_Xh.eps}.
On the positions of the desired vertical alignment lines there appears to be a 
big decrease or increase of $X_h$ at the window frame. This is because on these
positions a window containing (a large amount of horizontal lines) starts or end.

We detect these big decreases or increases by creating a new pseudo peak profile
$D$ that takes the absolute of the derivative of $X_h$, Figure \ref{fig:w_Spil1TransImproved_Xh.eps}.
\[D = abs( X_{h}')\]
Next we extract the locations of the peaks as the previous method.

\paragraph{Fusing the window alignment methods}
We have presented two window alignment methods, next we fuse the methods to
gain a robust window alignment.
The target is to have as few as possible false positives while detecting all alignment locations.\\

If a window alignment position is found by both methods, often the peaks are
located very close to eachother, see Figure
\ref{fig:w_Spil1TransImproved_Xvh.eps}.  If this is the case we want to fuse the results by grouping the peaks.
Most of the times the peaks indicate the same window alignment but have
some disparity.  This is often the case when horizontal lines stop at the \emph{inside}
of a window frame while the vertical edges are located at the \emph{outer} side of the
window frame (this is supported by the fact that the disparity is often the size of the windowframe part).  
Yet, in other cases close peaks indicate different windows that are just hapen to be
located closely.  To apply a proper grouping of the peaks the challenge is to
distinguish these two cases.\\

First we decrease the total number of found window alignment locations by increasing the individual thresholds (from 0.3*max
peak to 0.5*max peak). Note that this has a positive side effect that the peaks that are found are more certain.
After this we group the peaks as follows: \\
First we calculate the average of the maximum window frame part and the minimum window
distance and call this the maximum peak group distance $G$.
Next we compare all peaks and if the distance between two peaks is lower than
$G$, we discard the peak with the least evidence (lowest peak). The result,
a set of unique peaks, can be seen in Figure \ref{fig:w_Spil1TransImproved_WAlinesMerged.eps}.\\

The advanced peak grouping is only required for the vertical alignment
of the windows: The horizontal inter window distance is often big enough to not
be mistaken by a window frame part.

\TODO{peak merging result misses for 1 dataset?}

\newpage
\paragraph{Results}
\fig{w_Spil1TransCrop1_ImHibaap.eps}{Dataset: Anne1, Regular window alignment (paralel projection): Based on a smoothed histogram (red line) that displays the amount of overlapping Houghlines, for the column division the horizontal houghlines are counted (at each $y$ position), for the row division the vertical houghlines are counted (at each $x$ position) }{0.45}
\newpage
\fig{w_Spil1TransImproved_Xv.eps}{Dataset: Anne2, Regular window alignment: Based on a histogram that displays amount of overlapping vertical Houghlines at each $x$ position}{0.7}
\fig{w_Spil1TransImproved_Xh.eps}{Dataset: Anne2, Alternative window alignment (orthogonal projection): Based on the shape of the smoothed histogram function. 
For the column division, the number of \emph{horizontal} Houghlines is counted (Note that this is the orthogonal oposite of the regular window alignment method). 
Peaks (that represent a big decrease or increase of the histogram function) are used for the alignment.}{0.7}
\newpage
\fig{w_Spil1TransImproved_Xvh.eps}{Dataset: Anne2, Regular+Alternative window alignment combined}{0.7}



\fig{w_Dirk4Trans_ImHibaap_Xvh.eps}{Dataset: Dirk, Regular+Alternative window alignment combined}{0.45}
\fig{w_Dirk4Trans_ImHibaap_Xvh_zoom.eps}{Dataset: Dirk, Regular+Alternative window alignment combined, zoomed}{0.45}

\paragraph{Discussion}
\TODO{diagram maken window alignment -> peak merging -> etc}
If we evaluate the independent window detection results we see (Figure
\ref{fig:w_Spil1TransImproved_Xh.eps}, \ref{fig:w_Spil1TransImproved_Xv.eps})
that in both datasets neither the regular window alignment method nor the
improved window alignment detect 100\% of the window alignment lines.  The
effect of fusing the methods is high as, after fusing, 100\% of the window
alignment is found on both datasets: each window alignment line is detected by
at least one method.  This is basically explained by the fact that both methods
are based on a different Houghline direction.  We now discuss the interesting
cases, where only one method succeeds in detecting the window alignment.\\

% doorlezen... ------------------------
If we take a look at the regular window alignment in Figure
\ref{fig:w_Spil1TransImproved_Xv.eps} we see that for the first 4 window
columns the right side of the window are not detected by the regular window
alignment method. 


\label{lab:occlusion}
This is because the original (unrectified) image (Figure
\ref{fig:w_spil6ImOri.eps}) is not a frontal image.  This makes building wall
extensions (middle of Figure \ref{fig:w_spil6ImOri.eps}) or drainpipes occlude
parts of the window.  
In this case the windows are countersunked into the wall
making the building wall itself ocluding the right window frame of the first 4
window columns (Figure \ref{fig:w_Spil1TransImproved_Xv.eps}).  The color of
the reflection of the window is very similar to the bricks and because the
window is not seperated by the window frame, the edge detector doesn't find a
strong edge.  On positions where the window frame is missing, no vertical
Houghlines will be detected and (as the regular window aligment is based on the
amount of vertical Houghlines) no window alignment will be found.  
This artefact can
be seen in the edge image Figure \ref{fig:w_Spil1TransCrop1_ImEdge.eps}, few or
no edges are present, and at the low height of the peaks in Figure
\ref{fig:w_Spil1TransImproved_Xv.eps} at these positions.\\

However, the alternative window alignment is based on the Houghlines in the
other orthogonal direction. For the vertical alignment this is the horizontal direction.
Because the occlusion artefact has no effect on the horizontal Houghlines (that begin
and stop at the \emph{inside} of the window frame), the alternative window
alignment method is a suitable alternative for these window alignment
locations.\\

In general, the alternative window alignment performs better then the regular
window alignment method.  However, in a few cases the regular window alignment
outperformes the alternative window alignment method.  For example the left
side of the second window column in Figure
\ref{fig:w_Dirk4Trans_ImHibaap_Xvh_zoom.eps} is not detected by the alternative
window alignment method.
This is because this window type has no horizontal divisions. Only the top and
bottom of the windowframe produce an edge, therefore the derivative wil give a
low peak on this position (which is too low to survive the threshold).  
This threshold is very high as it is determined by taking a fraction of the maximum peak 
which is located at the windows that do have horizontal divisions (for example the
most left or most right window column)). 


\TODO{move to conclusion}
This means that applying a threshold that is relative to a maximum peak
also implies a certain assumption on equality of the window types.
As it doesn't work very well on window types that differ (in for example
horizontal divisions)

\TODO{move to future work}
A solution of the missing window alignment lines on a scene with different
window types (Figure \ref{fig:w_Dirk4Trans_ImHibaap_Xvh_zoom.eps}) would be to
decrease the threshold if multiple window types are found. One could design a
measure of variety of the window types (e.g. by taking the variation of the
derivative into account for example). The value of the variety will determine
how much the threshold is lowered. Again the challenge is to detect the window
alignment on hard window types (with no horizontal division) but keep the
number of false positives (e.g. a drain pipe) zero.





this could become a problem if the window frame is also occluded.
%----------




%------------------------------------------------------------------------------------------
\TODO{Waar moet dit, window classification?}
%\paragraph{Comparison} %(discussion)
\TODO{compare classification methods and explain differences}
We described a basic method of window alignment and build a significant improvement.

If we compare the basic method (Figure \ref{fig:w_Spil1TransCrop1_ImClassRect.eps})
with the improved method (Figure \ref{w_Spil1TransImproved_windowsGrouped.eps})
we see that the improved method detected each window alignment with an maximum
error of 3 pixels whereas the basic method has an error of 15 pixels.  Its hard
to say something about the classification method I because the window alignment
lines had a large error.
The improved classification method Figure
\ref{fig:w_Spil1TransImproved_windowsGrouped.eps} however, classified 100 \% of
the windows correct.
\TODO{also talk abooot other classification}
%------------------------------------------------------------------------------------------


\TODO{drain pipe result niet erg ivm vlg stap}

\paragraph{Conclusion}
\TODO{}


\TODO{make diagram: edge- hough -window alignment - window classificatio - window grouping}


\subsubsection{Basic window classification (based on line amount)}
The image is now divided in a new grid of blocks based on these
alignment. The next challenge is to classify the blocks as window and
non-window areas: the window classification, we developed two different methods for this.

Instead of classifying each block independently, we classify full rows and
columns of blocks as window or non-window areas.  This approach results in a accurate
classification as it combines a full blockrow and blockcolumn as evidence for a singular
window. 

The method exploits the fact that the windows are assumed to be
aligned.
A blockrow that contains windows will have a high amount of vertical
Houghlines, Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}
(green). For the blockcolumns the number of horizontal Houghlines
 (red) is high at window areas.  We use this property to classify 
 the blockrows/blockcolumns. 

For each blockrow the overlap of all vertical Houghlines are summed up.
(Remark that with this method we take both the length of the Houghlines and
amount of Houghlines implicitly into account.)

To prevent the effect that the size of the blockrow influences the outcome, this total value
is normalized by the size of the blockrow.
\[\forall Ri\in \{1..numRows\} : R_i = \frac{HoughlinePxCount}{R_i^{width} \cdot R_i^{height}}\]

Leaving us with $||R||$ (number of blockrows) scalar values that give a rank of a blockrow begin a window area or not.
This is also done for each blockcolumn (using the normalized horizontal amount of
Houghlines pixels) which leaves us with $C$.

\fig{w_Spil1TransCrop1_ImClassRectBarh.eps}{Normalized vertical Houghline pixel count of
the blockrows (R)}{0.6}
If we examine the distribution of $R$ and $C$, we see two clusters appear: one with
high values (the blockrows/blockcolumns that contain windows) and one with low values (non window
blockrows/blockcolumns). For a specific example we displayed the values of $R$ in Figure \ref{fig:w_Spil1TransCrop1_ImClassRectBarh.eps}.
Its easy to see that the high values, blockrow 4,5,7,8,10 and 11, correspond to the
six window blockrows in Figure \ref{fig:w_Spil1TransCrop1_ImClassRect.eps}.

How do we determine which value is classified as high?  A straight forward
approach would be to apply a threshold, for example 0.5 would work fine.
However, as the variation of the values depend on (unknown) properties like the
number of windows, window types etc., the threshold maybe classify insufficient
in another scene.  Hence working with the threshold wouldn't be robust. 

Instead we use the fact that a blockrow is either filled with windows or not, hence
there should always be two clusters.  We use \emph{$k$-means} clustering (with
$k=2$) as the classification procedure.
\TODO{ref}
This results in a set of Rows and Columns that are classified as window an
non-window areas.

The next step is to determine the actual windows $W$.
A block $w\in W$ that is crossed by $R_j$ and $C_k$ is classified as a
window iff \emph{$k$-means} classified both $R_j$ and $C_k$ as window areas. These are displayed in 
 Figure \ref{fig:w_Spil1TransCrop1_ImClassRect.eps} as green rectangles.

The last step is to group a set of windows that belong to each other. This is done by 
grouping adjacent positively classified blocks. These are displayed as red
rectangles in Figure \ref{fig:w_Spil1TransCrop1_ImClassRect.eps}.

As the figure gives a binary representation of the windows it is not possible
to see how certain a block is classification.
To get insight about this we developed a measure of certainty function.

\[P(R_i) = \frac{R_i}{max(R)}\]
\[P(C_i) = \frac{C_i}{max(C)}\]
\[C(w) = \frac{P(w^{R_i}) + P(w^{C_i})}{2}\]
As you can see $C$ is normalized, this is to ensure the value of the maximum
certainty is exactly 1. The results can now be relatively interpreted, e.g. if the rectangle's $P=0.5$
then the system nows for 50 \% sure it is a window, compared to its best window ($P=1$). 
And, as the normalization implies this, there is at least one window with $P=1$. 

The visualization of the measure of certainty is shown in Figure
\ref{fig:w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps},  
the whiter the area the higher the measure of certainty.

\paragraph{Results} % method II, improved classification
\newpage
\fig{w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps}{Window classification measure of certainty, 
the whiter the area the higher the measure of certainty.}{0.45}

\TODO{include results of binary classification}
The bright rows and columns in Figure
\ref{fig:w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps},  indicate window
blockrows and blockcolumns, whereas the dark rows and columns indicate
non-window areas.
The stripe patterns support that the classification process exist of individual
row and column classification.
The area of window positions is particularly white, as it 
intersects a bright (positively classified) row and column.
One can compare this result to the windows in the original image 
(Figure \ref{fig:w_Spil1TransCrop1_ImOri.eps}).


\paragraph{Conclusion}
We can conclude that this certainty based classification works quitte good on
this dataset.  However, it requires a very well alignment of the windows which
is a disadvantage.  As it is based on the number of found Houghlines, the
errors in the window alignment will propagate to the classification.  This
makes this classification method inappropriate for a system where one cannot
fully rely on the window alignment.  \\
This conclusion amplifies the need for a robust classification method (that is
independent, (or at least less sensitive)) to window alignment errors.


\subsubsection{Improved window classification (based on shape of the histogram function)}
\TODO{introduce a little bit}
\TODO{say that anne2 dataset is more rectified }
If we take a look at Figure \ref{fig:w_Spil1TransImproved_WAlinesMerged.eps}
we see that $X_h$ (the amount of horizontal Houghlines) has two shapes that
repeat:  At the location where a window is present $X_h$ is concave whereas at
non-window areas $X_h$ is convex. This is because lots of
horizontal lines are found at certain window positions (the window centers) and few
are found at positions that are far away from these certain positions, which are the
centers of non-window areas (that lie between windows).  The shape type of $X_h$ is
used as a cue for our second window classifier.\\

This shape type of $X_h$ is detected as follows:
As in \textbf{improved window alignment}, the first step is to examine the
derivative of $X_h$, blue line in Figure
\ref{fig:w_Spil1TransImproved_WAlinesMerged.eps}.  We investigate the positions
where $D=X_{h}'$ changes from sign, these are the peaks or valleys of $X_h$.
$X_h$ is concave at the sign changes from positive to negative (+,-) and $X_h$
is convex if the sign changes (-,+).\\

We expect one sign change per block, however it is possible that multiple sign
changes occur. In this case we smooth $X_h$ again and repeat the algorithm
until for each block a maximum of one sign change is found. \\

Now we have detected the shape type (concave or convex), we can directly
classify the blockrows and blockcolumns as window areas and non-window areas.
The windows are determined as the previous classifier by combining the
(positively classified) blockrows and blockcolumns.\\

For the sake of representation only blockcolumns are presented, the method for
the blockrows is the same, the projection profiles are projected to the
Y-axis.

\paragraph{Results} % method II, %measurements, observations, perceptions
\label{sec:results}.

\TODO{discuss drain pipe, dat het goed is dat het resultaat mooi is ondanks drainpipe}



%Results of the Anne dataset:\\





\TODO{Explain datasets}

\fig{w_Spil1TransImproved_WAlinesMerged.eps}{Dataset: Anne2, Improved window classification method: Concave and convex shapes on window/nonwindow locations}{0.7}
\fig{w_Spil1TransCrop1_ImClassRect.eps}{Dataset: Anne1, Basic window classification method: The extracted windows, red:the grouping}{0.45}
\fig{w_Spil1TransImproved_windowsGrouped.eps}{Dataset: Anne2, Improved window classification method: The extracted windows, red:the grouping}{0.7}
\fig{w_Dirk4Trans_ImClassRect.eps}{Dataset: Dirk, Improved window classification method: The extracted windows, red:the grouping}{0.45}

\TODO{Waar onderstaand?}
\newpage
\fig{w_Dirk4Trans_ImHoughResult.eps}{$\theta$-constrained Hough transform}{0.45}

\TODO{make im of grapscale prob}
\TODO{make a diagram, histogram - alignment lines -> window classification etc.}
\TODO{ iets zeggen over dataset Dirk}
\TODO{more realistic dataset, with a occluding tree and some light spots created by the sun }
\TODO{not all windows are aligned}
\TODO{dat grouping in beide classificatiemethoden hetzelfde is}
\TODO{discuss/ interpret results of dirk dataset (aanvullende kracht van methoden etc.)}
\TODO{to much alignment lines, beter to much than to few}
\TODO{reason, the two doors below bicycles on the left side}
\TODO{especially the horizontal division of the lowest row of windows is amazing, refer to}

Facts of Figure \ref{fig:w_Dirk4Trans_ImHibaap_Xvh.eps}\\
the amount of alignment lines is very large.

Facts of Figure \ref{fig:w_Dirk4Trans_ImClassRect.eps}\\
\begin{itemize}
	\item 100 \% of the alignment lines are positioned at window areas.
	\item 100 \% of the windows are detected
	\item 44 windows are found
	\item 5 windows (11\%) are false positive
	\item 39 windows (87\%) are true positive
\end{itemize}

Grouping of positive classified windows:\\
\begin{itemize}
	\item 39 windows are positive classified of which:
	\item 35 windows (90\%) are grouped correct
	\item 4 windows (10\%) are grouped incorrect
\end{itemize}



This first two window rows are classified as two groups but this should be one.\\

\TODO{tabel met percentage correct versch datasets}

% Appendix images
%\fig{w_Spil1TransCrop1_ImEdge.eps}{Dataset: Anne1, Result edge detection}{0.45}
% w_Dirk4Trans_ImEdge.eps                               
% w_Dirk4Trans_ImHibaap_Xh.eps                          
% w_Dirk4Trans_ImHibaap_Xv.eps                           

\TODO{plot original  function (not only peaks)}



%\paragraph{Interpretation of the results of the Anne dataset}
\TODO{..}







%\paragraph{Interpretation of the results of the Dirk dataset}
It is easy to see in Figure \ref{fig:w_Dirk4Trans_ImHibaap_Xvh.eps} that there are 
too many alignment lines detected and that not every alignment line is placed correctly. 
This is mainly because the scene is partially aligned, the two
front doors contain many vertical edges that are not aligned with the windows.
Another cause of this artefact is that the bicycles on the left cause a large
amount of found Hough lines, see \ref{fig:w_Dirk4Trans_ImHoughResult.eps}.
 


%--- dirk dataset: --%
For the actual detection of the windows, the problem of finding to many window alignment lines
(see Figure \ref{fig:w_Dirk4Trans_ImHibaap_Xvh.eps})
didn't seem to be a problem. This is because after the peak fusing module discards many close alignment lines, the
many small subwindows are grouped to clear big windows, see red rectangles in Figure
\ref{fig:w_Dirk4Trans_ImClassRect.eps}.\\


This first two window rows are classified as two groups but this should be one.
Normally the peak merging step would handle this problem, but with this dataset
we could not use a large \emph{maximum peak distance} value because the windows
in the image (especially the first and last columns) are very close.




\paragraph{Conclusion} %\subsubsection{Method II: Histogram based approach} 

\subsubsection{Future research} %\subsubsection{Method II: Histogram based approach} 
\paragraph{Performance measure for extreme viewing angles}
It would be nice to investigate the effect of the occlusion and to examine the
robustness of the window detector under extreme viewing angles.
For example the viewing angle could be plotted against the percentage of
correct detected windows.

\paragraph{Window alignment: Peak grouping}
We fused the result of the horizontal and vertical Houghlines and discarded
peaks that where close.  A more accurate result would be achieved if close peaks
where averaged, the height of the peak could be used as a weight.\\
We used a manual maxium peak group distance ($G$), this value could also be automatically derived from the image
by for example taking a percentage of the window, or by detecting the size of the window frame parts.\\ 
It is challenging to handle multiple close peaks, e.g. if 4 peaks
are close, then peak 1 could indicate the same window as peak 2 but inidicate a different window then peak 4. The location of the close peaks: inside, at the border, or outside the window could add important evidence, some methods of the window classification could be used to detect these values.

\paragraph{Window alignment refinement}
To get more accurate result or to handle scenes with poor window alignment a refinement procedure could be applied.
As mentioned in the related work, Lee et all \cite{Lee_extraction} applied window refinement.
Although this comes with accurate results, the iterative refinement is a
computational expensive procedure. 
It would be nice to have a dynamic system that is aware of this 
accuracy and computational time trade of. A system that only refines the results when the recourses are available.
For example if a car is driving and uses window detection for building recognition the refinement is disabled.
But if the car is lowering speed the refinement procedure could be activated.
Resulting in accurate building recognition which opens the door for augmented reality.

\paragraph{Feature fusion}
Both window refinement and window alignment steps could use some additional
evidence which could be provided by feature based methods.  For example a
\emph{multi scale Harris corner detector} could help an accurate alignment or
refinement of the windows.






\subsection{Discussion}  % both methods
The outcome of Classification method I is non-deterministic, as it depends on to the
random initialization of the cluster centers. This means that our results could be correct by
coincidence.  To exclude this artefact, we ran the cluster algorithm 10 times
on all datasets, fortunately it resulted in consistent classifications.
\TODO{ also talk aboooot other datasets you knuw,}






\subsection{Conclusion}
% not summarize!
% what can be derived from the results+?
\TODO{We showed that projecting the image to a frontal view is a good preprocessing}
\TODO{step of a robust window detector.}
\TODO{Furthermore our algorithms work in real time. \\}



