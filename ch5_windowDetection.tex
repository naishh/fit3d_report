% set ignorecase
\TODO{kmeans good luck images}
\TODO{Zorgen dat paragraphs ook 3.5.3.1 nummering krijgt}
\TODO{research question!}

\section{Window detection}
\label{sec:windowDetection}
\subsection{Introduction}
In this chapter we deal with an important aspect of semantic urban scene interpretation: window detection. 
From the introduction we learned that window detection can play an important role in
a variety of domains: semi automatic 3D reconstruction/modelling of city models, documenting
historical buildings, analysis of old building deformation, augmented
reality, building recognition, etc.\\

This chapter deals with our developed methods of window detection and it is organized
as follows:\\
  We start with related work and put our work in context.  Next we describe our first
  window detection approach that is invariant to viewing direction.  After this
  we present a facade rectification method. Next the rectification result is used for our
  second method that assumes orthogonal and aligned windows.  Finally we show
  and discuss our results. 

\TODO{in state of art een lijn creeren, opbouwen, in context plaatsen, theoretisch afwimplene}
\subsection{Window detection: state of art} % related work
A large amount of research is done on semantical interpretation of urban scenes. 
First we briefly discuss the research that is done on window detection using
approaches that differ from our approach.  After this, we discuss state of art
window detection that lays at the basis of our approach in detail.\\

\subsubsection{Alternative approaches on window detection}
Muller et al. \cite{Muller_procedural} detect regularity and symmetry in the building. The
symmetry is detected in the vertical (floors) and horizontal (window
rows) direction.
They use shape grammars to divide the building wall in tiles, windows, doors etc.
The results are used to derive a 3d model of high 3d visual quality.
\figsSmall{label_window_nice_geometry}{p_window_nice_geometry1.eps}{p_window_nice_geometry2.eps}{Results of Muller et al.}{original texture}{reconstruction}
Although they achieved some interesting results, their method has some
disadvantages.  As their method is fully based on detecting symmetry, they have
to assume repeating and aligned windows: this constrains the variety of scenes
the system can handle.
Furthermore they match template window objects which they predefine. This 
constraints the variety of window types that could be matched.  At last 
they use expensive algorithms that make it impossible for the system
to run in real time.  \\

Using a thermal camera, Sirmacek \cite{Sirmacek_thermal}
detects heat leakage on building walls as an indicator for doors or windows.
Windows are detected with L-shaped features as a set of \emph{steerable filters}.
The windows are grouped using \emph{perceptual organization rules}:
they search and group intersecting L-shapes to close a window shape. A shape is
determined closed if it can separate an inside region from the outside, i.e.
determine if it is a window.  \\

Ali et al. \cite{Ali_facades} describe the windows with \emph{Haar} like
features, the features are combined using a cascading classifier. The cascading
classifier (which acts like a decision tree) is learned using the \emph{Ada
boost} algorithm.  They also use window detection to determine the region of
the facade.\\

Although this method is used a lot in computer vision, it is not the most
promising approach to window detection because it has to be supervised.  This means
that it requires a large dataset in which every window must be accurately
annotated.  As the cascader uses a fixed swiping window it is sensitive to
scale: all window sizes must be equal and a size range must be given to the system.
Furthermore the system is always over fitted to the learning data
, making it hard to generalize (detect windows that are
not included in the dataset).  A more general descriptor of the window that is
size invariant is desirable.\\

\TODO{research question}

To investigate this, we developed two methods of which one method does not
require repeating windows nor aligned windows.  One of the main targets of our research
is to have small requirements on the input data.  First our system doesn't need 
a large annotated dataset. Furthermore we took the images with a mobile phone
and decided to extract the windows from the image space only, this makes us
independent of additional expensive data like heat or laser range images.
It doesn't mean the previous work on window detection using laser or heat
images isn't of good use.  Instead we learned a lot from the previous research
as they have to match the laser or heat data to the real image space.  This
matching process involves a description (semantical annotation) of the facade.
Let's explain a method of that kind and discuss other approaches that are more
similar to our approach.\\

\subsubsection{Similar approaches}
Pu and Vosselman \cite{Pu_refiningbuilding} combine laser range images with
ground images to reconstruct facade details.  They solve inconsistency between
laser and image data and improve the alignment of a 3d model with a matching
algorithm.  In one of the matching strategies they compare the edges of a 3d
model to extracted Hough lines of both ground and laser range images.  They
match the lines by comparing the angle, location and length differences. These
criteria are also used in our approach.\\

They also detect windows and use them to provide a significant better alignment
of the 3d model.  As windows have a high reflection, they form hole like shapes
in the laser range images.  These holes are directly used to extract the
windows. Unfortunately due to bad laser range data, the results where far from
accurate.\\

The work of Pu and Vosselman \cite{Pu_refiningbuilding} provides an useful
practical application of window detection. It amplifies the need for a robust
window detection technique that is independent of laser range data.\\


Recky et al. \cite{Recky_kmeans} developed a window detector that is build on
the primary work of Lee and Nevatia \cite{Lee_extraction} (which is discussed
next).  In order to make clear orthogonal projections they rectify the facade. 
To determine the alignment of the windows the edges are projected into their orthogonal direction. 
For example the horizontal edges are projected in the vertical direction to 
establish the vertical division of the windows.\\
\fig{p_projection_profiles.eps}{Projection profiles of Lee and Nevatias work}{0.4}
A function is developed that counts the amount of projected edges on each
location. This is the \emph{Projection profile}, see Figure
\ref{fig:p_projection_profiles.eps}.
A threshold is applied on the projection profile to indicate the window
boundaries that are used for the window alignment.\\

In the next step they use color to disambiguate the window areas from non-window
areas.  To be more precise, they convert the image to CIE-Lab color space and
use k-means to classify the windows.  Although this method is robust, both color
transformation and k-means clustering are very computational expensive.
Furthermore the classification based on color is sensitive to change in
illumination conditions.\\


As in the work of Recky et al. \cite{Recky_kmeans}, Lee et al.
\cite{Lee_extraction}
perform orthogonal edge projection to find the window alignment.  As different
shapes of windows can exist in the same column/row, they only use the window alignment as
a hypothesis.  Then, using this hypothesis, they perform a refinement for each
window independently. 
Although this comes with accurate results, the iterative refinement is
a computational expensive procedure. As we want to run our system in real time
this method is not suitable for our application. \\

\subsubsection{Our work in context}
The state of art window alignment procedure in \cite{Recky_kmeans} and \cite{Lee_extraction}
is very robust.  Therefore we have decided to use this method as a basis and improved
the alignment algorithm. Furthermore we have build a different window classification method.  

Our improvement on the alignment procedure is as follows.
In the previous work \cite{Recky_kmeans} and \cite{Lee_extraction} 
a single projection profile for each direction is used.  We improved this 
process by fusing two (more advanced) projection profiles for each direction.
E.g. for the determination of the horizontal division of the windows we fuse both
horizontal and vertical projection profiles.\\

Furthermore we have build two alternative window classification procedures which are 
based on a higher level of shape interpretation of these projection profiles.
As the classification is based on the projection profiles (edge information) we
don't require expensive color transformations and we only apply (rectification)
transformations on line segment endpoints. This makes our algorithm invariant to
change in illumination and perform in real-time.  
	










\subsection{Method I: Connected corner approach} 
\subsubsection{Situation and assumptions}
A window consists of a complex structure involving a lot of connected horizontal
and vertical lines, we use this property to detect the windows. 
We introduce the concept \emph{connected corner}, this is a corner that is 
connected to a horizontal and vertical line.  
The search for these connected corners is based on edge information.
The connected corners give a good indication of the position of the windows. 
In this approach the viewing direction is not required to be frontal.
The windows could be arbitrarily located and they don't need
to be aligned to each other neither to the X and Y axis of the image.
As such the windows are detected individually.

\subsubsection{Edge detection and Houghline extraction}
Edge detection is done using the Canny edge detector motivated earlier in
section (\ref{sec:edgeDet}).
From the edge images two groups of Hough lines are extracted. 
The groups fall in the two window directions horizontal and vertical.
This is done by controlling the allowed angles, $\theta$ bin ranges, in the Hough transform.
The horizontal group has a range of $\theta = [-30..0..30)$ degrees, where $\theta = 0$ presents a horizontal line. 
The vertical group has a range of $\theta = [80..90..100)$ degrees. 

We use these ranges because 1) the user hardly ever holds the camera exactly
orthogonal.  2) we work with unrectified facades, meaning we deal with
perspective distortion.  To be more concrete, if the user takes a photo (Figure
\ref{fig:cameraPitch.eps}) with a certain Yaw \= 0, the horizontal lines become
skew.  The range of the vertical group is smaller then the horizontal group as
the user often takes photos with a low pitch value and a high yaw.
\fig{cameraPitch.eps}{Pitch roll and yaw of the camera}{0.5}

The results of the edge detection and the Hough transform of two images can be seen in Figure \ref{fig:w_Dirk6_ImEdge.eps} and
 \ref{fig:w_Dirk6_ImHoughResult.eps}.

\TODO{maak analogie met L en H patronen hoe we die herkennen in de hersenen}

\subsubsection{Connected corners extraction}
\fig{cCornerTypes}{First row: different type of connected corner candidates. Second row: the
result the clean connected corner}{0.4} 
A window often consists of several subwindows enclosed in rectangular window
frames.  As the color of the window frames differ from the glas, the amount of
horizontal and vertical edges is large at these locations.  As the
horizontal and vertical edges come from rectangular window frames they often
share a corner of the window.  We developed an approach where we pair up these
horizontal and vertical lines to determine \emph{connected corners} that
indicate a window.\\

Often a connected corner contains a small gap or an extension which we tolerate,
these cases are illustrated in Figure \ref{fig:cCornerTypes} in the top row.
A horizontal gap, a vertical and horizontal gap and a vertical elongation. The
cleaned up corners are given in the bottom row.  When the horizontal and
vertical lines intersect, the gap distance is $D=0$.  When the lines do not
intersect, the distance $D$ between the intersection point $P_i$ and the endpoint $P_e$ of the
line is measured $D = ||P_i-P_e||$, this is illustrated as dotted lines in Figure
\ref{fig:cCornerTypes}.  Next, $D$ is compared to a \emph{maximum intersection
distance} threshold $midT$.  And if $D<=midT$, the intersection is close enough
to form a connected corner.\\

After two Hough lines are classified as a connected corner, they are extended or
trimmed, depending on the situation. The results are shown in the second row in
Figure \ref{fig:cCornerTypes}.
In Figure \ref{fig:cCornerTypes}(I)  the horizontal line is extended.  Figure
\ref{fig:cCornerTypes}(II) shows that the vertical line is trimmed.  In Figure
\ref{fig:cCornerTypes}(III) both lines are extended.  At last, Figure
\ref{fig:cCornerTypes}(IV) shows how both lines are trimmed.


\subsubsection{Window area extraction}
To retrieve the actual windows, each connected corner is mirrored along its 
diagonal through the endpoints. The connected corner now contains four sides which form a 
quadrangle window area.
All quadrangles are filled and displayed in Figure
\ref{fig:w_Dirk6_ImcCorner_windowFilled.eps}, this result is discussed in section
\ref{sec:results}.


\subsubsection{Results}
\label{sec:results}.
\TODO{more datasets..}
% old dataset:
%\fig{w_spil6cCornerImEdge.eps}{Edge detection}{0.6}
%\fig{w_spil6cCornerImHoughResult.eps}{Result of $\theta$ constrained Hough transform}{0.6}
%\fig{w_spil6cCornercCorner.eps}{Found connected corners}{0.6}
%\fig{w_spil6cCornerWindows.eps}{Connected corner as windows}{0.6}
%\fig{cCornerSpilTrans1.eps}{Found connected corners on the rectified image}{0.45}

%of the image, see Figure \ref{fig:w_Spil1TransCrop1_ImOri.eps}.

\newpage
\fig{w_Dirk6_ImEdge.eps}{Edge detection}{0.45}
\fig{w_Dirk6_ImHoughResult.eps}{Result of $\theta$ constrained Hough transform}{0.45}
\fig{w_Dirk6_ImcCorner_cCorner.eps}{Found connected corners}{0.45}
\fig{w_Dirk6_ImcCorner_windowFilled.eps}{Window regions}{0.45}
\clearpage

Figure \ref{fig:w_Dirk6_ImcCorner_windowFilled.eps} displays the result of the
connected corner approach on an unrectified scene.  It contains 110 windows of
which are 109 detected, this is 99\%. Furthermore there are some false positive
areas, this is about 3 \%.
Sometimes a window is not detected, for example the window on the right top
isn't detected, this is because its smaller then the minimum window width.\\


\subsubsection{Future research} %\subsubsection{Method I: Connected corner approach} 
We only developed L-shaped connected corners. In future research we could
connect more parts of the window. E.g. to form U shaped connected corners or
even complete rectangular shapes. The latter is difficult because the edges are
often incomplete due to for example occlusion.\\

Furthermore the next step in this study would be an analysis of the substructure
of the windows.  The big window that contains sub windows could be
found by calculating the convex hull of the red areas in Figure
\ref{fig:w_Dirk6_ImcCorner_windowFilled.eps}.  The sub windows could be found
by grouping connected corners that resamble the same subwindow.\\

This could be done by clustering the connected corners at their location.
It would be useful to assume the number of subwindows as this can be used to
determine the maximum inter-cluster distance.  
We could also incorporate not only the center of the connected corner as a
parameter of the cluster space but also the length and position of the
connected corners' horizontal and vertical line parts.  The inter cluster
distance and the number of grouped connected corners could form a good source for
the certainty of the sub window.\\







\subsection{Facade rectification}
\subsubsection{Introduction}
In order to apply our second method of window detection (\ref{sec:method2}),
we need the windows on the facade to be orthogonal and aligned.
Therefore we rectify the facade, this can be achieved in a manual or in an automatic way.\\

The manual rectification method uses point to point correspondences. This 
requires annotation of the corner points of the facade that are mapped with the
corners of a rectangle. This mapping is used to calculate a transformation matrix. 
 The downside of this method is that it is not very accurate as it doesn't take
 the width and height ratio of the facade into account.
It also does not take the camera lens distortion into account.
 Another downside is that it requires (manual) annotation of the corner points.
As we want our rectification method to be accurate and fully automatic this
method is not suitable.\\

The second method involves the extraction of a 3D plane of the facade.  This
method is more complex but gives more accurate results.  Extracting a 3D plane
from a series of images is a comprehensive process and a large amount of
research is done in this area.  As we want to focus on the annotational part of
facade interpretation, we used existing software to assist our rectification.\\

I. Esteban's \emph{FIT3D toolbox} \cite{FIT3D} comes with an add-on which
extracts a 3D model from a series of frames.  This add-on involves a process tha
calculates the motion between a series of frames in order to extract a point
cloud of matching features. This point cloud is used to extract a plane for
every unique wall.  More details of this process are explained in
(\ref{sec:prelimFIT3D}).

% todo heel chapter facade rectification in preliminaries?

\subsubsection{3D plane based rectification} 
The next challenge is to use the extracted 3D plane in order to rectify the facade.
It would be straight forward to rectify the full image. However this is
computational very expensive as each pixel needs to be projected. To keep the
computational cost to a minimum we project only the necessary data. Since we
are using Hough lines we project only the coordinates of the endpoints of the found Hough lines. 
This is allowed because the projective transformation is we apply is a affine
transformation which preserves the
straightness of the lines \cite{linearalgebra}. Note that this means we apply the edge detection and
Houghline extraction on the unrectified image.\\

If $h$ is the number of Hough lines, the number of projections is $2h$.
When we rectify the full image the number of projection is $w$ x $h$, where $w$,$h$ are the width and height of
the image. To give an indication, for the \emph{Anne1} dataset 
this means we apply 600 projections in stead of 1572864: a factor of almost 3k faster.\\

The Houghline endpoints are projected to the 3D plane we extracted in the same
way as we explained in chapter \ref{sec:skylinedetection}. We have send
rays from the camera center trough the Hough line endpoints and calculated the
intersection with the 3D plane.  The result is a 3D point cloud where each
point is labeled to its corresponding Houghline.\\

The next step is to transform the facade (and therefore the Hough lines) to a
frontal view. Instead of transforming the facade we rotate and translate the camera. 
This means the heading (z-axis) of the camera needs to be equivalent to the normal of the facade plane. 
We determine the rotation matrix $R$ by calculating the angle between the camera's heading an the normal of
the facade. This process involves calcuting 1) the orthogonal rotation vector
$\vec{n}$ and 2) the angle $\theta$, see Figure \ref{fig:axisAnglePresentation.eps}.

\fig{axisAnglePresentation.eps}{$\vec{a}$ is the camera's heading and $\vec{b}$
is the normal of the facade plane, $\vec{n}$ and $\theta$ are used to generate
rotation matrix A \cite{axisangle}}{0.15}
 
Next, $R$ is applied to the 3D point cloud resulting in a set of rectified 3D
points that are grouped to their Hough lines.

%This presentation uses a unit vector $u$ indicating the direction of a directed axis, and an
%angle describing the magnitude of the rotation around this axis.
%This axis is orthogonal to $z$ and $z'$ and can therefore be
%calculated by $u = cross(z,z')$ where cross defines the cross product of
%two vectors.
%The magnitude of the rotation $\alpha$ is equivalent to the angle $z$ between $z'$ given $u$. 
%E.g. if the images are taken from a(n almost) frontal view (the facade is almost rectified) $\alpha$ is low.
%$\alpha$ and $u$ are used to create a rotation matrix $R$ which is applied to the 3D point cloud.

%The result is a set of rectified 3D points that are grouped to their Hough lines.

\subsubsection{Results} % facade rectification
We show the result of two datasets, Anne1 and Dirk.
Note that we also rectified the image pixels itself, this is only for
the purpose of displaying the projected Hough lines in context.

\newpage
\fig{w_spil6ImOri.eps}{Dataset: Anne1, Original, (unrectified) image}{0.45}
%\fig{w_Spil1TransCrop1_ImOri.eps}{Dataset: Anne1, Rectified image}{0.45}
\fig{w_Spil1TransCrop1_ImHoughResult.eps}{Dataset: Anne1, (Projected) Hough lines on rectified image}{0.35}
\clearpage

\newpage
\fig{w_Dirk4_ImOri_Unrectified.eps}{Dataset: Dirk, Original, (unrectified) image, }{0.45}
\fig{w_Dirk4Trans_ImHoughResult.eps}{Dataset: Dirk, (Projected) Hough lines on rectified image}{0.45}




\TODO{Met iemand overleggen wat betere volgorde is}
\subsection{Datasets}
To avoid over fitting we used multiple datasets that contain different urban
scenes.  Furthermore every dataset has a challenge/handicap.
We used three datasets that are recorded in the suburban area
'de Baarsjes' of Amsterdam.  All images have an original resolution of 3072x2304
pixels and are downscaled (using cubic interpolation) to 1280x1024 pixels.

\subsubsection{Anne1}
The challenge of this dataset is that it suffers from rectification errors.
This makes the window alignment a challenging task as it assumes the windows to
be aligned with the x-axis and y-axis of the image.
The rectification error can be seen as the skew window alignment (and skew
drainpipe) on the left of the image. 
Furthermore the yaw value of the camera (horizontal viewing angle) was (relative
to the other datasets) high, making parts of the windows occluded.  The height
of the windows on the right side of the image is 698 px, at the left side this
is 320 px: a resolution of more then 2 times smaller which makes it hard to detect
the left windows. Trilinear interpolation is used to minimize this loss.
To reduce the number of handicaps (and to focus on the rectification and
occlusion error) we cut off the bottom of the image which included cars, unaligned doors and windows.
\fig{w_Spil1TransCrop1_ImOri.eps}{Dataset: Anne1, Rectified image}{0.45}
\clearpage

\subsubsection{Anne2}
This dataset contains images of the same scene as Anne1.
It has zero rectification error: the windows are perfectly aligned, although the
resolution on the left is as in the Anne1 dataset, two times smaller.
The challenges of this dataset are the occlusion artefacts and the 
bottom area of the image (cars, unaligned doors and windows).
\fig{w_Spil1TransImproved_ImOri.eps}{Dataset: Anne2}{0.6}
\clearpage

\subsubsection{Dirk}
The Dirk dataset represents an everyday scene as it contains light spots on the facade,
bicycles and an occluding tree.  It contains zero rectification error but the
windows are partially aligned.  The windows are very close to each other (making it
hard to detect non-window areas between them), furthermore they differ in shape, size
and in type.  The yaw of the camera is relatively low, implicating little or no
occlusion artefacts.
\fig{w_Dirk4Trans_ImOri.eps}{Dataset: Dirk}{0.55}
\clearpage

\TODO{tabel van datasets en handicaps!}



\subsection{Method II: Histogram based approach} 
\label{sec:method2}
\subsubsection{Introduction}
In the previous section we saw that from a series of images, a 3D model of a
building can be extracted. Furthermore we saw that using this 3D model the
scene could be converted to a frontal view of a building, where a building wall
appears orthogonal.  This frontal view enables us to assume orthogonality and
alignment of the windows.  We exploit these properties to build a robust window
detector as follows: first we rectify the image as described in the previous section.  Then the alignment of the windows
is determined. This is based on a histogram of the Hough lines. We use this
alignment to divide the image in window and not window regions.  Finally these
regions are classified and combined which gives us the windows.
We present a regular and alternative window alignment method
followed by two different kind of window classifications. 


\paragraph{Situation and assumptions}
To be more precise in our assumptions, we assume the windows have orthogonal
sides.  Furthermore we assume that the windows are aligned. This means that a
row of windows share the same height and $y$ position. For a column of windows
the width and $x$ position has to be equal.  Note that this doesn't mean that
all windows have the share the same size.



\subsubsection{Extracting the window alignment}
\paragraph{Regular window alignment}
\TODO{Schematje}
%explain pipe line
%(color transform)
%edge extraction
%Houghline extraction
We introduce the concept alignment line. We define this as a horizontal or
vertical line that aligns multiple windows. In Figure
\ref{fig:w_Spil1TransCrop1_ImHibaap.eps}
we show the alignment lines as two groups, horizontal (red) and
vertical (green) alignment lines.  The combination of both groups give a grid of
blocks that we classify as window or non-window areas.\\

\fig{w_Spil1TransCrop1_ImHibaap.eps}{Dataset: Anne1, Regular window alignment (parallel projection): Based on a smoothed histogram (red line) that displays the amount of overlapping Hough lines, for the column division the horizontal Hough lines are counted (at each $y$ position), for the row division the vertical Hough lines are counted (at each $x$ position) }{0.45}


% MOTIVATION
How do we determine these alignment lines? We make use of the fact that among a
horizontal alignment line a lot of horizontal Hough lines are present, see
Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}. For the vertical alignment lines
the number of vertical Hough lines is high, see the green lines.

We begin by extracting the pixel coordinates of Hough transformed line
segments. We store them in two groups: horizontal and vertical.% (crosses in Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}). 
We discard the dimension that is least informative by projecting the coordinates to
the axis that is orthogonal to its group. 
This means that for each horizontal Houghline the coordinates on the line are projected to the X
axis and for each vertical Houghline the coordinates are projected to the Y
axis. We have now transformed the data in two groups of 1 dimensional
coordinates which represent the projected position of the Hough lines.\\

Next we calculate two histograms H(orizontal) and V(ertical), containing respectively
$w$ and $h$ bins, where $w$ x $h$ is the dimension of the image.  The histograms
are presented as small yellow bars in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.

The peaks are located at the positions where an increased number of Hough lines
start or end.  These are the interesting positions as they are highly correlated
to the alignment lines of the windows. 

It is easy to see that the number of peaks is far more then the desired number of alignment lines.
Therefore we smooth the values using a moving average filter.
The result, red lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}
, is a smooth \emph{projection profile} which contains the right number of peaks. The peaks
are located at the average positions of the window edges. \\

Next step is to calculate the peak areas and after this the peak positions. 
Before we find the peak positions we extract the peak \emph{areas} by thresholding the
function. To make the threshold invariant to the values, we set the threshold to 0.5 $\cdot$ max Peak. 
This value works for most datasets but is a parameter that can be changed.
%The two thresholds are presented as black dotted lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.\\
Next we create a binary list of peaks P, P returns 1 for positions that are contained in
a peak, i.e. are above the threshold, and 0 otherwise.
We detect the peak areas by searching for the positions where P = 1
(where the function passes the threshold line). 
If we loop through the values of P we detect a peak-start on position $s$ if ${P(s-1),P(s)}={0,1}$
and a peak-end on $e$ if ${P(e-1),P(e)}={1,0}$. 
I.e. if P = 0011000011100, then two peaks are present. The first peak covers positions $(3,4)$, 
the second peak covers $(9,10,11)$.\\

Having segmented the peak areas, the next step is to extract the peak positions. 
Each peak area has only one peak and since we used an average smoothing filter, the shape of 
the peaks are often concave. Therefore we extract the peaks by locating the max of each peak area. 
These locations are used to draw the window alignment lines, they can be seen
as dotted red lines and dotted green lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.

\paragraph{Alternative window alignment}
As you can see in Figure \ref{fig:w_Spil1TransImproved_Xv.eps}
a few window alignment lines are not found and a few lines are found at wrong locations.
The right side of the window frame of the first 4 columns of windows is not found.
This means we have to find another way to detect the window alignment on these positions.\\

For the vertical alignment we only took vertical lines into account.
In this method we examine the projection profile of the \emph{horizontal} Hough lines
projected on the X axis, $X_h$, Figure \ref{fig:w_Spil1TransImproved_Xh.eps}.
On the positions of the desired vertical alignment lines there appears to be a 
big decrease or increase of $X_h$ at the window frame. This is because on these
positions a window containing (a large amount of horizontal lines) starts or end.

We detect these big decreases or increases by creating a new pseudo peak profile
$D$ that takes the absolute of the derivative of $X_h$, Figure \ref{fig:w_Spil1TransImproved_Xh.eps}.
\[D = abs( X_{h}')\]
Next we extract the locations of the peaks as the previous method.

\paragraph{Fusing the window alignment methods}
We have presented two window alignment methods, next we fuse the methods to
gain a robust window alignment.
The target is to have as few as possible false positives while detecting all alignment locations.\\

If a window alignment position is found by both methods, often the peaks are
located very close to each other, see Figure
\ref{fig:w_Spil1TransImproved_Xvh.eps}.  If this is the case we want to fuse the results by grouping the peaks.
Most of the times the peaks indicate the same window alignment but have
some disparity.  This is often the case when horizontal lines stop at the \emph{inside}
of a window frame while the vertical edges are located at the \emph{outer} side of the
window frame (this is supported by the fact that the disparity is often the size of the window frame part).  
Yet, in other cases close peaks indicate different windows that are just happen to be
located closely.  To apply a proper grouping of the peaks the challenge is to
distinguish these two cases.\\

First we decrease the total number of found window alignment locations by increasing the individual thresholds (from 0.3*max
peak to 0.5*max peak). Note that this has a positive side effect that the peaks that are found are more certain.
After this we group the peaks as follows: \\
First we calculate the average of the maximum window frame part and the minimum window
distance and call this the maximum peak group distance $G$.
Next we compare all peaks and if the distance between two peaks is lower than
$G$, we discard the peak with the least evidence (lowest peak). The result,
a set of unique peaks, can be seen in Figure \ref{fig:w_Spil1TransImproved_WAlinesMerged.eps}.\\

The advanced peak grouping is only required for the vertical alignment
of the windows: The horizontal inter window distance is often big enough to not
be mistaken by a window frame part.

\TODO{peak merging result misses for 1 dataset?}
\TODO{peak merging result plaatje met en zonder merging onder elkaar}

\newpage
\paragraph{Results}

\fig{w_Spil1TransImproved_Xv.eps}{Dataset: Anne2, Regular window alignment: Based on a histogram that displays amount of overlapping vertical Hough lines at each $x$ position}{0.7}
\fig{w_Spil1TransImproved_Xh.eps}{Dataset: Anne2, Alternative window alignment (orthogonal projection): Based on the shape of the smoothed histogram function. 
For the column division, the number of \emph{horizontal} Hough lines is counted (Note that this is the orthogonal opposite of the regular window alignment method). 
Peaks (that represent a big decrease or increase of the histogram function) are used for the alignment.}{0.7}
\fig{w_Spil1TransImproved_Xvh.eps}{Dataset: Anne2, Regular+Alternative window alignment combined}{0.7}

\fig{w_Dirk4Trans_ImHibaap_Xvh.eps}{Dataset: Dirk, Regular+Alternative window alignment combined}{0.45}
\fig{w_Dirk4Trans_ImHibaap_Xvh_zoom.eps}{Dataset: Dirk, Regular+Alternative window alignment combined, zoomed}{0.45}
\clearpage

\paragraph{Discussion}
The results are promising, as for both datasets 100 \% of the alignment lines
are positioned either at the boundary of a window area or inside a window area.

\subparagraph{Discussion:Many alignment lines}
In Figure \ref{fig:w_Dirk4Trans_ImHibaap_Xvh.eps}, too many alignment lines
detected and not every alignment line is placed correctly.  This is mainly
because the windows are partially aligned: the two front doors contain many
vertical edges that are not aligned with the upper windows therefore a double alignment
is found (doors and windows).  Another cause of this artefact is that the
bicycles on the left (see Figure \ref{fig:w_Dirk4Trans_ImHoughResult.eps}) cause
a large amount of found Hough lines which implies a (false) alignment line.
Although the causes are clear, we don't have to worry about this additional
window alignment lines that lie inside a window area as adjacent window areas
are grouped by the classification module.\\
Fortunately no alignment lines are found at non-window areas.


\subparagraph{Discussion:Fusing the methods}
If we evaluate the independent window detection results on the Anne2 dataset we see (Figure
\ref{fig:w_Spil1TransImproved_Xh.eps}, \ref{fig:w_Spil1TransImproved_Xv.eps})
that in both datasets neither the regular window alignment method nor the
improved window alignment detect 100\% of the window alignment lines.  The
effect of fusing the methods is high as, after fusing, 100\% of the window
alignment is found on both datasets: each window alignment line is detected by
at least one method.  This is basically explained by the fact that both methods
are based on a different Houghline direction.  We now discuss the interesting
cases, where only one method succeeds in detecting the window alignment.\\

\label{lab:occlusion} If we take a look at the regular window alignment in
Figure \ref{fig:w_Spil1TransImproved_Xv.eps} we see that for the first 4 window
columns the right side of the window is not detected.  This is because the
original (unrectified) image (Figure \ref{fig:w_spil6ImOri.eps}) is not a
frontal image.  This makes building wall extensions (middle of Figure
\ref{fig:w_spil6ImOri.eps}) or drainpipes occlude parts of the window.  In this
case the windows are countersunked into the wall making the building wall itself
occluding the right window frame (Figure \ref{fig:w_Spil1TransImproved_Xv.eps}).
The color of the reflection of the window is very similar to the bricks and
because the window and wall are not separated by the window frame, the edge
detector doesn't find a strong edge.  This means that on all positions where the
window frame is missing, no vertical Hough lines will be detected and (as the
regular window alignment is based on the amount of vertical Hough lines) no window
alignment will be found.  This artefact can be studied in the edge image Figure
\ref{fig:w_Spil1TransCrop1_ImEdge.eps}: few or no edges are present, and at the
low height of the peaks in Figure \ref{fig:w_Spil1TransImproved_Xv.eps} at these
positions.\\

However, the alternative window alignment does find a window alignment on this
positions. This is because the method is based on the opposite Houghline
direction : for the vertical window alignment the horizontal Houghline direction
is taken into account.  This occlusion artefact has no effect on the horizontal
Hough lines which makes the alternative window alignment method a strong
alternative for the alignment of (partially) occluded windows.\\

In general, the alternative window alignment performs better then the regular
window alignment method.  This is because this method takes the  horizontal
window frame parts into account which is a priori stronger as there are more
horizontal window parts then vertical window parts present.  E.g. Figure
\ref{fig:w_Spil1TransCrop1_ImHoughResult.eps} every window has (as it has two
vertical sub windows) 3 horizontal window frame parts but only 2 vertical window
parts).  Furthermore the method is more robust because it relies on a higher
level of histogram interpretation (by using the derivative).  However, in a few
cases alternative window alignment method is outperformed by the regular window
alignment . For example, in Figure
\ref{fig:w_Dirk4Trans_ImHibaap_Xvh_zoom.eps}, the left
side of the second window column is not detected.
This is because this window type has no horizontal subdivisions. Only the top and
bottom of the window frame produce an edge, therefore the derivative of the
amount of Hough lines will return a small peak on this position (which is too
small to survive the threshold).  
The threshold is a priori hard to survive because it has a high value as it is
determined by taking a fraction of the maximum peak which is located at the
windows that do have horizontal subdivisions (for example the most left or most
right window column)). 


\TODO{result1 drain pipe result niet erg ivm vlg stap}



\paragraph{Future research}
\subparagraph{Future research:Determine window alignment of different window types}
A solution of the missing window alignment lines on a scene with different
window types (Figure \ref{fig:w_Dirk4Trans_ImHibaap_Xvh_zoom.eps}) would be to
decrease the threshold if multiple window types are found. One could design a
measure of variety of the window types. This could be done by taking the variation of the
derivative of the amount of Hough lines. This amount of variation will determine
the amount of decrease in threshold. Let's explain this by two examples:
If there is just a few variation the maximum peak is very representative for a
window so the threshold could be for example 0.7*max peak. However if there is a
large variation is found (which means multiple window types are present), the
threshold should be lowered to 0.3*max peak to detect the hard window types
(with few horizontal divisions).
Another method would be to cluster the amount of Hough lines in $n$+1 values
where $n$ is the number of window types (the 1 is for the non-window area).
Areas that transcend from a window area cluster to a non-window area cluster
(or vice versa) are determined as the window alignment locations.
For both methods the challenge is again to detect the window alignment with
unknown window types but keep the number of false positives (e.g. a drain pipe)
zero.

\subparagraph{Future research:Peak grouping}
We fused the result of the horizontal and vertical Hough lines and discarded
peaks that where close.  A more accurate result would be achieved if close peaks
where averaged, the height of the peak could be used as a weight.  We used a
manual maximum peak group distance ($G$), this value could also be automatically
derived from the image by for example taking a percentage of the window, or by
detecting the size of the window frame parts.\\ It is challenging to handle
multiple close peaks, e.g. if 4 peaks are close, then peak 1 could indicate the
same window as peak 2 but indicate a different window then peak 4. The location
of the close peaks: inside, at the border, or outside the window could add
important evidence, some methods of the window classification could be used to
detect these values.

\subparagraph{Future research:Window alignment refinement}
To get more accurate result or to handle scenes with poor window alignment a
refinement procedure could be applied.  As mentioned in the related work, Lee et
all \cite{Lee_extraction} applied window refinement.  Although this comes with
accurate results, the iterative refinement is a computational expensive
procedure.  It would be nice to have a dynamic system that is aware of this
accuracy and computational time trade of. A system that only refines the results
when the recourses are available.  For example if a car is driving and uses
window detection for building recognition the refinement is disabled.  But if
the car is lowering speed the refinement procedure could be activated.
Resulting in accurate building recognition which opens the door for augmented
reality.\\

Both window refinement and window alignment steps could use some additional
evidence which could be provided by feature based methods.  For example a
\emph{multi scale Harris corner detector} could help an accurate alignment or
refinement of the windows.








\subsubsection{Basic window classification (based on line amount)}
The image is now divided in a new grid of blocks based on these
alignment. The next challenge is to classify the blocks as window and
non-window areas: the window classification, we developed two different methods for this.

Instead of classifying each block independently, we classify full rows and
columns of blocks as window or non-window areas.  This approach results in a accurate
classification as it combines a full blockrow and blockcolumn as evidence for a singular
window. 

The method exploits the fact that the windows are assumed to be
aligned.
A blockrow that contains windows will have a high amount of vertical
Hough lines, Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}
(green). For the blockcolumns the number of horizontal Hough lines
 (red) is high at window areas.  We use this property to classify 
 the blockrows/blockcolumns. 

For each blockrow the overlap of all vertical Hough lines are summed up.
(Remark that with this method we take both the length of the Hough lines and
amount of Hough lines implicitly into account.)

To prevent the effect that the size of the blockrow influences the outcome, this total value
is normalized by the size of the blockrow.
\[\forall Ri\in \{1..numRows\} : R_i = \frac{HoughlinePxCount}{R_i^{width} \cdot R_i^{height}}\]

Leaving us with $||R||$ (number of blockrows) scalar values that give a rank of a blockrow begin a window area or not.
This is also done for each blockcolumn (using the normalized horizontal amount of
Hough lines pixels) which leaves us with $C$.

\TODO{DIT PLAATJE eruit en mooie staafdiagrammen erin}
\fig{w_Spil1TransCrop1_ImClassRectBarh.eps}{Classification values for window
block rows representing the normalized vertical Houghline pixel count of
(R)}{0.6}
If we examine the distribution of $R$ and $C$, we see two clusters appear: one with
high values (the blockrows/blockcolumns that contain windows) and one with low
values (non-window blockrows/blockcolumns). For a specific example we displayed the values of $R$ in Figure \ref{fig:w_Spil1TransCrop1_ImClassRectBarh.eps}.
Its easy to see that the high values, blockrow 4,5,7,8,10 and 11, correspond to the
six window blockrows in Figure \ref{fig:w_Spil1TransCrop1_ImClassRect.eps}.

How do we determine which value is classified as high?  A straight forward
approach would be to apply a threshold, for example 0.5 would work fine.
However, as the variation of the values depend on (unknown) properties like the
number of windows, window types etc., the threshold maybe classify insufficient
in another scene.  Hence working with the threshold wouldn't be robust. 

Instead we use the fact that a blockrow is either filled with windows or not, hence
there should always be two clusters.  We use \emph{$k$-means} clustering (with
$k=2$) as the classification procedure.
\TODO{ref}
This results in a set of Rows and Columns that are classified as window an
non-window areas.

The next step is to determine the actual windows $W$.
A block $w\in W$ that is crossed by $R_j$ and $C_k$ is classified as a
window iff \emph{$k$-means} classified both $R_j$ and $C_k$ as window areas. These are displayed in 
 Figure \ref{fig:w_Spil1TransCrop1_ImClassRect.eps} as green rectangles.

The last step is to group a set of windows that belong to each other. This is done by 
grouping adjacent positively classified blocks. These are displayed as red
rectangles in Figure \ref{fig:w_Spil1TransCrop1_ImClassRect.eps}.

As the figure gives a binary representation of the windows it is not possible
to see how certain a block is classification.
To get insight about this we developed a measure of certainty function.

\[P(R_i) = \frac{R_i}{max(R)}\]
\[P(C_i) = \frac{C_i}{max(C)}\]
\[C(w) = \frac{P(w^{R_i}) + P(w^{C_i})}{2}\]
As you can see $C$ is normalized, this is to ensure the value of the maximum
certainty is exactly 1. The results can now be relatively interpreted, e.g. if the rectangle's $P=0.5$
then the system nows for 50 \% sure it is a window, compared to its best window ($P=1$). 
And, as the normalization implies this, there is at least one window with $P=1$. 

The visualization of the measure of certainty is shown in Figure
\ref{fig:w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps},  
the whiter the area the higher the measure of certainty.



\newpage
\paragraph{Results} % basic window classification
\fig{w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps}{Dataset: Anne1, Basic window classification method:Measure of certainty, 
the whiter the area the higher the measure of certainty.}{0.45}
\fig{w_Spil1TransCrop1_ImClassRectRot.eps}{Dataset: Anne1, Basic window classification method: The extracted windows, red:the grouping}{0.65}
\clearpage

\TODO{include results of binary classification}
The bright rows and columns in Figure
\ref{fig:w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps},  indicate window
blockrows and blockcolumns, whereas the dark rows and columns indicate
non-window areas.
The stripe patterns support that the classification process exist of individual
row and column classification.
The area of window positions is particularly white, as it 
intersects a bright (positively classified) row and column.
One can compare this result to the windows in the original image 
(Figure \ref{fig:w_Spil1TransCrop1_ImOri.eps}).

\TODO{include table of false positives etc, detection rate etc. etc. misch staat
dit nog op github}


\paragraph{Discussion}  % 
The outcome of this method is non-deterministic, as it depends on to the random
initialization of the cluster centers. This means that our results could be
correct by coincidence.  To exclude this artefact, we ran the cluster algorithm
10 times on all datasets. Unfortunately for the Dirk dataset, 2 of 10 times it 
resulted in a bad result. This result can be found in Appendix
Figures \ref{fig:w_Dirk4Trans_ImClassRectI_kmeansTilt_barsH.eps},
\ref{fig:w_Dirk4Trans_ImClassRectI_kmeansTilt_barsV.eps},
\ref{fig:w_Dirk4Trans_ImClassRectI_kmeansTilt_prob.eps} and
\ref{fig:w_Dirk4Trans_ImClassRectI_kmeansTilt_windows.eps}.
This result can be explained as follows. The Dirk dataset has window types in the
middle of that contain a horizontal subdivision while the others don't.
The windows in the middle will cause k-means to drag the cluster center to a
value that is too high making windows classified as non-window areas.

\paragraph{Future research}
A solution to the bad luck on the initialization of the cluster centers 
is to increase the number of cluster centers to $n$+1, 
where $n$ is the number of window types (the 1 is for the non-window area).




\subsubsection{Improved window classification (based on shape of the histogram function)}
\TODO{introduce a little bit}
\TODO{say that anne2 dataset is more rectified }
If we take a look at Figure \ref{fig:w_Spil1TransImproved_WAlinesMerged.eps}
we see that $X_h$ (the amount of horizontal Hough lines) has two shapes that
repeat:  At the location where a window is present $X_h$ is concave whereas at
non-window areas $X_h$ is convex. 
This is because the window contains framed sub windows that create 
many edges. The number of edges increase is large at the centre of the window 
At positions that lie between the windows (the non-window areas) the number of
edges is low. 
The concave and convex shape of $X_h$ also supports that these values 
increase towards the window center and decrease towards
non-window area centers. This artefact is used for our 
our second window classifier.\\

This shape type of $X_h$ is detected as follows:
We took a similar approach as in improved window alignment (). First we examine the derivative of $X_h$, blue line in Figure
\ref{fig:w_Spil1TransImproved_WAlinesMerged.eps}.  Next we investigate the positions
where $D=X_{h}'$ changes from sign, these are the peaks or valleys of $X_h$.
$X_h$ is concave at the sign changes from positive to negative (+,-) and $X_h$
is convex if the sign changes (-,+).\\

We expect one sign change per block, however it is possible that multiple sign
changes occur. In this case we smooth $X_h$ again and repeat the algorithm
until for each block a maximum of one sign change is found. \\

Now we have detected the shape type (concave or convex), we can directly
classify the blockrows and blockcolumns as window areas and non-window areas.
The windows are determined as the previous classifier by combining the
(positively classified) blockrows and blockcolumns.\\

For the purpose of clear illustration, only blockcolumns are drawn. The method for
the blockrows is almost the same: the projection profiles are projected to the
Y-axis.

\newpage
\paragraph{Results} % improved window classification
\TODO{make a diagram, histogram - alignment lines -> window classification etc.}
\TODO{discuss drain pipe, dat het goed is dat het resultaat mooi is ondanks drainpipe}

\subparagraph{Results:Dirk dataset}
\fig{w_Spil1TransImproved_WAlinesMerged.eps}{Dataset: Anne2, Improved window
classification method: The red line shows concave shapes at window locations and
convex shapes at non-window locations}{0.7}
\fig{w_Spil1TransImproved_windowsGrouped.eps}{Dataset: Anne2, Improved window classification method: The extracted windows, red:the grouping}{0.7}

Although the scene contains a lot occlusion artefacts and suffers resolution
loss, especially on the left side of the image, the results for the Anne2
dataset, a 100\% detection and a 100\% true positive rate are very well.
%todo

This is mainly due the high interpretation level of the Histogram function.  We
used the fact that the histogram function has a very consistent pattern, at
every window area its shape is convex and every non-window its shape is concave.
\clearpage


\newpage
\subparagraph{Results:Anne2 dataset}

\fig{w_Dirk4Trans_ImOri.eps}{Dataset: Dirk, Rectified image of a realistic scene which is realistic as it contains light spots, bicycles. Note that the windows are partially aligned and the differ in size and type}{0.55}
\fig{w_Dirk4Trans_ImHibaap_WAlinesMerged.eps}{Dataset: Dirk, Improved window
classification method: The red line shows concave shapes at window locations and
convex shapes at non-window locations}{0.45}
\fig{w_Dirk4Trans_ImClassRect.eps}{Dataset: Dirk, Improved window classification method: The extracted windows, red:the grouping}{0.45}
\TODO{IMPROVED result on Anne1 dataset, investigate which combinations of dataset and methods i miss}

\begin{table}[t]
\caption{Improved window classification results on Anne2 and Dirk dataset}
\begin{tabular}{|l||c|c|}
\hline
Type										& Anne2 dataset (Fig \ref{fig:w_Spil1TransImproved_windowsGrouped.eps})	& Dirk dataset (Fig \ref{fig:w_Dirk4Trans_ImClassRect.eps})\\
\hline
Detection rate								& 100 \% 	& 100 \%	 \\
True positive rate							& 100 \%	&  87 \%	 \\
False positive rate							&   0 \%	&  11 \%	 \\
False negative rate							&   0 \%    &   9 \%     \\
\hline
\end{tabular}
\end{table}

\begin{table}[t]
\caption{Grouping results on Anne2 and Dirk dataset}
\begin{tabular}{|l||c|c|}
\hline
Type										& Anne2 dataset (Fig \ref{fig:w_Spil1TransImproved_windowsGrouped.eps})	& Dirk dataset (Fig \ref{fig:w_Dirk4Trans_ImClassRect.eps})\\
\hline
\hline
Grouped correct			& 100 \%  & 90\%  \\
Grouped incorrect		&  0 \%  & 10\%  \\
\hline
\end{tabular}
\end{table}
\clearpage



\TODO{certainty based (white strokes) classification on anne2 dataset, pas als
skyline af is, img zijn wel al klaar}

\TODO{remark that we goedkeuren partieel resultaat van windows helemaal rechts
en links dirk dataset}


%\paragraph{Discussion} eigenlijk een discussie maar toch ook weer niet

\TODO{Interpretation of the results of the Anne1 dataset}
%\subparagraph{Interpretation of the results of the Anne1 dataset}
%dit kan pas als ik ook ann1 op 
%The scene suffers  from..
%contained and this can be found in the results


\TODO{compaire to other classification methods}
\TODO{(perform both classifications on both datasets (2x3 img), visual concatenate images vertically)}




%\subparagraph{Interpretation of the results of the Dirk dataset}
\TODO{especially the horizontal division of the lowest row of windows is amazing, refer to}

\TODO{Meer over lof spreken} 
We discussed in the section about window alignment that the classification
module would handle the redundant window alignment lines. 
This can be seen in Figure \ref{fig:w_Dirk4Trans_ImClassRect.eps} where the 
classification took advantage of the fact that all alignment lines are located at a boundary of a window or in a window. 
The peak fusing module discarded many close alignment lines and the residual redundant alignment lines create
small green adjacent sub windows which where grouped to clear red big windows.\\

This grouping result is promising as 90\% of the grouping went well.
This is a quite good result given the scene contains a large variety in window
shape, window size and window type.  This result is mostly due an almost perfect window alignment and classification 
(as the grouping only groups adjacent positively classified sub windows).
The first two window rows in Figure \ref{fig:w_Dirk4Trans_ImClassRect.eps}
however are classified as two groups but this should be one.
Normally the peak merging step would handle this problem, but with this dataset
we could not use a large \emph{maximum peak distance} value because the windows
in the image (especially the first and last columns) are very close.  More on
this in \emph{Future research on window classification
(\ref{sec:futureResearchWindowClassification})}.



\paragraph{Future research} % improved classification
\TODO{put a opbouwende line in future work}
\label{sec:futureResearchWindowClassification}
\subparagraph{Future work:Extensive evaluation methods}
As the classification worked very well it would be interesting to find out on which
point it will fail?  This could be investigated by using low quality images (which are taken for
example with a cell phone), furthermore we could downscale the images and/or add
Gaussian noise to the data.\\
\TODO{speak about video, or sequence of images, detection + tracking etc}

The results of the classification module depend heavily on the result of the
window alignment. It would be nice feature research to make an independent
evaluation of the classification modules.  This could be done with a random
window alignment generator. Some evaluation should be developed and it would
help if the windows are annotated.

\subparagraph{Future work:Grouping error minimization}
Although the grouping module gave promising results it can be optimized.  The
grouping module now groups adjacent positively classified window areas.  Some
window in Figure areas are false negatively classified, see the left side of
Figure \ref{fig:w_Dirk4Trans_ImClassRect.eps}.  This is caused by a small area
between the windows that is classified as a non-window area. This could be
solved by by adding a minimum size constraint of a area to be threaded as a
non-window area.  In this way small negatively classified areas cannot interrupt
the adjacent windows.

\subparagraph{Performance measure for extreme viewing angles}
It would be nice to investigate the effect of the occlusion and to examine the
robustness of the window detector under extreme viewing angles.
For example the viewing angle could be plotted against the percentage of
correct detected windows.


%\subsubsection{Comparison of basic and improved window classification}
\TODO{compare classification methods and explain differences}





\subsection{Conclusion}
% not summarize!
% what can be derived from the results+?
\TODO{We showed that projecting the image to a frontal view is a good preprocessing step of a robust window detector.}
\TODO{Furthermore our algorithms work in real time. \\}


We proposed 

\subsubsection{Connected corner approach} % cCorner
As can be concluded from our results, the connected corner method is suitable
for, and robust to, scenes with a variation in window sizes and types. This
	makes the connected corner approach suitable for a wide range of window
	scenes where no or few prior information about the windows is known.

	Furthermore, the system has small requirements on the input data.
	It is independent of calibrated input data and 3d information
	about the building and no image rectification is required. 

\subsubsection{Window alignment}

% histograms alignment is gooood
We can conclude that developing histogram functions of the amount of Hough lines
is a strong approach towards determination of the window alignment as the
results in this work and in previous work a very promising.

% alternative method is better
We proposed two window detection methods and can conclude that the alternative
approach performs better because it is based on a horizontal window division and
because it uses a higher level interpretation of the histogram function.

	\paragraph{Occlusion}
	We showed that window alignment becomes a challenging task if the windows are partially
	occluded (due to for example a non frontal viewing angle).  One of the main solutions 
	we proposed is to use multiple window
	detection approaches.  To have a 100\% detection rate one need to be certain
	that if one approach fails at least one other approach must succeed.  We
	proposed the strong combination of two methods where the first method filled the
	gab of the second method and the other way around.
	For example we took care of the window parts that suffer vertical occlusion by a method that
	detects horizontal window parts.  We can conclude that a method that fills the
	gab of the occluded alignment locations must generaly rely on Hough lines that
	lie in the orthogonal direction of the occlusion.

	\paragraph{Relative thresholding fails at differing window types}
	Furthermore we discussed the implication of the use of different window types in
	the same scene.  We can conclude that applying a threshold that is relative to a
	maximum peak doesn't work well on window types that differ.  (in for example
	horizontal subdivisions).  This is caused by the relative high maximum peak
	which is determined by the window with many subwindows which produce a large
	amount of edges.  The window types with few subdivision don't survive this
	threshold and missing alignment lines is the result (Figure
	\ref{fig:w_Dirk4Trans_ImHibaap_Xvh_zoom.eps}).  This means that if we want to
	stick with the relative thresholding method, we have to assume a certain
	equality of the window types. Otherwise an alternative thresholding method
	should be developed (e.g. a threshold that is altered depending on the window
	type).

	\TODO{make diagram: edge- hough -window alignment - window classification - window grouping}
	\TODO{diagram maken window alignment -> peak merging -> etc}


\subsubsection{Window classification} %window classification method 1
	We discussed two different window classification methods. 
	\paragraph{Certainty based window classification} %window classification method 1
	The certainty based window classification works quite good on
	the dataset we used.  However, it requires a very well alignment of the windows which
	is a disadvantage.  As it is based on the number and length of the found Hough lines, the
	errors in the window alignment will propagate to the classification.  This
	makes this classification method inappropriate for a system where one cannot
	fully rely on the window alignment.  \\

%---- ondrestanad in begin zetten?
	This conclusion amplified the need for a robust classification method (that is
	independent, (or at least less sensitive)) to window alignment errors.

	% zie fb frans
	\paragraph{Histogram based window classification} %\subsubsection{Method II: Histogram based approach} 
	It is a typically artificial intelligent approach, to look for a high
	interpretation of the data.  We can conclude that the results become very robust
	if one takes a high level interpretation of the Histogram function. \\
	Furthermore we can conclude that:

	\begin{itemize}
		\item Redundant lines found by the window alignment module cause no
	problems as long as they are located at a boundary of a window or in a window.
		\item The grouping module contained a small weakness, a fix is provided in the next
		section.
		\item The improved window classification outperformed the basic window
		classification.
	\end{itemize}

