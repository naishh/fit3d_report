%TODO pipeline rectification process
%
% 		motion estimation
% 		scale estimation
% 			produces MAP
% 		plane fitter
% 			produces normal of wall
% 		edgeIm or houghlines reprojection
%


% list of figures
% - original image
% -red and green houghlines, with ori figure as background
% -hibaap with 0.7 times max peak
% -figure with only the windows as big red rectangular areas
% -grayvalues voting
%	- values that fall out of cluster are displayed striped
% -grayvalues voting with clusterlines
% 	- or clustervalues in colored bar graph


\section{Window detection}







% TODO explain efficiency of Houghline coordinate transformation (instead of
% transforming the image
\label{chap:windowDetection}
\subsection{Introduction}
\fig{datasetIm.eps}{Original image}{0.6}
\fig{datasetImRectified.eps}{Rectified image}{0.6}
From the previous section we know that from a serie of images a 3D model of a
building can be extracted. Furthermore we saw that with the 3D information the
scene could be viewed from another viewing point. 

We projected the scene to a frontal view of a building, where a building wall appears
orthogonal, and showed what interesting possibilities this opens.
One example is robust window detection.
In this section we present three developed methods for robust window detection
and discuss the effect of the scene transformation.

We begin with an approach that is invariant to viewing direction.  Then we
present our second method that assumes orthogonal and aligned windows.


\subsubsection{Related work}
TODO
% tjoint article quoten

\subsection{Edge detection and Houghline extraction} 
% this is described in a early chapter.
Edge detection and Houghline extraction is done as is described in chapter ??
% TODO
The results can be seen in Figure \ref{fig:hibaapEdge.eps} and
\ref{fig:hibaapHough.eps}.
\fig{hibaapOri.eps}{Original image}{0.4}
\fig{hibaapEdge.eps}{Result edge detection}{0.4}
\fig{hibaapHough.eps}{Houghlines with endpoints}{0.4} 
\subsubsection{Efficient Projecting} 
We are interested in the frontal view of the building and it would be straight
forward to project the original image, however this is computational
expensive. To keep the computational cost to a minimum we project only the
Houghlines. The edge detection and Houghline extraction is done on the original
unprojected image. We only project the Houghline segment
endpoints. If $h$ is the number of Houghlines, the number of projections is $2h$
When we project the original image this is $w$ x $h$ where w,h are the dimensions of
the image. To give an indication for dataset %TODO
this means 600 projections in stead of 1572864.

\subsection{Method I: Connected corner approach} 
\subsubsection{Situation and assumptions}
We introduce the concept \emph{connected corner}, this is a corner that is 
connected to a horizontal and vertical line.  
In this method we search for connected corners based on edge information.
The connected corners give a good indication of the position of the windows, as 
a window consists of a complex structure involving a lot of connected horizontal
and vertical lines. 

In this approach the windows could be arbitrarily located and they don't need
to be aligned to each other neither to the X and Y axis of the image.
%voting
%the connected lines give a direction

\subsubsection{Method}
From the edge image we extract two groups of Houghlines, horizontal and
vertical.  We set the $\theta$bin ranges in the Hough transform that control the
allowed angles of the Houghlines to extract the two groups.

Next we pair up horizontal and vertical lines to form a connected corner.
Often a connected corner is not fully connected or over connected.
We consider different types of connected corners, see Figure \ref{fig:cCornerTypes} 
\fig{cCornerTypes}{First row: different type of connected corner candidates. Second row: the
result the clean connected corner}{0.4}

To clean up the lines, the algorithm discards intersections between the
horizontal and vertical Houghlines that are far. 
Two intersection point distances are measured: $d_h$ for the horizontal Houghline and $d_v$
for the vertical Houghline.  If the intersection falls on both associated Houghlines,
	the total distance $D=0$.  Otherwise the Euclidean distance is measured from the
	intersection to the closest endpoint. This is done for both Houghlines.  If
	the intersection falls outside both Houghlines (Figure
	\ref{fig:cCornerTypes}(IV)) ($d_h>0$ and $d_v>0$), the total
	distance is calculated by $D=(d_h + d_v)/2$.\\
	Next $D$ is compared to
	a \emph{maximum intersection distance} threshold $midT$.  And if $D<=midT$,
	the intersection is close enough to form a connected corner.\\

After two Houghlines are classified as a connected corner, they are stretched or
trimmed, depending on the situation. The results are shown in the second row in
Figure \ref{fig:cCornerTypes}.
In Figure \ref{fig:cCornerTypes}(I)  the horizontal line is stretched.  Figure
\ref{fig:cCornerTypes}(II) shows that the vertical line is trimmed.  In Figure
\ref{fig:cCornerTypes}(III) both lines are stretched.  At last Figure
\ref{fig:cCornerTypes}(IV) shows how both lines are trimmed.


Because we know the orientation of the connected corner we can estimate where
a window could be located.  We add a vote in the middle of the window. 
This is represented as a blue cross in Figure \ref{fig:cCornerTypes}.
This coordinate is retrieved using the X coordinate of the middle point of the horizontal line
and the Y coordinate of the middle point of the vertical line of the connected corner.  
Note that this is officially not allowed as we did not assume orthogonal windows
neither did we assume the windows to be aligned with the X and Y axis of the
image.  

\subsubsection{Discussion}
A disadvantage of this method is that it only finds plausible window centers
(and for each windowcenter one of the cornerpositions).  It would be more
useful to find the complete window region. 

%This is difficult as mostly by occlusion or by the angle view the window edges are incomplete
The big advantage is that this method doesn't require the windows to be aligned.
Furthermore it's robust to a variation in window sizes. This makes this approach suitable
for a wide range of window scenes where no or few prior information about the
	windows is known.


\subsubsection{Future work}
Remove children cc's leaving only parents
cluster the windows

Connect more parts of the window to form U shaped windows or complete rectangles.\\
Increase vote when window has small sub windows that are included.\\
More accurate middle point of window estimation. 


\subsection{Method II: Histogram based approach} 
\subsubsection{Situation and assumptions}
In this method we assume that the viewing direction of the wall containing the windows is frontal.
If the image isn't the frontal view of the buildingwall we calculated the frontal projection
see section %todo section rectification
To be more precise we assume the windows have orthogonal sides.  Furthermore we
assume that the windows are aligned, This means that a row of windows share the
same height and $y$ position. For a column of windows the width and $x$
position has to be equal.  Note that this doesn't mean that all windows have te
share the same size.

\subsubsection{Method}
The extraction of the windows is done in different steps. 
First the alignment of the windows is determined, this is based on collecting
the Houghlines' start and endpoints. Then we use this allignment to devide the
image in window or not window regions.  Finally these regions are classified
and combined which gives us the windows.


\fig{hibaapHist.eps}{(smoothed) Histograms and window alignment lines}{0.4}
\paragraph{Extract Window alignment}
%explain pipe line
%(color transform)
%edge extraction
%Houghline extraction
We introduce the concept alignment line. We define this as a horizontal or
vertical line that aligns multiple windows. In Figure
\ref{fig:hibaapHist.eps}
we show the alignment lines as two groups, horizontal (red) and
vertical (green) alignment lines.  The combination of both groups give a grid of
rectangles that we classify as window or non-window areas.\\

% MOTIVATION
How do we determine this alignment lines? We make use of the fact that among a
horizontal alignment line a lot of horizontal Houghlines start and end (see red
crosses in Figure \ref{fig:hibmaapHough.eps}. For the vertical alignment lines
the number of vertical Houghline start and ends is high (see green crosses in
Figure \ref{fig:hibmaapHough.eps}.\\

We begin by extracting the coordinates of the endpoints of the Hough transformed line
segments. We store them in two groups, horizontal and vertical% (crosses in Figure \ref{fig:hibaapHough.eps}). 
We project the coordinates to the axis that is orthogonal to the group. This
means that for each horizontal Houghline two coordinates are projected to the X
axis and for each vertical Houghline two coordinates are projected to the Y
axis. We have now transformed the data in two groups of 1 dimensional
coordinates which represent the projected position of the Houghlines.\\

Next we calculate two histograms H(orizontal) and V(ertical), containing respectively
$w$ and $h$ bins where $w x h$ is the dimension of the image.  The histograms
are presented as small yellow bars in Figure \ref{fig:hibaapHist.eps}.

The peaks are located at the positions where an increased number of Houghlines
start or end.  These are the interesting positions as they are highly correlated
to the alignment lines of the windows. 

It is easy to see that the number of peaks is fare more then the desired number of alignment lines.
A common solution would be to decrease the number of bins of the histograms. A
disadvantage of this method is that this comes with a price, it decreases the accuracy. Therefor
we keep the maximum resolution and, instead, smooth the values using a moving average filter.
The result, red lines in Figure \ref{fig:hibaapHist.eps}
, is a smooth function which contains the right number of peaks. Also the peaks
are located at the right positions. Next step is to calculate the exact
positions of these peaks.

Before we find the peak positions we extract the peak \emph{areas} by thresholding the
function. To make the threshold invariant to the values we use 0.5 $\cdot$ max Peak. 
(This value works for most datasets but is made up and can be altered)
The two thresholds are presented as black dotted lines in Figure \ref{fig:hibaapHist.eps}.\\
Next we create a binary function P that returns 1 for positions that are contained in
a peak, i.e. are above the threshold, and 0 otherwise.
% TODO latex, afkijken locate my plate
%P(x)  { 1, H(x)>t
%	  { 0, H(x)<=t
We detect the peak areas by searching for the positions where P = 1
(where the function passes the threshold line). 
If we loop through the values of P we detect a peak-start on position $s$ if ${P(s-1),P(s)}={0,1}$
and a peak-end on $e$ if ${P(e-1),P(e)}={1,0}$. 
I.e. if P = 0011000011100, then two peaks are present. The first peak covers positions $(3,4)$, 
the second peak covers $(9,10,11)$.\\

Having classified the peak areas, the next step is to extract the peak positions. 
Each peak area has only one peak and, since we used an average smoothing filter, the shape of 
the peaks are often concave. Therefor we extract the peaks by locating the max of each peak area. 
These locations are used to draw the window alignment lines, they can be seen
as dotted red lines and dotted green lines in Figure \ref{fig:hibaapHist.eps}

The image is now divided in a grid of rectangular areas. The next challenge is to 
classify the areas as window and non-window areas: the window classification.

% TODO kort iets schrijven over
\paragraph{Window classification}
Instead of classifying each rectangle independedly we classify full rows and
columns as window or non-window areas.  This approach results in more accurate
classification as it uses a full row and column as evidence for a singular
window. The method exploits the fact that the windows are assumed to be
aligned.
%TODO Ref
A row that contains windows is remarkable by its high amount of vertical
Houghlines, Figure % todo
(green). For the columns the number of horizontal Houghlines, Figure %todo
 (red) is high at window areas.  We use this property to classify 
 the rows/columns and we do this as follows.

For each row the number of vertical Houghline pixels that lie in this row are summed up.
(Remark that with this method we take both the length of the Houghlines and amount of Houghlines 
implicitly into account.)

To prevent the effect that the size of the row influences the outcome, this total value
is normalized by the size of the row.
\[\forall Ri\in \{1..numRows\} : R_i = \frac{HoughlinePxCount}{R_i^{width} \cdot R_i^{height}}\]

Leaving us with $||R||$ (number of rows) scalar values that give a rank of a row begin a window area or not.
This is also done for each column (using the normalized horizontal amount of
Houghlines pixels) which leaves us with $C$.

If we take a look at the distribution of $R$ and $C$ we see two clusters apear: one with
high values (the rows/columns that contain windows) and one with low values (non window
rows/columns).  A straight forward approach would be to apply the classification using a
threshold for this value.  However, as the height of the values depend on
(unknown) properties like the number of windows, windowtypes etc., the threshold
would be hard to determine and the method won't be robust. Instead we use the fact 
there should always be two clusters and use \emph{$k$-means}
clustering (with $k=2$) as the classification procedure.
%todo ref
This results in a set of Rows and Columns that are classified as window an
non-window areas.

The next step is to determine the actual windows $W$.
A rectangular area $w\in W$ that is crossed by $R_j$ and $C_k$ is classified as a
window iff \emph{$k$-means} classified both $R_j$ and $C_k$ as window areas, see
Figure %todo
% figure with only the windows as big red rectangular areas

As the figure gives a binary representation of the windows it is not possible
to see the detailed information about the values behind the classification.
Therefor we developed a probabilistic function. 
\[P(R_i) = \frac{R_i}{max(R)}\]
\[P(C_i) = \frac{C_i}{max(C)}\]
\[P(w) = \frac{P(R_i) + P(C_i)}{2}\]
As you can see $P$ is normalized, this is to ensure the value of the maximum
probability is exactly 1. The results can now be relatively interpreted, e.g. if the rectangle's $P=0.5$
then the system nows for 50 percent sure it is a window compared to its best window ($P=1$).

In Figure %todo
the probabilities for each rectangle are displayed.

To get insight about the probabilities that lie behind the individual rows and columns
we designed another representation in Figure %todo
The whiter the area the more probable a rectangle is classified as a window.

\subsubsection{Discussion}
\paragraph{Method I}
\paragraph{Method II}
pro's 
This method is invariant to the height of the values
As the data is 1 dimensional this method doesn't use 
con's
The outcome is non-deterministic, as it depends on to the random initialization of the cluster centers.
% TODO Ref biblograyhf



\subsubsection{Results}
Below the results of the different methods on different datasets.
\fig{cCornerSpilTrans1.eps}{TODO}{0.6}
\fig{hibaapHistSpilTrans.eps}{TODO}{0.4}
% todo discuss results
The results are promissing
On different dataset 
oclusion


\subsubsection{Future work}
Some alternative ideas to classify the rectangles as window or non-window
\begin{itemize}
	\item sum and normalize edge pixels for every block, large amount of edge pixels means window behind rectangle (working on it right now)
\end{itemize}

\subsection{Results}
\subsection{Discussion}  % (What do my results mean to me and why)
\subsection{Conclusion and Future work}


%TODO 
% find a way to compare accuracy window detector
% compare methods, show robustness


% title: Graph Theory and Mean Shift Segmentation Based Classiﬁcation of Building Facades
% 
% Most of the previous algorithms are computationally expensive and not suitable for real time applications. Because
% of the requirements of real-time applications robust and fast
% classiﬁcation of facades is still an open research topic
% 
% 
% Image-based Procedural Modeling of Facades
% 
% 
% 
% refs:
% (canny edge detector:)
% 
% Canny, J., 1986. A computational approach to edge detection.
% IEEE Transactions on pattern analysis and machine intelli-
% gence pp. 679–698.
% 
