%TODO pipeline rectification process
%
% 		motion estimation
% 		scale estimation
% 			produces MAP
% 		plane fitter
% 			produces normal of wall
% 		edgeIm or houghlines reprojection
%
\section{Window detection}
% TODO explain efficiency of Houghline coordinate transformation (instead of
% transforming the image
\label{chap:windowDetection}
\subsection{Introduction}
\fig{datasetIm.eps}{Original image}{0.6}
\fig{datasetImRectified.eps}{Rectified image}{0.6}
From the previous section we saw that from a serie of images a 3D model of a
building could be extracted. Furthermore we saw that with the 3D information the
scene could be viewed from another viewing direction. 

We projected the scene to a frontal view of a building, where a building wall appears
orthogonal, and show what interesting possibilities this opens.
One example is robust window detection.
In this section we present three developed method for robust window detection
and discuss the effect of the scene transformation.

We begin with an approach that is invariant to viewing direction.  Then we
present our second method that assumes orthogonal and aligned windows.  Then we
present a third feature based method.  Finally we show the power of combining
the methods.



\subsubsection{Related work}
TODO
% tjoint article quoten

\subsection{Edge detection and Houghline extraction} 
% this is decribed in a early chapter.
Edge detection and Houghline extraction is done as is described in chapter
The result can be seen in Figure \ref{fig:hibaapEdge.eps} and
\ref{fig:hibaapHough.eps}.
\fig{hibaapOri.eps}{Original image}{0.4}
\fig{hibaapEdge.eps}{Result edge detection}{0.4}
\fig{hibaapHough.eps}{Houghlines with endpoints}{0.4} 
\subsubsection{efficient Projecting} 
We are interested in the frontal view of the building and it would be straight
forward to project the original image, however this is computational
expensive. To keep the computational cost to a minimum we project only the
Houghlines. The edge detection and Houghline extraction is done on the original
unprojected image. Leaving the projection done on the Houghline segment
endpoints. If $h$ is the number of Houghlines the number of projections is $2H$
When we project the original image this is $wxh$ where w,h are the dimensions of
the image. To give an indication for dataset %TODO
this means 600 projections in stead of 1572864.

\subsection{Method I: Connected corner approach} 
\subsubsection{Situation and assumptions}
We introduce the concept \emph{connected corner}, this is a corner that is 
connected to a horizontal and vertical line.  
In this method we search for connected corners based on edge information.
The connected corners give a good indication of the position of the windows as 
a window consist of a complex structure involving a lot of horizontal and vertical lines.

In this approach the windows could be arbitrarily located and don't need
to be aligned.
%voting
%the connected lines give a direction

\subsubsection{Method}
From the edge image we extract two groups of Houghlines, horizontal and
vertical.  We set the Theta bin ranges in the Hough transform that control the
allowed angles of the Houghlines to extract the two groups.

Often a connected corner is not fully connected or over connected.
We consider different types of connected corners, see Figure \ref{fig:cCornerTypes} 
\fig{cCornerTypes}{First row different type of connected corner candidates. Second row the
result the clean connected corner}{0.4}

The algorithm searches for intersections between the horizontal and vertical
Houghlines.  If the intersection is near the endpoints of the Houghlines the
lines are ready to form a connected corner.  The lines are first stretched or
trimmed dependent of the situation. Then a clean connected corner is created by
connecting the line segment endpoints to the intersection.
The result is shown in the second row Figure \ref{fig:cCornerTypes}.
\emph{Frans: shall I put a pseudo code algorithm here?}

Because we know the orientation of the connected corner we can estimate where
a window could be located.  We add a vote in the middle of the window. This
coordinate is retrieved using the X coordinate of the mile point of the horizontal line
and the Y coordinate of the mile point of the vertical line of the connected corner.  
This is represented as a blue cross in Figure \ref{fig:cCornerTypes}.

\subsubsection{Results}
\fig{cCornerSpilTrans1.eps}{TODO}{0.6}



\subsection{Method II: Histogram based approach} 
\subsubsection{Situation and assumptions}
In this method we assume the wall containing the windows to be rectified.
To be more precise we assume the windows to have orthogonal sides.
Furthermore we assume the windows to be aligned.

\subsubsection{Method}
The main idea is that we extract the alignment of the windows based on
calculating histograms of the Houghlines' endpoints.

\fig{hibaapHist.eps}{(smoothed) Histograms and window alignment
lines}{0.4}
\paragraph{Alignment lines}
%explain pipe line
%(color transform)
%edge extraction
%Houghline extraction
We introduce the concept alignment line. We define this as a horizontal or
vertical line that aligns multiple windows. In Figure
\ref{fig:hibaapHist.eps}
we represent the alignment lines as two groups, horizontal (red) and
vertical (green) alignment lines.  The combination of both groups give a grid of
rectangles that have to be classified as window or non-window.
First we explain the extraction of the alignment lines which consist of several
steps.

We begin by extracting the coordinates of the endpoints of the Hough transformed line
segments. We store them in two groups, horizontal and vertical (crosses in Figure
\ref{fig:hibaapHough.eps}). 


We project the coordinates to the axis that is orthogonal to the group. This means
that the horizontal Houghlines are projected to the X axis and the vertical
Houghlines are projected in the Y axis, leaving the data in two groups of 1
dimensional coordinates.

We calculate two histograms H(orizontal) and V(ertical), containing respectively
$w$ and $h$ bins where $w x h$ is the dimensions of the image.  The graphs of
the histograms are presented as small yellow bars in Figure
\ref{fig:hibaapHist.eps}

The peaks are located at the positions where an increased number of Houghlines
start or end.  These are the interested positions as they are highly correlated
to the alignment lines of the windows. 

As can be seen the number of peaks is fare more then the desired number of alignment lines 
A common solution would be to decrease the number of bins of the histograms. A
disadvantage of this method is that this also decreases the accuracy. Therefor
we keep the maximum resolution and smooth the function using a using a moving
average filter.
% with a span of %TODOthe width of 
The result, red lines in Figure \ref{fig:hibaapHist.eps}
, is a smooth function which contains the right number of peaks. Also the peaks
are located at the right positions. Next step is to calculate the exact positions of these
peaks?

Before we find the peak positions we extract the peak \emph{areas} by thresholding the
function, see the black dotted line in Figure \ref{fig:hibaapHist.eps}
% the threshold is set to 20% of max?
We create a binary function P that returns 1 for positions that are contained in
a peak, i.e. are above the threshold, and 0 otherwise.
% TODO latex, afkijken locate my plate
%P(x)  { 1, H(x)>t
%	  { 0, H(x)<=t

We detect the peak areas by searching for the positions where the function
passes the threshold line. 
If we loop through the values of P we detect a peak start on position $s$ if {P(s-1),P(s)}={0,1}
and a peak ends on $e$ if {P(e-1),P(e)}={1,0}. 
I.e. if P = 0011000011100, then two peaks are present. The first covers positions {3,4}, 
the second covers {9,10,11}. 

Having classified the peak areas, the next step is to extract the peak positions. 
Each peak area has only one peak and, since we used an average smoothing filter, each
peak area has a concave shape. Consequently we can easily extract the peaks
by locating the max of each peak area. 
On these locations we have drawn the window alignment lines, dotted red and yellow lines
in Figure \ref{fig:hibaapHist.eps}

The image is now divided in a grid of rectangular areas. The next challenge is to 
classify the rectangles as a window or non-window area.


\subsubsection{Results}
\fig{hibaapHistSpilTrans.eps}{TODO}{0.4}

\subsubsection{Future work}
Some alternative ideas to classify the rectangles as window or non-window
\begin{itemize}
	\item sum and normalize edge pixels for every block 
\end{itemize}


\subsection{Method III: Feature detection approach}
	TODO explain contribution and method of multi scale harris corner detector

\subsection{Fusing the methods}
	TODO

\subsection{Results}
\subsection{Discussion}  % (What do my results mean to me and why)
\subsection{Conclusion and Future work}


%TODO 
% find a way to compare accuracy window detector
% compare methods, show robustness


% title: Graph Theory and Mean Shift Segmentation Based Classiﬁcation of Building Facades
% 
% Most of the previous algorithms are computationally expensive and not suitable for real time applications. Because
% of the requirements of real-time applications robust and fast
% classiﬁcation of facades is still an open research topic
% 
% 
% Image-based Procedural Modeling of Facades
% 
% 
% 
% refs:
% (canny edge detector:)
% 
% Canny, J., 1986. A computational approach to edge detection.
% IEEE Transactions on pattern analysis and machine intelli-
% gence pp. 679–698.
% 
