% set ignorecase



% todo's
% iets schrijven over datasets



%TODO pipeline rectification process
%
% 		motion estimation
% 		scale estimation
% 			produces MAP
% 		plane fitter
% 			produces normal of wall
% 		edgeIm or houghlines reprojection
%


% list of figures
% - original image
% -red and green houghlines, with ori figure as background
% -hibaap with 0.5 times max peak
% -figure with only the windows as big red rectangular areas
% -grayvalues voting
%	- values that fall out of cluster are displayed striped
% -grayvalues voting with clusterlines
% 	- or clustervalues in colored bar graph


\section{Window detection}


% TODO explain efficiency of Houghline coordinate transformation (instead of
% transforming the image
\label{chap:windowDetection}
\subsection{Introduction}
\fig{datasetIm.eps}{Original image}{0.6}
\fig{datasetImRectified.eps}{Rectified image}{0.6}

In this section we present two developed methods for robust window detection and
discuss the effect of the scene transformation.
\emph{ todo motivation}

From the previous section we know that from a serie of images a 3D model of a
building can be extracted. Furthermore we saw that with the 3D information the
scene could be converted to another viewing point. 

We projected the scene to a frontal view of a building, where a building wall appears
orthogonal, and showed what interesting possibilities this opens.
A frontal view of a building comes with orthogonality and alignment of the
windows. We exploit this properties to build a to build a robust window detector
%with a high accuracy for a wide range of scenes.
This projection also comes with some difficulties which we discuss in \ref{lab:occlusion}


We begin with an approach that is invariant to viewing direction.  Then we
present our second method that assumes orthogonal and aligned windows.


\subsubsection{Related work}
\emph{TODO}
%todo

% tjoint article quoten

\subsection{Projection}

\fig{w_Spil1TransCrop1_ImOri.eps}{Rectified image}{0.45}
\fig{w_Spil1TransCrop1_ImEdge.eps}{Result edge detection}{0.45}
\fig{w_Spil1TransCrop1_ImHoughResult.eps}{Houghlines with endpoints}{0.45} 
\subsubsection{Efficient Projecting} 
We are interested in the frontal view of the building and it would be straight
forward to project the original image, however this is computational
expensive. To keep the computational cost to a minimum we project only the
Houghlines. The edge detection and Houghline extraction is done on the original
unprojected image. We only project the Houghline segment
endpoints. If $h$ is the number of Houghlines, the number of projections is $2h$
When we project the original image this is $w$ x $h$ where w,h are the dimensions of
the image. To give an indication, for the \emph{Spil} dataset %TODO
this means 600 projections in stead of 1572864.\\
\emph{As the backprojection isn't done at the time of writing we 
we also present the rectified image}
% todo backprojection


\subsection{Edge detection and Houghline extraction}
% this is described in a early chapter.
Edge detection and Houghline extraction is done as is described in chapter 
\emph{(not included chapter)} % todo
The results can be seen in Figure \ref{fig:w_Spil1TransCrop1_ImEdge.eps} and
\ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}.


\subsection{Method I: Connected corner approach} 
%Because the viewing point isn't assumed to be frontal, we we set the 
% motivation

\subsubsection{Situation and assumptions}
We introduce the concept \emph{connected corner}, this is a corner that is 
connected to a horizontal and vertical line.  
In this method we search for connected corners based on edge information.
The connected corners give a good indication of the position of the windows, as 
a window consists of a complex structure involving a lot of connected horizontal
and vertical lines. 

In this approach the windows could be arbitrarily located and they don't need
to be aligned to each other neither to the X and Y axis of the image.
%voting
%the connected lines give a direction

\subsubsection{Method}
From the edge image we extract two groups of Houghlines, horizontal and
vertical.  We set the $\theta$ bin ranges in the Hough transform that control the
allowed angles of the Houghlines to extract the two groups. The horizontal group
has a range of [-30..0..30] degrees, where 0 presents a horizontal line. The vertical
group has a range of [80..90..100] degrees. This ranges seem
to work good on an empirical basis for all datasets.

Next we pair up horizontal and vertical lines to discard found edges that are
not origined from a window. These paired up lines form a connected corner.
Often a connected corner is not fully connected or over connected.
We consider different types of connected corners, see Figure \ref{fig:cCornerTypes} 
\fig{cCornerTypes}{First row: different type of connected corner candidates. Second row: the
result the clean connected corner}{1}

To clean up the lines, the algorithm discards intersections between the
horizontal and vertical Houghlines that are apart. 
Two intersection point distances are measured: $d_h$ for the horizontal Houghline and $d_v$
for the vertical Houghline.  If the intersection falls on both associated Houghlines,
	the total distance $D=0$.  Otherwise the Euclidean distance is measured from the
	intersection to the closest endpoint. This is done for both Houghlines.  If
	the intersection falls outside both Houghlines (Figure
	\ref{fig:cCornerTypes}(IV)) ($d_h>0$ and $d_v>0$), the total
	distance is calculated by $D=(d_h + d_v)/2$.\\
	Next $D$ is compared to
	a \emph{maximum intersection distance} threshold $midT$.  And if $D<=midT$,
	the intersection is close enough to form a connected corner.\\

After two Houghlines are classified as a connected corner, they are stretched or
trimmed, depending on the situation. The results are shown in the second row in
Figure \ref{fig:cCornerTypes}.
In Figure \ref{fig:cCornerTypes}(I)  the horizontal line is stretched.  Figure
\ref{fig:cCornerTypes}(II) shows that the vertical line is trimmed.  In Figure
\ref{fig:cCornerTypes}(III) both lines are stretched.  At last Figure
\ref{fig:cCornerTypes}(IV) shows how both lines are trimmed.


Because we know the orientation of the connected corner we can estimate where
a window could be located.  We add a vote in the middle of the window. 
This is represented as a blue cross in Figure \ref{fig:cCornerTypes}.
This coordinate is retrieved using the X coordinate of the middle point of the horizontal line
and the Y coordinate of the middle point of the vertical line of the connected corner.  
Note that this is officially not allowed as we did not assume orthogonal windows
neither did we assume the windows to be aligned with the X and Y axis of the
image.  



\subsection{Method II: Histogram based approach} 
\subsubsection{Introduction}
In this method we assume that the viewing direction of the wall containing the 
windows is frontal. This assumption makes it possible to exploit the orthogonality and
alignment of the windows. We first determine the alignment of the windows and
then extract the windows. 

\paragraph{Occlusion}
\emph{Wat is een goed plek voor dit stuk tekst?}
\label{lab:occlusion}
If the image isn't the frontal view of the buildingwall we project the image 
see section ?%todo section rectification
This projection comes with some difficulties, occlusion.  In a few cases an
buiding wall extention (middle of figure \ref{fig:datasetIm.eps}) a drainpipe
or the building wall itself is occluding a part of the window.  The less frontal
the view, the more occlusion negatively effects the cleanness of the projection.
To handle this occlusion artefact and to increase accuracy we combine the
windows probabilities.  

\subsubsection{Situation and assumptions}
To be more precise in our assumptions, we assume the windows have orthogonal sides.  Furthermore we
assume that the windows are aligned, This means that a row of windows share the
same height and $y$ position. For a column of windows the width and $x$
position has to be equal.  Note that this doesn't mean that all windows have the
share the same size.

\subsubsection{Method}
The extraction of the windows is done in different steps. 
First the alignment of the windows is determined, this is based on collecting
the Houghlines' start and endpoints. Then we use this alignment to divide the
image in window or not window regions.  Finally these regions are classified
and combined which gives us the windows.


\fig{w_Spil1TransCrop1_ImHibaap.eps}{(smoothed) Histograms and window alignment lines}{0.45}
\paragraph{Extract Window alignment}
%explain pipe line
%(color transform)
%edge extraction
%Houghline extraction
We introduce the concept alignment line. We define this as a horizontal or
vertical line that aligns multiple windows. In Figure
\ref{fig:w_Spil1TransCrop1_ImHibaap.eps}
we show the alignment lines as two groups, horizontal (red) and
vertical (green) alignment lines.  The combination of both groups give a grid of
rectangles that we classify as window or non-window areas.\\

% MOTIVATION
How do we determine this alignment lines? We make use of the fact that among a
horizontal alignment line a lot of horizontal Houghlines start and end (see red
crosses in Figure \ref{fig:hibmaapHough.eps}. For the vertical alignment lines
the number of vertical Houghline start and ends is high (see green crosses in
Figure \ref{fig:hibmaapHough.eps}.\\

We begin by extracting the coordinates of the endpoints of the Hough transformed line
segments. We store them in two groups, horizontal and vertical% (crosses in Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}). 
We project the coordinates to the axis that is orthogonal to the group. This
means that for each horizontal Houghline two coordinates are projected to the X
axis and for each vertical Houghline two coordinates are projected to the Y
axis. We have now transformed the data in two groups of 1 dimensional
coordinates which represent the projected position of the Houghlines.\\

Next we calculate two histograms H(orizontal) and V(ertical), containing respectively
$w$ and $h$ bins where $w x h$ is the dimension of the image.  The histograms
are presented as small yellow bars in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.

The peaks are located at the positions where an increased number of Houghlines
start or end.  These are the interesting positions as they are highly correlated
to the alignment lines of the windows. 

It is easy to see that the number of peaks is fare more then the desired number of alignment lines.
A common solution would be to decrease the number of bins of the histograms. A
disadvantage of this method is that this comes with a price, it decreases the accuracy. Therefor
we keep the maximum resolution and, instead, smooth the values using a moving average filter.
The result, red lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}
, is a smooth function which contains the right number of peaks. As can be seen
in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}, the peaks
are located at the average positions of the window edges. Next step is to
calculate these positions. 

Before we find the peak positions we extract the peak \emph{areas} by thresholding the
function. To make the threshold invariant to the values, we set the threshold to 0.5 $\cdot$ max Peak. 
(This value works for most datasets but is made up and can be altered)
%The two thresholds are presented as black dotted lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.\\
Next we create a binary function P that returns 1 for positions that are contained in
a peak, i.e. are above the threshold, and 0 otherwise.
% TODO latex, afkijken locate my plate
%P(x)  { 1, H(x)>t
%	  { 0, H(x)<=t
We detect the peak areas by searching for the positions where P = 1
(where the function passes the threshold line). 
If we loop through the values of P we detect a peak-start on position $s$ if ${P(s-1),P(s)}={0,1}$
and a peak-end on $e$ if ${P(e-1),P(e)}={1,0}$. 
I.e. if P = 0011000011100, then two peaks are present. The first peak covers positions $(3,4)$, 
the second peak covers $(9,10,11)$.\\

Having classified the peak areas, the next step is to extract the peak positions. 
Each peak area has only one peak and, since we used an average smoothing filter, the shape of 
the peaks are often concave. Therefor we extract the peaks by locating the max of each peak area. 
These locations are used to draw the window alignment lines, they can be seen
as dotted red lines and dotted green lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}

The image is now divided in a grid of rectangular areas. The next challenge is to 
classify the areas as window and non-window areas: the window classification.

% TODO kort iets schrijven over
\paragraph{Window classification}
Instead of classifying each rectangle independedly we classify full rows and
columns as window or non-window areas.  This approach results in more accurate
classification as it uses a full row and column as evidence for a singular
window. The method exploits the fact that the windows are assumed to be
aligned.
A row that contains windows is remarkable by its high amount of vertical
Houghlines, Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}
(green). For the columns the number of horizontal Houghlines
 (red) is high at window areas.  We use this property to classify 
 the rows/columns. 

For each row the number of vertical Houghline pixels that lie in this row are summed up.
(Remark that with this method we take both the length of the Houghlines and amount of Houghlines 
implicitly into account.)

To prevent the effect that the size of the row influences the outcome, this total value
is normalized by the size of the row.
\[\forall Ri\in \{1..numRows\} : R_i = \frac{HoughlinePxCount}{R_i^{width} \cdot R_i^{height}}\]

Leaving us with $||R||$ (number of rows) scalar values that give a rank of a row begin a window area or not.
This is also done for each column (using the normalized horizontal amount of
Houghlines pixels) which leaves us with $C$.

%\fig{w_Spil1TransCrop1_ImClassRectBar.eps}{Unnormalized Houghline pixel count of
%the columns (C)}{0.6}
%\fig{w_Spil1TransCrop1_ImClassRectBarh.eps}{Unnormalized Houghline pixel count of
%the rows (R)}{0.6}
%In Figure \ref{fig:w_Spil1TransCrop1_ImClassRectBar.eps} and \ref{fig:w_Spil1TransCrop1_ImClassRectBar.eps} 
In Figure ? and ? % todo
we can take a look at the distribution of $R$ and $C$. We see two clusters appear: one with
high values (the rows/columns that contain windows) and one with low values (non window
rows/columns).  A straight forward approach would be to apply the classification using a
threshold for this value.  However, as the height of the values depend on
(unknown) properties like the number of windows, window types etc., the threshold
would be hard to determine and the method won't be robust. Instead we use the fact 
there should always be two clusters and use \emph{$k$-means}
clustering (with $k=2$) as the classification procedure.
%todo ref
This results in a set of Rows and Columns that are classified as window an
non-window areas.

The next step is to determine the actual windows $W$.
A rectangular area $w\in W$ that is crossed by $R_j$ and $C_k$ is classified as a
window iff \emph{$k$-means} classified both $R_j$ and $C_k$ as window areas, see
Figure %todo
% figure with only the windows as big red rectangular areas

As the figure gives a binary representation of the windows it is not possible
to see the detailed information about the values behind the classification.
Therefor we developed a probabilistic function. 
\[P(R_i) = \frac{R_i}{max(R)}\]
\[P(C_i) = \frac{C_i}{max(C)}\]
\[P(w) = \frac{P(R_i) + P(C_i)}{2}\]
As you can see $P$ is normalized, this is to ensure the value of the maximum
probability is exactly 1. The results can now be relatively interpreted, e.g. if the rectangle's $P=0.5$
then the system nows for 50 percent sure it is a window compared to its best window ($P=1$).

In Figure %todo
the probabilities for each rectangle are displayed.

To get insight about the probabilities that lie behind the individual rows and columns
we designed another representation in Figure \ref{fig:w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps}
The whiter the area the more probable a rectangle is classified as a window.

\fig{w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps}{TODO}{0.45}



\subsection{Results}
Below the results of the different methods on different datasets.
\fig{cCornerSpilTrans1.eps}{TODO}{0.45}
\fig{w_Spil1TransCrop1_ImClassRect.eps}{TODO}{0.45}
\emph{TODO include images other datasets}
\emph{todo compair methods and explain differences}


\subsection{Discussion}  % (What do my results mean to me and why)
% todo discuss resuts 
\paragraph{Method I: Connected corner approach} 
A disadvantage of this method is that it only finds plausible window centers
(and for each window center one of the corner positions).  It would be more
useful to find the complete window region. However, for the purpose of estimating
a people count for a real-time evacuation plan generator, the count of windows
would be enough.

The big advantage is that this method doesn't require the windows to be aligned.
Furthermore it's robust to a variation in window sizes. This makes this approach suitable
for a wide range of window scenes where no or few prior information about the
	windows is known.
\emph{TODO}


\paragraph{Method II: Histogram based approach} 
\emph{TODO}\\
pro's \\
This method is invariant to the height of the values
As the data is 1 dimensional this method doesn't use 
con's\\
The outcome is non-deterministic, as it depends on to the random initialization of the cluster centers.
\emph{TODO}\\
% TODO Ref biblograyhf



\subsection{Conclusion and Future work}
\subsubsection{Conclusion}
We showed that projecting the image to a frontal view is a good preprocessing
step of a robust window detector.
\emph{TODO}
%todo

\subsubsection{Future work}
\paragraph{Method I: Connected corner approach} 
It would be nice to have a clustering algorithm that groups connected corners to
a window. For this method it would be useful to assume the window size as this
correlates directly to the inter-cluster distance.\\

It would also be nice to incorporate not only the center of the connected corner
as a parameter of the cluster space but also the length and position of the of
the connected corners' horizontal and vertical line parts.  The inter cluster
distance and the number of grouped connected corner could form a good source for
the probability that a window is found.

% todo uitzoeken of ik dit niet al doe
We only developed L-shaped connected corners, it would be nice to connect more
parts of the window to form U shaped connected corners or even complete rectangles.\\
The later is difficult because the edges are often incomplete due to for example occlusion 
or the angle of viewing.


\paragraph{Method II: Histogram based approach} 
It would be nice to investigate the effect of the occlusion and to exploit the
robustness of the window detector under extreme viewing angles.
For example the viewing angle could be plotted against the percentage of
correct detected windows.
\emph{TODO}





% title: Graph Theory and Mean Shift Segmentation Based Classiﬁcation of Building Facades
% 
% Most of the previous algorithms are computationally expensive and not suitable for real time applications. Because
% of the requirements of real-time applications robust and fast
% classiﬁcation of facades is still an open research topic
% 
% 
% Image-based Procedural Modeling of Facades
% 
% 
% 
% refs:
% (canny edge detector:)
% 
% Canny, J., 1986. A computational approach to edge detection.
% IEEE Transactions on pattern analysis and machine intelli-
% gence pp. 679–698.
% 
