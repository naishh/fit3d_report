% set ignorecase



% todo's
% iets schrijven over datasets
% rectification is not always accurate, explain consequences



%TODO pipeline rectification process
%
% 		motion estimation
% 		scale estimation
% 			produces MAP
% 		plane fitter
% 			produces normal of wall
% 		edgeIm or houghlines reprojection
%


% list of figures
% -cCorner 
% -bar and barh
% -grayvalues voting
%	- values that fall out of cluster are displayed striped
% -grayvalues voting with clusterlines
% 	- or clustervalues in colored bar graph


\section{Window detection}


% TODO explain efficiency of Houghline coordinate transformation (instead of
% transforming the image
\label{chap:windowDetection}
\subsection{Introduction}
% todo
% papers die ik al heb geprint lezen
% digitale papers in map papers lezen
This chapter is about one of the tasks of semantic urban scene interpretation, Window detection. 
%explain what semantic 3d model is
Semantic interpretation of urban scenes is used in a wide range of applications.

\paragraph{3d City models} 
	Manual creation of 3d models is a time consuming and expensive procedure.
	Therefor semantic models are used for semi automatic 3d
	reconstruction/modelling [Procedural Modeling of Buildings].  
	The semantic understanding is also used in three-dimensional
	city models which are generated from aerial or satellite imagery.  The detected
	(doors and) windows are maped to the model to increase the level of detail. 
	Some other applications can automatically extract a CAD-like model of
	the building surface.\\

\paragraph{Historical buildings documentation and deformation analysis}
	In some field, Historical buildings are documentated. The complex
	structures that are contained in the facades are recorded and reconstructed.
	Window detection plays a central role in this. 
	Another field of research is the analysis of building deformation in areas containing old buildings.
	Window detection provides information about the region of interest that could
	be tracked over time for an accurate deformation analysis.
[A SEMI-AUTOMATIC IMAGE-BASED MEASUREMENT SYSTEM]


\paragraph{Interactive 3d models}
	There are some virtual training applications that are designed for
	emergency response who require interaction with a 3d model.  For the
	simulation to be realistic it is importand to have a model that is of high
	visual quality and has sufficient semantic detail.  Other applications that
	require semantic 3d models are virtual tourism, visual
	impact analysis, driving simulation and military simulation 
	systems.\\
%todo afbeelding uit pag 3 halen en hier plakken
http://www.rgi.nl/downloads/files/Shi Pu.pdf

\paragraph{Building recognition and urban planning}
	Building recognition is done using large image datasets where the
	buildings are mostly described by local information descriptors.  
	Some approaches try to describe the 3D building with laser range data. Some methods fuse the laser data with
	ground images. However those generated 3D models are a mesh structure which doesn't make the facade strucure explicit.
	For a more accurate disambiguation, other types of contextual information are
	desired.  The semantical interpretation of the facade can provide this need.
	In this context, window detection can be used as a strong discriminator.\\
	Building recognition is used in the field of urban planning where the semantic 3d
	models are used to provide importand references to the city scenes from the
	street level.

\paragraph{Augmented reality}
	Some mobile platforms apply augmented reality using facade and window
	detection to make a accurate overlay of the building. An example overlay is the
	same building but 200 years earlier.  Semantical information is used to not
	only indentify a respective building, but also find his exact location in the
	image.  The accuracy and realistic level of the 3d model are vital for a
	successfull simulation.  And because the applications are mobile, very fast
	building understanding algorithms are required.  Window
	detection plays an importand role in these processes.\\


As we explained window detection plays an importand role in the interpretation of 
urban scenes.  In this section we present two developed methods for robust
window detection.


We start with discussing related work and putting our work in context.  Then we
begin with a window detection approach that is invariant to viewing direction.
After this we present our second method that assumes orthogonal and aligned
windows.  Finally we show and discuss results. 


\subsubsection{Related work}
A large amount of research is done on semantical interpretation of urban scenes. First we
discuss related work that has a big overlap with our research in detail.
After this, we briefly discuss research on window detection that uses very
different approaches.


\paragraph{Simmilar approaches}
[REFINING BUILDING FACADE MODELS WITH IMAGES]
%www.isprs.org/proceedings/XXXVIII/3-W4/pub/CMRT09_217.pdf]
Pu and Vosselman %todo ref
use laser images together with Hough line extraction to reconstruct facade details.  
They solve inconsistency between laser and image data and improve the alignment of a 3d model with a matching algorithm.
In one of the matching strategies they compare the edges of a 3d model to Hough lines of ground images.
They matched the lines by comparing the angle, location and length difference of the model edges with the extracted Houghlines.
These criterea is also used in our approaches.\\
They also detected windows and used this to provided a significant better alignment of the 3d model.
The windows where extracted from the holes from laserpoints of a wall, these results where far from accurate.\\
The work of Vosselman provides a usefull practical application of window
detection and it amplifies the need for a robust window detection technique that
is independed of laserdata.\\


[Windows Detection Using K-means in CIE-Lab Color Space]
In %todo ref
Recky and Leberl made a window detector that is build on the primary work of Lee and Nevatia (which is discussed next)
%todo ref

%todo
In order to be able to assume aligned windows they rectify the facade.  After this they apply a threshold on an orthogonal projection of the extracted edges. 
For example they use a vertical edge projection to establish the horizontal division of the windows.
This is very simmilar to our approach, although we only project line segment endpoints.\\
The next step, labeling the areas containing windows, is however very different as they use color to disambiguate the windows.
To be more precise they convert the image to CIE-Lab color space and use k-means to classify the windows.
Although this method is robust, both color transformation and k-means clustering are very computational expensive.
In our method we use the same source, edge information, for the window alignment and for the window labeling.
As we don't require color transformation and only apply math on line segment
endpoints, our algorithm performs in realtime.

[Lee, Nevatia, Extraction and Integration of Window in a 3D Building Model
from Ground View images]
%http://nguyendangbinh.org/Proceedings/CVPR/2004/DATA/PS4_16.PDF
As in the work of Reky and Leberl
%todo ref
Lee and Nevatia 
%todo ref
perform orthogonal edge projection to find the window alignment.
As different shape of windows can exist in the same column, they use the window alignment as a hypothesis. 
Then, using this hypothesis, they perform a refinement for each window independedly.
Although this comes with accurate results, the iterative refinement is a computational expensive procedure.
Therefor we do not consider the refinement process in our research.

%%NEW PAPERS 
%[A model-based method for buildig reconstruction]
	%brute force matching of window primiteves
	%make 3d models and recover the geometry of a building


\paragraph{Other approaches}





[Image-based procedural modeling of Facades]
Muller et all,
%TODO REF
use an interesting way to detect the symmetry in the building. The symmetry is detected in two directions, vertical (floors) and horizontal (window rows).
The use shape grammars to devide the building wall in tiles, windows, doors etc.
The results are used to derive a 3d model of high 3d visual quality.
%todo insert an image of this paper of the nice geometry (ingevallen raam)

[Detection of windows and doors from thermal images by grouping geometrical features]
[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5764737]
%todo insert image
Using a thermal camera, Sirmacek,  
%todo ref
detects heat leakage on building walls as a indicator for doors or windows
L-shaped features are applied using a set of \emph{steerable filters}.
The windows are grouped using \emph{perceptual organisation rules}.

[Window detection in facades]
Ali et all %todo ref
describes the windows with Haar like features which are fet into a (Ada boost) cascaded decision tree.







% handige note
% \paragraph{Facade classification}
% That shows the importance of the facade classification
% study in three-dimensional city modeling. Burochin et al. [3]
% proposed a segmentation method to detect repetitive structures
% like windows in close-range optical images. For segmentation
% they defined a model by considering shape and reflectance
% Fig. 1. Overview of the proposed approach (Facade1 test image, segmentation
% result with facade graph, and classification result of the algorithm
% respectively.)
% of a window, then they applied matching process to find
% correspondence between model and image. In [2], Ali et
% al. gave summary of the researches on window detection.
% They also proposed a window detection system based on
% cascade classifiers. In a following study, Ali et al. proposed
% a system to detect windows in laser scanner data. The laser
% Therefore, they use these variations to detect windows [1]. Lee
% and Nevatia [8] proposed a robust system to detect windows in
% optical images. They extracted window boundaries searching
% for structures that satisfy regularity and symmetry rules. In
% addition to that, they extract three-dimensional models of
% windows by searching for image features. Teboul et al. [13]
% used shape grammars towards fixed tree representations which
% are able to capture a wide variety of building topologies
% for detailed facade segmentation. They obtained very high
% performance even for buildings which are partially occluded
% or which appears under different illumination conditions.





% todo tjoint article quoten


% todo find good location in thesis for this
\fig{w_spil6ImOri.eps}{Original image}{0.6}
\fig{w_Spil1TransCrop1_ImOri.eps}{Rectified image}{0.45}
\fig{w_Spil1TransCrop1_ImEdge.eps}{Result edge detection}{0.45}
\fig{w_Spil1TransCrop1_ImHoughResult.eps}{Houghlines with endpoints}{0.45} 




\subsection{Method I: Connected corner approach} 
%Because the viewing point isn't assumed to be frontal, we we set the 
% motivation

\subsubsection{Situation and assumptions}
We introduce the concept \emph{connected corner}, this is a corner that is 
connected to a horizontal and vertical line.  
In this method we search for connected corners based on edge information.
The connected corners give a good indication of the position of the windows, as 
a window consists of a complex structure involving a lot of connected horizontal
and vertical lines. 

In this approach the windows could be arbitrarily located and they don't need
to be aligned to each other neither to the X and Y axis of the image.
%voting
%the connected lines give a direction

\subsubsection{Method}
\paragraph{Edge detection and Houghline extraction}
Edge detection is done as is described in chapter 
\emph{(not included chapter)} % todo
From the edge image we extract two different groups of Houghlines, horizontal and % todo why?
vertical.  We set the $\theta$ bin ranges in the Hough transform that control the
allowed angles of the Houghlines to extract the two groups. The horizontal group
has a range of [-30..0..30] degrees, where 0 presents a horizontal line. The vertical
group has a range of [80..90..100] degrees. This ranges seem
to work good on an empirical basis for all datasets.
The results can be seen in Figure \ref{fig:w_Spil1TransCrop1_ImEdge.eps} and
 \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}.

Next we pair up horizontal and vertical lines to discard found edges that are
not origined from a window. These paired up lines form a connected corner.
Often a connected corner is not fully connected or over connected.
We consider different types of connected corners, see Figure \ref{fig:cCornerTypes}.
\fig{cCornerTypes}{First row: different type of connected corner candidates. Second row: the
result the clean connected corner}{0.5}

To clean up the lines, the algorithm discards intersections between the
horizontal and vertical Houghlines that are apart. 
Two intersection point distances are measured: $d_h$ for the horizontal Houghline and $d_v$
for the vertical Houghline.  If the intersection falls on both associated Houghlines,
	the total distance $D=0$.  Otherwise the Euclidean distance is measured from the
	intersection to the closest endpoint. This is done for both Houghlines.  If
	the intersection falls outside both Houghlines (Figure
	\ref{fig:cCornerTypes}(IV)) ($d_h>0$ and $d_v>0$), the total
	distance is calculated by $D=(d_h + d_v)/2$.\\
	Next $D$ is compared to
	a \emph{maximum intersection distance} threshold $midT$.  And if $D<=midT$,
	the intersection is close enough to form a connected corner.\\

After two Houghlines are classified as a connected corner, they are stretched or
trimmed, depending on the situation. The results are shown in the second row in
Figure \ref{fig:cCornerTypes}.
In Figure \ref{fig:cCornerTypes}(I)  the horizontal line is stretched.  Figure
\ref{fig:cCornerTypes}(II) shows that the vertical line is trimmed.  In Figure
\ref{fig:cCornerTypes}(III) both lines are stretched.  At last, Figure
\ref{fig:cCornerTypes}(IV) shows how both lines are trimmed.


Because we know the orientation of the connected corner we can estimate where
a window could be located.  We add a vote in the middle of the window. 
This is represented as a blue cross in Figure \ref{fig:cCornerTypes}.
This coordinate is retrieved using the X coordinate of the middle point of the horizontal line
and the Y coordinate of the middle point of the vertical line of the connected corner.  
Note that this is officially not allowed as we did not assume orthogonal windows
neither did we assume the windows to be aligned with the X and Y axis of the
image. However on a empirical basis most crosses lie within the original window
which is enough. Results are found in the Result section.


\subsection{Method II: Histogram based approach} 
\subsubsection{Introduction}
From the previous chapter we know that from a serie of images, a 3D model of a
building can be extracted. Furthermore we saw that using this 3D model the
scene could be converted to another viewing point. 

%todo instead of frontal: rectified
For accurate and robust window detection we projected the scene to a frontal
view of a building, where a building wall appears orthogonal. This frontal
view enables us to assume orthogonality and alignment of the windows. We
exploit this properties to build a robust window detector.
First we determine the the alignment of the windows and
then we label the areas that contain the windows. 


\subsubsection{Situation and assumptions}
To be more precise in our assumptions, we assume the windows have orthogonal sides.  Furthermore we
assume that the windows are aligned. This means that a row of windows share the
same height and $y$ position. For a column of windows the width and $x$
position has to be equal.  Note that this doesn't mean that all windows have the
share the same size.

\subsubsection{Method}
The extraction of the windows is done in different steps. 
First the alignment of the windows is determined, this is based on collecting
the Houghlines' start and endpoints. Then we use this alignment to divide the
image in window or not window regions.  Finally these regions are classified
and combined which gives us the windows.


\fig{w_Spil1TransCrop1_ImHibaap.eps}{(smoothed) Histograms and window alignment lines}{0.45}
\paragraph{Extract Window alignment}
%explain pipe line
%(color transform)
%edge extraction
%Houghline extraction
We introduce the concept alignment line. We define this as a horizontal or
vertical line that aligns multiple windows. In Figure
\ref{fig:w_Spil1TransCrop1_ImHibaap.eps}
we show the alignment lines as two groups, horizontal (red) and
vertical (green) alignment lines.  The combination of both groups give a grid of
rectangles that we classify as window or non-window areas.\\

% MOTIVATION
How do we determine this alignment lines? We make use of the fact that among a
horizontal alignment line a lot of horizontal Houghlines start and end (see red
crosses in Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}. For the vertical alignment lines
the number of vertical Houghline start and ends is high (see green crosses in
Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}.\\

We begin by extracting the coordinates of the endpoints of the Hough transformed line
segments. We store them in two groups, horizontal and vertical.% (crosses in Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}). 
We discard the dimension that is least informative by project the coordinates to
the axis that is orthogonal to its group. 
This means that for each horizontal Houghline two coordinates are projected to the X
axis and for each vertical Houghline two coordinates are projected to the Y
axis. We have now transformed the data in two groups of 1 dimensional
coordinates which represent the projected position of the Houghlines.\\

Next we calculate two histograms H(orizontal) and V(ertical), containing respectively
$w$ and $h$ bins where $w x h$ is the dimension of the image.  The histograms
are presented as small yellow bars in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.

The peaks are located at the positions where an increased number of Houghlines
start or end.  These are the interesting positions as they are highly correlated
to the alignment lines of the windows. 

It is easy to see that the number of peaks is fare more then the desired number of alignment lines.
A common solution would be to decrease the number of bins of the histograms. A
disadvantage of this method is that this comes with a price, it decreases the accuracy. Therefor
we keep the maximum resolution and, instead, smooth the values using a moving average filter.
The result, red lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}
, is a smooth function which contains the right number of peaks. The peaks
are located at the average positions of the window edges. Next step is to
calculate the peak areas and after this the peak positions. 

Before we find the peak positions we extract the peak \emph{areas} by thresholding the
function. To make the threshold invariant to the values, we set the threshold to 0.5 $\cdot$ max Peak. 
(This value works for most datasets but is made up and can be altered).
%The two thresholds are presented as black dotted lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.\\
Next we create a binary function P that returns 1 for positions that are contained in
a peak, i.e. are above the threshold, and 0 otherwise.
% TODO latex, afkijken locate my plate
%P(x)  { 1, H(x)>t
%	  { 0, H(x)<=t
We detect the peak areas by searching for the positions where P = 1
(where the function passes the threshold line). 
If we loop through the values of P we detect a peak-start on position $s$ if ${P(s-1),P(s)}={0,1}$
and a peak-end on $e$ if ${P(e-1),P(e)}={1,0}$. 
I.e. if P = 0011000011100, then two peaks are present. The first peak covers positions $(3,4)$, 
the second peak covers $(9,10,11)$.\\

Having classified the peak areas, the next step is to extract the peak positions. 
Each peak area has only one peak and, since we used an average smoothing filter, the shape of 
the peaks are often concave. Therefor we extract the peaks by locating the max of each peak area. 
These locations are used to draw the window alignment lines, they can be seen
as dotted red lines and dotted green lines in Figure \ref{fig:w_Spil1TransCrop1_ImHibaap.eps}.

The image is now divided in a grid of rectangular areas. The next challenge is to 
classify the areas as window and non-window areas: the window classification.

\paragraph{Window classification}
Instead of classifying each rectangle independently we classify full rows and
columns as window or non-window areas.  This approach results in more accurate
classification as it uses a full row and column as evidence for a singular
window. 

The method exploits the fact that the windows are assumed to be
aligned.
A row that contains windows is remarkable by its high amount of vertical
Houghlines, Figure \ref{fig:w_Spil1TransCrop1_ImHoughResult.eps}
(green). For the columns the number of horizontal Houghlines
 (red) is high at window areas.  We use this property to classify 
 the rows/columns. 

For each row the number of vertical Houghline pixels that lie in this row are summed up.
(Remark that with this method we take both the length of the Houghlines and amount of Houghlines 
implicitly into account.)

To prevent the effect that the size of the row influences the outcome, this total value
is normalized by the size of the row.
\[\forall Ri\in \{1..numRows\} : R_i = \frac{HoughlinePxCount}{R_i^{width} \cdot R_i^{height}}\]

Leaving us with $||R||$ (number of rows) scalar values that give a rank of a row begin a window area or not.
This is also done for each column (using the normalized horizontal amount of
Houghlines pixels) which leaves us with $C$.

\fig{w_Spil1TransCrop1_ImClassRectBarh.eps}{Normalized vertical Houghline pixel count of
the rows (R)}{0.6}
If we examine the distribution of $R$ and $C$, we see two clusters appear: one with
high values (the rows/columns that contain windows) and one with low values (non window
rows/columns). For a concrete example we displayed the values of $R$ in Figure \ref{fig:w_Spil1TransCrop1_ImClassRectBarh.eps}.
Its easy to see that the high values, row 4,5,7,8,10 and 11, correspond to the
six window row areas in Figure \ref{fig:w_Spil1TransCrop1_ImClassRect.eps}.

How do we determine which value is classified as high?  A straight forward
approach would be to apply a threshold, for example 0.5 would work fine.
However, as the variation of the values depend on (unknown) properties like the
number of windows, window types etc., the threshold maybe classify insufficient
in another scene.  Hence working with the threshold wouldn't be robust. 

Instead we use the fact that a row is either filled with windows or not, hence
there should always be two clusters.  We use \emph{$k$-means} clustering (with
$k=2$) as the classification procedure.
%todo ref
This results in a set of Rows and Columns that are classified as window an
non-window areas.

The next step is to determine the actual windows $W$.
A rectangular area $w\in W$ that is crossed by $R_j$ and $C_k$ is classified as a
window iff \emph{$k$-means} classified both $R_j$ and $C_k$ as window areas. These are displayed in 
 Figure \ref{fig:w_Spil1TransCrop1_ImClassRect.eps} as green rectangles.

The last step is to group a set of windows that belong to each other. This is done by 
grouping adjacent positively classified rectangles. These are displayed as red
rectangles in Figure \ref{fig:w_Spil1TransCrop1_ImClassRect.eps}.

As the figure gives a binary representation of the windows it is not possible
to see the probabilities behind the classification.
Therefor we developed a probabilistic function. 
\[P(R_i) = \frac{R_i}{max(R)}\]
\[P(C_i) = \frac{C_i}{max(C)}\]
\[P(w) = \frac{P(R_i) + P(C_i)}{2}\]
As you can see $P$ is normalized, this is to ensure the value of the maximum
probability is exactly 1. The results can now be relatively interpreted, e.g. if the rectangle's $P=0.5$
then the system nows for 50 percent sure it is a window, compared to its best window ($P=1$). 
And, as the normalization implies this, there are always one or more window with $P=1$. 

To get insight about the probabilities that lie behind the individual rows and columns
we designed another representation in Figure \ref{fig:w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps}
The whiter the area the more probable a rectangle is classified as a window.
\fig{w_Spil1TransCrop1_ImClassRectGrayscaleProb.eps}{Window classification probabilities, white means high.}{0.45}



\subsection{Results}
We tested both methods on different datasets.
\fig{w_spil6cCornerImEdge.eps}{Edge detection}{0.6}
\fig{w_spil6cCornerImHoughResult.eps}{Result of $\theta$ constrained Hough transform}{0.6}% todo theta constrain
\fig{w_spil6cCornercCorner.eps}{Found connected corners}{0.6}
\fig{w_spil6cCornerWindows.eps}{Connected corner as windows}{0.6}
\fig{cCornerSpilTrans1.eps}{Found connected corners on the rectified image}{0.45}
\fig{w_Spil1TransCrop1_ImClassRect.eps}{Classified rectangles}{0.45}

\fig{w_SpilFrontal6345_crop1_ImOri.eps}{Original Image}{0.6}
\fig{w_SpilFrontal6345_crop1_ImHibaap.eps}{Window alignment lines and histograms}{0.6}
\fig{w_SpilFrontal6345_crop1_ImClassRect.eps}{Classified rectangles}{0.6}



\emph{TODO include images other datasets} \\
\emph{todo compair methods and explain differences}


\subsection{Discussion}  % (What do my results mean to me and why)
\emph{ todo discuss results }
\paragraph{Method I: Connected corner approach} 
A disadvantage of this method is that it only finds plausible window centers
(and for each window center one of the corner positions).  It would be more
useful to find the complete window region. However, for the purpose of estimating
a people count for a real-time evacuation plan generator, the count of windows
would be enough.
% todo ref

% todo works on multiple window types

The big advantage is that this method doesn't require the windows to be aligned.
Furthermore it's robust to a variation in window sizes. This makes this approach suitable
for a wide range of window scenes where no or few prior information about the
	windows is known.
\emph{TODO}


\paragraph{Method II: Histogram based approach} 
% todo also works on multiple window types
One drawback is that the outcome is non-deterministic, as it depends on to the
random initialization of the cluster centers.

\emph{TODO}\\
% TODO Ref biblograyhf

\paragraph{Occlusion}
\label{lab:occlusion}
If the image isn't the frontal view of the buildingwall we project the image 
see section ?%todo section rectification
This projection comes with some difficulties, occlusion.  In a few cases an
buiding wall extention (middle of figure \ref{fig:w_spil6ImOri.eps}) a drainpipe
or the building wall itself is occluding a part of the window.  The less frontal
the view, the more occlusion negatively effects the cleanness of the projection.
However, this occlusion artefact is in most cases no problem as the system
combines the windows probabilities.  

\subsection{Conclusion and Future work}
\subsubsection{Conclusion}
We showed that projecting the image to a frontal view is a good preprocessing
step of a robust window detector.
\emph{TODO}
%todo

\subsubsection{Future work}
\paragraph{Method I: Connected corner approach} 
It would be nice to have a clustering algorithm that groups connected corners to
a window. For this method it would be useful to assume the window size as this
correlates directly to the inter-cluster distance.\\

It would also be nice to incorporate not only the center of the connected corner
as a parameter of the cluster space but also the length and position of the of
the connected corners' horizontal and vertical line parts.  The inter cluster
distance and the number of grouped connected corner could form a good source for
the probability that a window is found.

% todo uitzoeken of ik dit niet al doe
We only developed L-shaped connected corners, it would be nice to connect more
parts of the window to form U shaped connected corners or even complete rectangles.\\
The later is difficult because the edges are often incomplete due to for example occlusion 
or the angle of viewing.


\paragraph{Method II: Histogram based approach} 
It would be nice to investigate the effect of the occlusion and to exploit the
robustness of the window detector under extreme viewing angles.
For example the viewing angle could be plotted against the percentage of
correct detected windows.
\emph{TODO}






% todo put in appendix or in a chapter devoted to projecting/rectifying
\subsection{Efficient Projecting} 
As we are interested in the frontal view of the building, it would be straight
forward to project the original image. However this is computational
very expensive as each pixel needs to be projected. To keep the computational cost to a minimum we project only the
Houghlines. The edge detection and Houghline extraction is done on the original
unprojected image. We only project the Houghline segment
endpoints. If $h$ is the number of Houghlines, the number of projections is $2h$
When we project the original image this is $w$ x $h$ where w,h are the dimensions of
the image. To give an indication, for the \emph{Spil} dataset %TODO
this means 600 projections in stead of 1572864.\\
% todo good location
However for the purpose of display we also presented the rectified images.
% todo backprojection




% title: Graph Theory and Mean Shift Segmentation Based Classiﬁcation of Building Facades
% 
% Most of the previous algorithms are computationally expensive and not suitable for real time applications. Because
% of the requirements of real-time applications robust and fast
% classiﬁcation of facades is still an open research topic
% 
% 
% Image-based Procedural Modeling of Facades
% 
% 
% 
% refs:
% (canny edge detector:)
% 
% Canny, J., 1986. A computational approach to edge detection.
% IEEE Transactions on pattern analysis and machine intelli-
% gence pp. 679–698.
% 
