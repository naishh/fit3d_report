% subjects to write 
% houghlines
% axis angle theory, to projection
% how I calc the wall normal
%

\documentclass[10pt]{article}
\usepackage{graphicx, subfigure}

\usepackage{amsmath} % for the argmin
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{argmax}}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg\ min}}}

%\newcommand{\min}{\textrm{min}}
%functions
\setlength{\parindent}{0in}

\include{commands}



\title{\sc Improve 3D models from 2D images}

\author{T. Kostelijk\\mailtjerk@gmail.com}

\begin{document}
\maketitle

\begin{abstract}
Here comes the abstract
\end{abstract}

% TODO
% pictures
% explain motivation behind decisions
% explain what is new or what is unique
% TODO background section?
% image captions
% read whole report

% TODO: documentclass anders zodat alles breder is en plaatjes ook breder kunnen
% en niet bij 7cm al afbreken

% TODO
% name assumption; the photos taken are straight, the walls are vertical, ivm 
% (columnwise algo part of the skyline detector )


\section{Introduction}
 \subsection{Related work}
 \subsection{FIT3D toolbox}
 \subsection{Organization of this thesis}

\section{Projective Geometry}
	% get inspiration and/or copy from Isaacs paper 
	% introduce K
	% introduce 2d to 3d projection

\section{Skyline detection}
 \subsection{Introduction}

The sky and the earth are separated by a skyline in images. The detection of this skyline
has proven to be a very succesful computer vision application in a wide range of
domains. In this domain it is used to provide a countour of a building. This
contour is in a next step used to refine a 3D model of this building.\\
The organisation of this chapter is as follows.  First related work on skyline
detection is discussed, then a new algorithm of the skyline algorithm is
described and finally some results are presented.\\
% It is interesting to denote that the skyline detector a stand alone method and
% can be optimized individually without any knowledge of the other parts of the
% project.
 \subsection{Related work}
A lot of related work on skyline detection is done and it is used in a wide
range of domains. $[8]$ yields a good introduction of different skyline
detection techniques, these are listed below.

\subsubsection{Cloud detection for Mars Exploration Rovers (MER)}
Mars Exploration Rovers (MER) are used to detect clouds and dust devils on Mars.
In [1] their approach is to first identify the sky (equivalentl, the skyline)
and then determine if there are clouds in the region segmented as sky.

\subsubsection{Horizon detection for Unmaned Air Vehicles (UAV)}
In this domain, the horizon detector for UAVs can take advantage of the high
altitude of the vehicle and therefor the horizon can be approximated to be a
straigt line.  This turns the detection problem into a line-fitting problem.
Ofcourse this work is not applicable for detecting a building countour as the
straight line assumption doesn't work. But it needs to be mentioned that from this
idea some inspiration on line fitting is done because the building countour has
straight line segments.

\subsubsection{Planetary Rover localisation}
In [9] they use the skyline detection in planetary rovers, their approach is to
combine the detected skyline with a given map of the landscape (hills, roads) to
detect its current location. 
The advantage of their technique is the simplicity and effectiveness of the
algorithm which makes it suitable for this project.  A big drawback is that it
is geared toward speed over extremely high accuracy because it is 
interactive system where an operator refines the skyline.

As mentioned in the introduction, in this project we use the skyline to extract
the building contour to eventually update a 3D model which is a brand new
purpose of skyline detection.  There is no user interaction present, and the
accuracy is a matter of high importance.  This makes it different from existing
skyline techniques and caution should be taken by using existing algorithms.
From the related work the Planetary Rover localisation [9] seemed to fit most on
this project.  Therefor method [9] is used, but as a basis, and a custom
algorithm with higher accuracy is developed. This is explained in the next
section.

%talk about assumptions
%there are always this assumptions??
%but what can we (not) assume?
%Furthermore we can assume that some straight lines 

 \subsection{The Algorithm}
 \subsubsection{The original algorithm}
 %TODO lezen: Map-based localization using the panoramic horizon
The skyline detection algorithm as described in $[9]$ works as follows:
The frames are first preprocessed by converting them to Gaussian smoothed images.
The skyline of a frame is then detected by analysing the the image columns
seperately.
The smoothed intensity gradient is calculated from top to bottom. This is done
by taking the derivative of the gaussian smoothed image.
The system takes the first pixel with gradient higher then a threshold to be
classified as a skyline element.  This is done for every column in the image.
The result is a set of coordinates of length $W$,
where $W$ is the width of the image, that represent the skyline.

Taking the smoothed intensity gradient is the most basic method of edge
detection and has the disadvantage that is is not robust to more vague
edges. This is not surprising as it its purpose was a interactive system where the
user refines the result. It is clear that an optimization is needed.

  \subsubsection{The optimization}
 % page 15/16
 % idea for future work:
 % adaptive thresholding wrt average intensity!
The column based approach seem te be very useful and is therefor unchanged. 
The effectiveness of the algorithm is totally depended of the method of edge
detection and the preprocessing of the images. 
The original algorithm uses the smoothed intensity gradient as a way of
detecting edges. This is a very basic method and more sophisticated edge
detection algorithms are present.\\
To select a proper edge detector, a practical study is done on the different
Matlab build in edge detection techniques. The output of the different edge
detection techniques was studied and the Sobel edge detector came with the most
promising results. The Sobel edge detector outputs a binary image, therefor the column inlier
threshold method is replaced by finding the first white pixel. This is as the
original algorithm done from top to bottom for every column in the image.
\\ 
To make the algorithm more precise, two preprocessing
steps are introduced. First the contrast of the image is increased, this makes
sharp edges stand out more.  Secondly the image undertakes a Gaussian blur,
this removes a large part of the noise.

The system now has several parameters which has to be set manually by the user:
\begin{itemize}
	\item contrast,
	% officialy i don't do this contrast thing
	\item intensity (window size) of Gaussian blur,
	\item Sobel edge detector threshold,
\end{itemize}
\textit{Should I write down what parameter values I used or is this of too much
detail}

If the user introduces a new dataset these parameters needs to be changed
as the image quality and lightning condition are probably different.
%(Automatic parameter estimation based on the image would be interesting future
%work but lies without the scope of this research.)

 \subsection{Results}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The system assumes that the first sharp edge (seen from top to bottom) is
always the skyline/building edge. This gives raise to some outliers, for 
for example a streetlight or a tree. These outliers are removed as described in
the next section.  

The Skyline detector without outlier removal has an accuracy of 80 \% 
Some results on the Floriande dataset $[1]$ can be seen in Figure \ref{fig:outputskyline}.

\figs{outputskyline}{outputSkylineIm3-2.eps}{outputSkylineIm3-3.eps}{The output
of the edge detector}{The output of the skyline detector. The skyline elements are marked red}{1}


\section{Skyline projection}
 \subsection{Introduction}
The final product of this research is an accurate 3D model of an urban
landscape. This is accomplished in several steps, so far the first step: skyline detection is
explained. The next step is to create a basic 3D model of the urban landscape.
Then the retrieved skyline of the previous section is used to update the basic
3D model of the building. This is illustrated in Figure \ref{fig:uml1.eps}

\fig{uml1.eps}{Situation scheme}{0.5}



   \subsection{3D modelling}
First a rough 3D model of the urban landscape has to be generated. This is done
by taking a top-view Google maps image of the scene and extract the contour of
the building, this is done manually. 
The height of the building is estimated, also manually, and together with the
contour a rough 3D model is generated, see Figure \ref{fig:building.eps}. 
\fig{building.eps}{A rough 3D model of the building}{1}
%TODO: disadvantage: height of building everywhere the same


% TODO how do I combine the results of the different angles
\subsection{Project to 3D space}
When the skylines of the 2D images are projected to 3D it can give 
accurate information about the contour of the building(s). This is used to
refine the basic 3D model.  Next is explained how the skylines are projected to
3D.\\

A skyline consist of different skyline pixels. Every 2D skyline pixel presents a 3D point in space. No
information is known about the distance from the 3D point to the camera that
took the picture. What is known is de 2D location of the pixel which reduces the possible points in 3D
space to an infinite line.  This line is known and spanned by two 
coordinates:\\ 
\begin{itemize}
	\item The camera center %(camera centers are annotated for every image)
	\item $K'p$, where $K$ is the Calibration matrix of the camera and $p$ is the homogeneous pixel coordinate.
	\textit{Why K'p, I don't remember the theory behind it and can't find it in your paper. Would you explain this Isaac?}
\end{itemize}


%---
% TODO write in algorithm style?
For every skyline pixel a line spanned by the above two coordinates is derived.


\subsection{Intersect with building}
The lines derived as described in the previous section are not enough to refine
the 3D model because it is still unknown which skyline part belongs to which
part of the 3D model.

Therefor these lines of possibel pixel locations need to
be reduced to the actual 3D locations of the pixels.  This is done by intersecting
them with the walls of the rough 3D model and is done as follows.

The building is first divided into different walls.  Every wall of the building spans a plane. 
Intersections are then calculated between the lines and the planes of the building walls.\\
\textit{Isaac, should I put a intersection formula down here or is this trivial?}\\

A skyline pixel intersects with every wall as both the lines and planes are
infinite and have a very low change of being parallel.
The next challenge is to reduce the number of intersection for every skylinepixel
to one. In other words, to determine the wall that is responsible for that pixel. 
This is later on used to update the 3D model at the right place.

The details of this process is explained next:
%has the largest probability of being 

\subsection{Find most likely wall}
\subsubsection{When is a wall responsible?}
Lets define the intersection between the projected skylinepixel line and the
plane of the wall as the intersection point called $isp$. And the wall sides as ${w1, w2,
w3, w4 \in W}$. And $d$ as a distance measure which is explained later.\\ 
If a certain wall is responsible for a pixel, the intersection (i.e. projected
pixel) must lie either\\

\textbf{(1)} Somewhere on the wall 
\\
or
\\
\textbf{(2)} On a small distance $d$ from that wall\\
\textbf{(1)} is calculated by testing if the pixel lies inside the polygonal
representation of the wall. This is done using the Matlabs in-polygon
algorithm. If the in-polygon test succeeds $d$ is considered to be 0.\\

\textbf{(2)} Note that in this case the pixel is treated as an inlier because the 3D
model is sparse and the height of the building is estimated. It is 
calculated as follows: \\
First the distances from $isp$ to all four wall sides are calculated.
For every wall the minimum distance is stored.\\
$$\min_{w\in W} d(isp, w)$$\\
This is done for every wall. The wall with the smallest distance is the one that
most likely presents the pixel:\\
%--------------------------------------------------------------------------------------------------------------
%$\textrm{argmin}_{W \in Walls} ( min_{w\in W} d(isp, w) )$\\
%$\argmin_{W \in Walls} ( min_{w\in W} d(isp, w) )$\\
$$\argmin{W \in Walls} ( min_{w\in W}\ d(isp, w) )$$\\

%threshold..

If there are two (or even more) walls that are classified equally well to
present the pixel (that is if they both succeed the in-polygon or have exactly the
same $d$ value) then the nearest wall is selected. The nearest wall is
calculated by taking the wall with the smallest Euclidean distance from the $isp$ to the
camera center.
In the next section the intersection point - wall distance, $d$, is explained.

\subsubsection{Calculate the intersection point - wall distance, $d$}
Every wall is rectangular and consist of four wallsides, a wallside spans an
infinite line. As can be seen in Figure TODO, a intersection point ($isp$) is projected orthogonally on these four lines.
The projected $isp$ is denoted as $isp_{proj}$.

\textit{Should I place a projection formula here?}

% TODO dia project the points on the axis
If $isp$ is close to a wallside then $e(isp, isp_{proj})$ (where $e$ is defined as
the Euclidean distance) is small.  Note that this doesn't mean that if
$e(isp,isp_{proj})$ is small $isp$ is always close to the wall. 
In Figure \ref{fig:d.eps} the distance between $p$ and $p_{proj}$ is very small
but $p$ lies far away from the wall.  This is ofcourse because $p$ is projected
to the infinite line spanned by the wallside $w$ and in this case it got
unfortunatelly projected far next to the wallside.

Because of this artefact, it is not robust to define $d$ as the the perpendicular
projection distance.  To make the distant measure more robust $d$ is calculated
differently if $isp_{proj}$ lies outside the wallside segment of the line. In
this case the Euclidean distance between $isp_{proj}$ and the closest
corner-point of the wallside is returned. In the other cases the perpendicular
projection distance is used. This is illustrated in Figure
\ref{fig:d.eps}.

\fig{d.eps}{The behavior of the distance measure $d$. For $p$ and $q$ the
euclidean distance to the closest corner point is taken. For $q$ the
perpendicular projection distance is taken}{0.2}

% Formally:
% Let $c1,c2,c3,c4 \in Cornerpoints$ be the corners of a wall.\\
% Let $between(a,b,c)$ be a function that returns true if a lies between b and c.\\
% if $between(isp_{proj}, c1, c2)$\\
% $d = e(isp, isp_{proj})$\\
% otherwise\\
% $d = min_{c \in Cornerpoints} e(isp, c)$\\
% \\
% \textit{TODO nice latex code with large \} sign}\\


\subsubsection{Determine whether $isp_{proj}$ lies on, to the left or to the
right of a wallsegment}
To determine whether the $isp_{proj}$ lies on, to the left or to the right of a
wallsegment a efficient algorithm is designed. As will be clear in a few moments
the projection calculation can be skipped, $isp$ is used instead together with a
computational cheap trick.\\

% TODO dia new image with red arrows?
Take a look at Figure \ref{fig:d.eps} again, consider the angles between the wallsegment and the
the wallsegments endpoints to $p$,$q$ and $r$.
For $q$ this is ($w$, $w0 q$), and ($w$, $w1 q$), note that both angles are acute (equal to or less than
$90^{\circ}$).
This is not true for $p$ and $r$ where one of the angles is always obtuse.
What can be concluded is that if both angles are acute then $isp_{proj}$ will be located on the wallsegment $w$. If not,
$isp_{proj}$ lies to the left or to the right of the wallsegment $w$ according to
which one of the angles is obtuse.\\
The angles are acute or obtuse if the dot product of the vectors involved are
respectively positive or negative.\\
To summarize: determining in which region the $isp_{proj}$ lies is boiled down to
two dot product calculations with the advantage that the actual projection
calculation can be skipped.
%Again this result determines if the distance should be computed to one of the
%corner points P0 or P1, or as the perpendicular distance to the line L (the
%wallside) itself.


\textit{TODO come back on outlier removal}
\subsection{Results}
% outliers: occluding tree, occluding lamp
% other building on background

\subsection{Discussion}
% TODO what if a plain flys by? note in drawbacks of method

\section{Update 3D model}
This module is not finished yet.


\section{References}
\begin{itemize}
\item $[1]$ 
Esteban, I., Dijk, J. Groen, F.C.A. FIT3D toolbox: multiple view geometry and
3D reconstruction for matlab. International Symposium on Security. Defence
Europe (SPIE), \[2010\].
\item $[8]$ Castano, Automatic detection of dust devils and clouds on Mars.
\item $[9]$ Cozman, Outdoor visual position estimation for planetary rovers.
\end{itemize}

\end{document}


