% TODO
% subjects to write 
% axis angle theory, to projection
% how I calc the wall normal


\section{Skyline detection}
 \subsection{Introduction}
If we take a regular image where both sky and earth are present, there
is often a clear seperatation between them. This separation is called the
skyline. %oehhhhhhh 
The detection of this skyline has proven to be a very succesful computer vision
application in a wide range of domains. In this domain it is used at urban
images to provide a countour of a building. This contour is later used to
provide 3D information about the scene.\\
The organisation of this chapter is as follows.  First we give a summary of some
related work on skyline detection.  Next we explain how we developed a new
robust skyline detection algorithm.  Finally we present some results.

\subsection{Related work}
%TODO
%mooie intro zin die niet hetzelfde is als vorige paragraaf
Skyline detection is used in a wide range of domains.
In $[8]$, Castano et all present a nice introduction of different skyline
detection techniques, these are listed below.
% TODO finish with papers in hand (now not present)

%------------------------------------------------------------------------
%\paragraph{Automatic detection of dust devils and clouds on Mars}\\
%Mars Exploration Rovers are used to detect clouds and dust devils on Mars.
%In [],%TODO REF
%their approach is to first identify the sky (equivalent, the skyline)
%and then determine if there are clouds in the region segmented as sky.
%%TODO elaborate on method?

\subsubsection{Horizon detection for Unmaned Air Vehicles}
%TODO images?
In this domain, the horizon detector for Unmaned Air Vehicles takes can take advantage of the high
altitude of the vehicle and therefor the horizon can be approximated to be a
straigt line. 
In [], %TODO Ref vision-guided flight staility and control for micro air vehicles
an algorithm is proposed which is based on the idea that this straight line spearates the image
onto two regions that have a different appearence. They use color as a measure
of appearance and generate two color distributions: one for the sky and one
for the ground. The distributions are optimized using bisection search on a criterium that uses the covariance and the eigen values of the distributions. The line that best separates the two distributions is determined to be the horizon.\\
This work is not applicable for detecting a building countour as the
straight line assumption doesn't work. But it needs to be mentioned that from this
idea some inspiration on line fitting is done because the building has walls that have straight lines.

%TODO, every related work article a pro and a con






%------------------------------------------------------------------------



\subsection{Method} % the algorithm, maar Isaac vindt dit fijner
\subsubsection{Situation and assumptions}
Before we present the method let's define the situation and make some
assumptions.\\

\textit{definition: skyline in urban scene}
A skyline in an urban scene is a set op points of the length $w$ (where $w$ is the
width of the size of the image) where each point describes the location of the
transition from the sky to an object (e.g. a bulding) which is mounted on the earth.

%motivate assumption
The first question arises: how are we going to detect this transition. In
general the color of the sky is very different then the buildingcolor. A
colorbased: using an edge detector would be an intuitive decision.\\
However, the sky and building itself also contain edges (e.g. clouds and 
windows) so how do we determine the right edge?
The number of possible edges could be decreased by thresholding the intensity of
the edge but it would still be a difficult task to determine the right edge.\\

To solve this problem we draw an assumption that is inspired on.
%TODO ref
%inspired on the idea of: %TODO 
Instead of using the sharpest edge we take the most upper sharp edge and
classify this edge as the skyline.

\textit{Top sharp edge assumption}
The first sharp edge (seen from top to bottom) in the image 
presents the skyline.
%TODO read original paper and see if i can get some assumption
%TODO discussion:
%drawback assumption, plane flys by


%TODO
%aim
%discuss input and output algorithm 


\subsubsection{Related algorithm}
To put our work in context, we first describe
a related skyline detection algorithm as presented in %TODO REF 
$[9]$.\\

% nog meer woordjes vooraf?
%gaussian smooth (explain if reader is thumb)
To increase the difference between sharp and vague edges (and let sharp edges
stand out more and vague edges disappear) the images are converted to Gaussian
smoothed images.  The smoothed image, which is a collection of $w$ x $h$ pixels,
is first divided in \#$w$ columns.  Next, each column produces a new column that
stores its vertical derivatives. This is called the smoothed intensity gradient.
The values of this column are high when a big change in color happens (e.g. an
edge is detected) at that location in the image. 
The system walks through the values of a column starting from the top.  When it
detects a pixel with a gradient higher then a certain threshold it stores its
y-value (the location of the heighest sharp edge of that column) and continues
to the next column.  The result is a set of $y$ coordinates of length $w$, that
represents the skyline. 

\subsubsection{Improved algorithm}
%TODO talk about the robustness
%TODO introduce a scene with bad lightning conditions
%motivation
Taking the smoothed intensity gradient is the most basic method of edge
detection and has the disadvantage that is is not robust to more vague edges. It
is not surprising that the algoritm in []%REF 
was used in an interactive system
where the user refines the result. 
Our aim is to develop a autonomous skyline detector, the only user interaction
that we allow is to provide the system some parameters. We will now discuss
the adaptations that we developed wrt the related algorithm.\\

The column based approach of the related algorithm seem te be very useful and is therefor unchanged. 
The related algorithm uses the smoothed intensity gradient as a method to detect edges. 
Because of the accuracy disadvantage of this method we took another approach in
detecting edges. Because we used Matlab as our implementation platform, we where
able to test different edge detecting types. The following edge detecting types
where tested.

%Because the column based part of the algorithm is individual, it can read any image, we could totally develop a new preprocess step.
%TODO better english
%----------------------------------------------------------------------------------------------------

The output of the different edge detection techniques was studied on an emprical
basis and the Canny edge detector came with the most promising results. This is
probably because Canny is the most 'intelligent' edge detector.  It uses two
thresholds, to detect strong and weak edges, and includes the weak edges in the
output only if they are connected to strong edges. In table %REF
a Matlab buildin edge detectors are listed with their method explanation.
%TODO check in matlab if canny uses 2 thresholds as args 

\begin{tabular}{|l|l|}
	\hline
	Edge detecting type		& method\\
	Sobel					& The Sobel method finds edges using the Sobel
	approximation to the derivative. It returns edges at those points where the
	gradient of I is maximum.\\
	\hline
	Prewitt					& The Prewitt method finds edges using the Prewitt
	approximation to the derivative. It returns edges at those points where the
	gradient of I is maximum.\\
	\hline
	Roberts					& The Roberts method finds edges using the Roberts
	approximation to the derivative. It returns edges at those points where the
	gradient of I is maximum.\\
	\hline
	Laplacian				& The Laplacian of Gaussian method finds edges by
	looking for zero crossings after filtering I with a Laplacian of Gaussian
	filter.\\
	\hline
	zero-cross				& The zero-cross method finds edges by looking for zero
	crossings after filtering I with a filter you specify.\\
	\hline
	Canny					& The Canny method finds edges by looking for local
	maxima of the gradient of I. The gradient is calculated using the derivative of
	a Gaussian filter. The method uses two thresholds, to detect strong and weak
	edges, and includes the weak edges in the output only if they are connected to
	strong edges. This method is therefor less likely than the others to be fooled
	by noise, and more likely to detect true weak edges.\\
	\hline
\end{tabular}

%TODO ML 
% check edge detector at different dataset and compair results
%However on an another scene an other edge type could perform better. 
Because the optimal edge detector type can be scene dependend, it can be set
by the user as a parameter in our functions.\\

%TODO ML check in matlab if every edge detector returns a binary image
The Canny edge detector outputs a binary image, therefor the column inlier
threshold is set to 1, which means that it finds the first pixel that is white. 
This is, as in the related algorithm, done from top to bottom for every column in
the image.\\

Because we know we are looking for sharp edges we improved the algorithm by
introducing two preprocessing steps. First the contrast of the image is
increased, this makes sharp edges stand out more.  Secondly the image undertakes
an extra Gaussian blur, this removes a large part of the nois. Note that
dependend on the edge detector type this could mean that the image is blurred
twice.\\

%TODO ML make different edge images, with and without each step (contrast, blur
% etc)

The system now has several parameters which has to be set manually by the user:
\begin{itemize}
	\item contrast,
	% officialy i don't do this contrast thing ghehe, whoepsie daisy fooling the
	% reader
	\item intensity (window size) of Gaussian blur,
	\item Sobel edge detector threshold,
\end{itemize}

% write down what parameter I used, and worked best
If the user introduces a new dataset these parameters needs to be configured
as the image quality and lightning condition are scene dependend.


\subsection{Results}% (What did I find)
% canny edge images
% skyline detector shizzle

\subsection{Discussion}  % (What do my results mean to me and why)
% what are the differences with the method in [9]?
\subsection{Conclusion and Future work}





% ----------------------------------------------------------------------------------------
% old shizzle menizzle, lets rewrite this shit aaight
\section{--Skyline detection}
 \subsection{--Introduction}

%Nice
The sky and the earth are separated by a skyline in images. The detection of this skyline
has proven to be a very succesful computer vision application in a wide range of
domains. In this domain it is used to provide a countour of a building. 

%TODO
This
contour is in a next step used to refine a 3D model of this building.\\
The organisation of this chapter is as follows.  First related work on skyline
detection is discussed, then a new algorithm of the skyline algorithm is
described and finally some results are presented.\\
% It is interesting to denote that the skyline detector a stand alone method and
% can be optimized individually without any knowledge of the other parts of the
% project.
 \subsection{--Related work}
A lot of related work on skyline detection is done and it is used in a wide
range of domains. $[8]$ yields a good introduction of different skyline
detection techniques, these are listed below.

\subsubsection{Cloud detection for Mars Exploration Rovers (MER)}
Mars Exploration Rovers (MER) are used to detect clouds and dust devils on Mars.
In [1] their approach is to first identify the sky (equivalentl, the skyline)
and then determine if there are clouds in the region segmented as sky.

\subsubsection{Horizon detection for Unmaned Air Vehicles (UAV)}
In this domain, the horizon detector for UAVs can take advantage of the high
altitude of the vehicle and therefor the horizon can be approximated to be a
straigt line.  This turns the detection problem into a line-fitting problem.
Ofcourse this work is not applicable for detecting a building countour as the
straight line assumption doesn't work. But it needs to be mentioned that from this
idea some inspiration on line fitting is done because the building countour has
straight line segments.

\subsubsection{Planetary Rover localisation}
In [9] they use the skyline detection in planetary rovers, their approach is to
combine the detected skyline with a given map of the landscape (hills, roads) to
detect its current location. 
The advantage of their technique is the simplicity and effectiveness of the
algorithm which makes it suitable for this project.  A big drawback is that it
is geared toward speed over extremely high accuracy because it is 
interactive system where an operator refines the skyline.

As mentioned in the introduction, in this project we use the skyline to extract
the building contour to eventually update a 3D model which is a brand new
purpose of skyline detection.  There is no user interaction present, and the
accuracy is a matter of high importance.  This makes it different from existing
skyline techniques and caution should be taken by using existing algorithms.
From the related work the Planetary Rover localisation [9] seemed to fit most on
this project.  Therefor method [9] is used, but as a basis, and a custom
algorithm with higher accuracy is developed. This is explained in the next
section.


%talk about assumptions
%there are always this assumptions??
%but what can we (not) assume?
%Furthermore we can assume that some straight lines 

 \subsection{--The Algorithm}
 \subsubsection{--The original algorithm}
 %TODO lezen: Map-based localization using the panoramic horizon
The skyline detection algorithm as described in $[9]$ works as follows:
The frames are first preprocessed by converting them to Gaussian smoothed images.
The skyline of a frame is then detected by analysing the the image columns
seperately.
The smoothed intensity gradient is calculated from top to bottom. This is done
by taking the derivative of the gaussian smoothed image.

The system takes the first pixel with gradient higher then a threshold to be
classified as a skyline element.  This is done for every column in the image.
The result is a set of coordinates of length $W$,
where $W$ is the width of the image, that represent the skyline.

Taking the smoothed intensity gradient is the most basic method of edge
detection and has the disadvantage that is is not robust to more vague
edges. This is not surprising as it its purpose was a interactive system where the
user refines the result. It is clear that an optimization is needed.

  \subsubsection{--The optimization}
 % page 15/16
 % idea for future work:
 % adaptive thresholding wrt average intensity!
The column based approach seem te be very useful and is therefor unchanged. 
The effectiveness of the algorithm is totally depended of the method of edge
detection and the preprocessing of the images. 
The original algorithm uses the smoothed intensity gradient as a way of
detecting edges. This is a very basic method and more sophisticated edge
detection algorithms are present.\\



To select a proper edge detector, a practical study is done on the different
Matlab build in edge detection techniques. The output of the different edge
detection techniques was studied and the Sobel edge detector came with the most
promising results. The Sobel edge detector outputs a binary image, therefor the column inlier
threshold method is replaced by finding the first white pixel. This is as the
original algorithm done from top to bottom for every column in the image.
\\ 
To make the algorithm more precise, two preprocessing
steps are introduced. First the contrast of the image is increased, this makes
sharp edges stand out more.  Secondly the image undertakes a Gaussian blur,
this removes a large part of the noise.

The system now has several parameters which has to be set manually by the user:
\begin{itemize}
	\item contrast,
	% officialy i don't do this contrast thing
	\item intensity (window size) of Gaussian blur,
	\item Sobel edge detector threshold,
\end{itemize}
\textit{Should I write down what parameter values I used or is this of too much
detail}

If the user introduces a new dataset these parameters needs to be changed
as the image quality and lightning condition are probably different.
%(Automatic parameter estimation based on the image would be interesting future
%work but lies without the scope of this research.)

 \subsection{--Results}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The system assumes that the first sharp edge (seen from top to bottom) is
always the skyline/building edge. This gives raise to some outliers, for 
for example a streetlight or a tree. These outliers are removed as described in
the next section.  

The Skyline detector without outlier removal has an accuracy of 80 \% 
Some results on the Floriande dataset $[1]$ can be seen in Figure \ref{fig:outputskyline}.

\figs{outputskyline}{outputSkylineIm3-2.eps}{outputSkylineIm3-3.eps}{The output
of the edge detector}{The output of the skyline detector. The skyline elements are marked red}{1}


\section{--Skyline projection}
 \subsection{--Introduction}
The final product of this research is an accurate 3D model of an urban
landscape. This is accomplished in several steps, so far the first step: skyline detection is
explained. The next step is to create a basic 3D model of the urban landscape.
Then the retrieved skyline of the previous section is used to update the basic
3D model of the building. This is illustrated in Figure \ref{fig:uml1.eps}

\fig{uml1.eps}{Situation scheme}{0.5}



   \subsection{--3D modelling}
First a rough 3D model of the urban landscape has to be generated. This is done
by taking a top-view Google maps image of the scene and extract the contour of
the building, this is done manually. 
The height of the building is estimated, also manually, and together with the
contour a rough 3D model is generated, see Figure \ref{fig:building.eps}. 
\fig{building.eps}{A rough 3D model of the building}{1}
%TODO: disadvantage: height of building everywhere the same


% TODO how do I combine the results of the different angles
\subsection{--Project to 3D space}
When the skylines of the 2D images are projected to 3D it can give 
accurate information about the contour of the building(s). This is used to
refine the basic 3D model.  Next is explained how the skylines are projected to
3D.\\

A skyline consist of different skyline pixels. Every 2D skyline pixel presents a 3D point in space. No
information is known about the distance from the 3D point to the camera that
took the picture. What is known is de 2D location of the pixel which reduces the possible points in 3D
space to a line.  This line is known and spanned by two 
coordinates:\\ 
\begin{itemize}
	\item The camera center %(camera centers are annotated for every image)
	\item $K'p$, where $K$ is the Calibration matrix of the camera and $p$ is the homogeneous pixel coordinate.
	\textit{Why K'p, I don't remember the theory behind it and can't find it in your paper. Would you explain this Isaac?}
\end{itemize}


%---
% TODO write in algorithm style?
For every skyline pixel a line spanned by the above two coordinates is derived.


\subsection{--Intersect with building}
The lines derived as described in the previous section are not enough to refine
the 3D model because it is still unknown which skyline part belongs to which
part of the 3D model.

Therefor these lines of possibel pixel locations need to
be reduced to the actual 3D locations of the pixels.  This is done by intersecting
them with the walls of the rough 3D model and is done as follows.

The building is first divided into different walls.  Every wall of the building spans a plane. 
Intersections are then calculated between the lines and the planes of the building walls.\\
\textit{Isaac, should I put a intersection formula down here or is this trivial?}\\

A skyline pixel intersects with every wall as both the lines and planes are
infinite and have a very low change of being parallel.
The next challenge is to reduce the number of intersection for every skylinepixel
to one. In other words, to determine the wall that is responsible for that pixel. 
This is later on used to update the 3D model at the right place.

The details of this process is explained next:
%has the largest probability of being 

\subsection{--Results}
% discuss occluding tree, occluding lamp
% other building on background


\subsection{--Discussion}
% TODO what if a plain flys by? note in drawbacks of method

\section{--Update 3D model}
This module is not finished yet.


\section{--References}
\begin{itemize}
\item $[1]$ 
Esteban, I., Dijk, J. Groen, F.C.A. FIT3D toolbox: multiple view geometry and
3D reconstruction for matlab. International Symposium on Security. Defence
Europe (SPIE), \[2010\].
\item $[8]$ Castano, Automatic detection of dust devils and clouds on Mars.
\item $[9]$ Cozman, Outdoor visual position estimation for planetary rovers.
\end{itemize}

% TODO
% pictures
% explain motivation behind decisions
% explain what is new or what is unique


% TODO
% put this in one of the first chapters :
% an intro which answers the question:
% detail of the problem, why is this such a interesting problem
% what did others and how is my work related?
% write something about mogelijke complications en/of juist wat je er wel aan hebt
%

% TODO background section?
% image captions
% read whole report

% TODO: documentclass anders zodat alles breder is en plaatjes ook breder kunnen
% en niet bij 7cm al afbreken

% TODO
% name assumption; the photos taken are straight, the walls are vertical, ivm 
% (columnwise algo part of the skyline detector )


% TODO
% discussion:
% Furthermore it would be good to fully discard the flat roof assumption. This will allow a building to have any shape, which is nice. The drawback is that a new method of outlier detection has to be developed. 
% Object recognition play a great part in this. If one knows where, for example, an occluding object is located, this could be used to filter the output of the skyline detector. This however rises a new problem, namely that the 3D model has no augmentations at the position of occluding objects. Determining what would be located behind the occluding object would be an interesting AI challenge and will incorporate pattern recognition, making use of repetitive structures and off course combining the multiple views to reveal as much information as possible.\\



